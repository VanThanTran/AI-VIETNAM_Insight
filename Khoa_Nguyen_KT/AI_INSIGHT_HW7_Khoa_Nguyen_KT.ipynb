{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tên: Nguyễn Thọ Anh Khoa\n",
    "\n",
    "Facebook: Khoa Nguyễn KT\n",
    "\n",
    "Lớp: AI Insight "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensorflow – Exercise 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Đọc hiểu các về cách cài đặt bài linear, losgistic và softmax regression dùng Gradient tape trong tensorflow.\n",
    "\n",
    "## a) Linear-Researcher.ipynb\n",
    "\n",
    "## b) Logistic-Researcher.ipynb \n",
    "\n",
    "## c) Softmax-Researcher.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  a) Linear-Researcher.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thay vì khai báo các parmeter cần học và hàm thực thi predict riêng lẻ thì ở đây sử dung 1 calss model thuận tiện cho việc sử dụng. class này bao gồm method __init__ sẽ khởi tạo W and b, và method __call__ dùng như 1 function predict. Sau đó sẽ sử dụng gradient tape để backward và cập nhật parameter. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import tensorflow as tf \n",
    "import numpy as np\n",
    "\n",
    "data = np.genfromtxt('data_lir.csv', delimiter=',')\n",
    "X = data[:, 0:1]\n",
    "y = data[:, 1:]\n",
    "\n",
    "class Linear_Model(object):\n",
    "    def __init__(self):\n",
    "        self.W = tf.Variable(0.5)\n",
    "        self.b = tf.Variable(1.0)\n",
    "    def __call__(self, x):\n",
    "        return self.W*x + self.b\n",
    "    \n",
    "lir_model = Linear_Model()\n",
    "epochs = 100\n",
    "lr = 0.01\n",
    "\n",
    "losses = []\n",
    "for epoch in range(epochs):\n",
    "    with tf.GradientTape() as tape:\n",
    "        #predict\n",
    "        output = lir_model(X)\n",
    "        \n",
    "        #loss\n",
    "        loss = tf.reduce_mean(tf.square(output - y))\n",
    "        losses.append(loss.numpy())\n",
    "        \n",
    "        #gradient\n",
    "        dW, db = tape.gradient(loss, [lir_model.W, lir_model.b])\n",
    "        \n",
    "        #update\n",
    "        lir_model.W.assign_sub(lr*dW)\n",
    "        lir_model.b.assign_sub(lr*db)\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(losses)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b) Logistic-Researcher.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "data = np.genfromtxt('iris_full_lor.csv', delimiter=',', skip_header=1)\n",
    "X = data[:, 0:4]\n",
    "y = data[:, 4:]\n",
    "\n",
    "#create model\n",
    "class Lor_Model(object):\n",
    "    def __init__(self, num_features):\n",
    "        self.W = tf.Variable(tf.random.uniform((num_features,1), dtype=tf.float64), dtype=tf.float64)\n",
    "        self.b = tf.Variable(0.01, dtype=tf.float64)\n",
    "    def __call__(self, x):\n",
    "        return tf.math.sigmoid(tf.matmul(x, self.W) + self.b)\n",
    "    \n",
    "lor_model = Lor_Model(X.shape[1])\n",
    "lr = 0.1\n",
    "epochs = 100\n",
    "losses = []\n",
    "loss = tf.keras.losses.BinaryCrossentropy()\n",
    "for epoch in range(epochs):\n",
    "    with tf.GradientTape() as tape:\n",
    "        #predict \n",
    "        output = lor_model(X)\n",
    "        \n",
    "        #loss\n",
    "        loss_v = loss(y, output)\n",
    "        losses.append(loss_v.numpy())\n",
    "        \n",
    "        #gradient\n",
    "        dW, db = tape.gradient(loss_v, [lor_model.W, lor_model.b])\n",
    "        \n",
    "        #update\n",
    "        lor_model.W.assign_sub(lr*dW)\n",
    "        lor_model.b.assign_sub(lr*db)\n",
    "        \n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(losses)\n",
    "plt.show\n",
    "\n",
    "res = tf.round(lor_model(X)).numpy()\n",
    "acc = (res == y)\n",
    "acc.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c) Softmax-Researcher.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Các step của softmax tương tự như homework trước, chỉ khác về  cách thực hiện trên numpy còn phần này thực hiện trên tensorflow.\n",
    "\n",
    "- Hàm predict dùng functon tf.math.softmax() của tensorflow để tìm softmax\n",
    "\n",
    "- Ở phần weight, W, và b được khai báo đôc lập để thực hiện giống theo công thức và phù hợp với hàm predict ở trên, nhưng cũng có thể gộp W và b thành 1 variable \n",
    "\n",
    "- Loss ở đây được sử dụng là SparseCategoricalCrossentropy() Sparse sẽ có tính năng giống như convert label thành one_hot_encoding\n",
    "\n",
    "- Các biến tham gia trong đạo hàm sẽ luôn được duy trì như variable của tensorflow chỉ có loss sẽ được lấy ra theo như numpy thay vì tensor để show trên graph.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "tf.keras.backend.set_floatx('float64')\n",
    "\n",
    "### Data preparation\n",
    "data = np.genfromtxt('iris_full_sm.csv', delimiter=',', skip_header=1)\n",
    "X = data[:,0:4]\n",
    "y = data[:,4:]\n",
    "\n",
    "\n",
    "# predict    \n",
    "def predict(x, W, b):\n",
    "    return tf.math.softmax(tf.matmul(x, W) + b)\n",
    "\n",
    "    \n",
    "### weights\n",
    "W = tf.Variable(tf.random.normal((4, 3), dtype=tf.float64))\n",
    "b = tf.Variable(tf.random.normal((3,), dtype=tf.float64))\n",
    "\n",
    "### training\n",
    "learning_rate = 0.1\n",
    "num_epochs = 200\n",
    "\n",
    "### loss function\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy()\n",
    "\n",
    "losses = [] # for debug\n",
    "for epoch in range(num_epochs):\n",
    "    with tf.GradientTape() as t:\n",
    "        # output\n",
    "        output = predict(X, W, b)\n",
    "        \n",
    "        # loss\n",
    "        loss_v = loss(y, output)\n",
    "        losses.append(loss_v.numpy())\n",
    "        \n",
    "        # gradient\n",
    "        dW, db = t.gradient(loss_v, [W, b])\n",
    "        \n",
    "        # update\n",
    "        W.assign_sub(learning_rate * dW) \n",
    "        b.assign_sub(learning_rate * db)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Đọc hiểu các về cách cài đặt bài linear, losgistic và softmax regression dùng tf.keras trong tensorflow.\n",
    "\n",
    "## a) Linear-Engineer.ipynb\n",
    "\n",
    "## b) Logistic-Engineer-Iris.ipynb\n",
    "\n",
    "## c) Softmax-Engineer-Iris.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a) Linear-Engineer.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ở phần này tận dụng hoàn toàn vào các tính năng mà framework cung cấp, người dùng không cần can thiệp quá sâu và đã có API hỗ trợ hết.\n",
    "Các bước thực hiện tạo model -> khởi tạo values cho các parameters -> compile model (gồm chọn loại optimizer, loss function) -> train model bằng các dùng features, label, batch_size, epochs -> save model hoặc weights\n",
    "\n",
    "Các điểm lưu ý là:\n",
    "\n",
    "- Để đưa giá trị vào weight ta sử dụng model.layers[vij_tris_layer_muốn_set].set_weights([list numpy các giá trị của weight]). (ở đây là xo và b1)\n",
    "\n",
    "- Khi save model để sử dụng lại thì có 2 cách save weight hoặc save toàn bộ model\n",
    "    + Khi save weight thì chỉ bộ weight được lưu lại, nếu muốn sử dụng phải tạo lại kiến trúc của model và kiến trúc này phải giống như trong lúc train thì mới có thể load check point lên được. Thông thường kiểu load này được dùng trong lúc train muốn dùng lại để train tiếp.\n",
    "    + Khi save toàn bộ model thì cho dù không biết được kiến trúc của model thì vẫn chỉ cần load lên và sử dụng ngay được. Kiểu load này thích hợp cho việc deploy, ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4 samples\n",
      "Epoch 1/100\n",
      "4/4 [==============================] - 0s 72ms/sample - loss: 10.3619\n",
      "Epoch 2/100\n",
      "4/4 [==============================] - 0s 1ms/sample - loss: 2.1497\n",
      "Epoch 3/100\n",
      "4/4 [==============================] - 0s 1ms/sample - loss: 0.5724\n",
      "Epoch 4/100\n",
      "4/4 [==============================] - 0s 2ms/sample - loss: 0.2693\n",
      "Epoch 5/100\n",
      "4/4 [==============================] - 0s 797us/sample - loss: 0.2109\n",
      "Epoch 6/100\n",
      "4/4 [==============================] - 0s 1ms/sample - loss: 0.1995\n",
      "Epoch 7/100\n",
      "4/4 [==============================] - 0s 1ms/sample - loss: 0.1971\n",
      "Epoch 8/100\n",
      "4/4 [==============================] - 0s 991us/sample - loss: 0.1965\n",
      "Epoch 9/100\n",
      "4/4 [==============================] - 0s 2ms/sample - loss: 0.1962\n",
      "Epoch 10/100\n",
      "4/4 [==============================] - 0s 1ms/sample - loss: 0.1960\n",
      "Epoch 11/100\n",
      "4/4 [==============================] - 0s 2ms/sample - loss: 0.1957\n",
      "Epoch 12/100\n",
      "4/4 [==============================] - 0s 716us/sample - loss: 0.1955\n",
      "Epoch 13/100\n",
      "4/4 [==============================] - 0s 868us/sample - loss: 0.1953\n",
      "Epoch 14/100\n",
      "4/4 [==============================] - 0s 2ms/sample - loss: 0.1951\n",
      "Epoch 15/100\n",
      "4/4 [==============================] - 0s 2ms/sample - loss: 0.1949\n",
      "Epoch 16/100\n",
      "4/4 [==============================] - 0s 943us/sample - loss: 0.1946\n",
      "Epoch 17/100\n",
      "4/4 [==============================] - 0s 2ms/sample - loss: 0.1944\n",
      "Epoch 18/100\n",
      "4/4 [==============================] - 0s 2ms/sample - loss: 0.1942\n",
      "Epoch 19/100\n",
      "4/4 [==============================] - 0s 2ms/sample - loss: 0.1940\n",
      "Epoch 20/100\n",
      "4/4 [==============================] - 0s 1ms/sample - loss: 0.1938\n",
      "Epoch 21/100\n",
      "4/4 [==============================] - 0s 866us/sample - loss: 0.1935\n",
      "Epoch 22/100\n",
      "4/4 [==============================] - 0s 1ms/sample - loss: 0.1933\n",
      "Epoch 23/100\n",
      "4/4 [==============================] - 0s 1ms/sample - loss: 0.1931\n",
      "Epoch 24/100\n",
      "4/4 [==============================] - 0s 2ms/sample - loss: 0.1929\n",
      "Epoch 25/100\n",
      "4/4 [==============================] - 0s 736us/sample - loss: 0.1927\n",
      "Epoch 26/100\n",
      "4/4 [==============================] - 0s 2ms/sample - loss: 0.1925\n",
      "Epoch 27/100\n",
      "4/4 [==============================] - 0s 1ms/sample - loss: 0.1922\n",
      "Epoch 28/100\n",
      "4/4 [==============================] - 0s 800us/sample - loss: 0.1920\n",
      "Epoch 29/100\n",
      "4/4 [==============================] - 0s 1ms/sample - loss: 0.1918\n",
      "Epoch 30/100\n",
      "4/4 [==============================] - 0s 1ms/sample - loss: 0.1916\n",
      "Epoch 31/100\n",
      "4/4 [==============================] - 0s 2ms/sample - loss: 0.1914\n",
      "Epoch 32/100\n",
      "4/4 [==============================] - 0s 951us/sample - loss: 0.1912\n",
      "Epoch 33/100\n",
      "4/4 [==============================] - 0s 4ms/sample - loss: 0.1910\n",
      "Epoch 34/100\n",
      "4/4 [==============================] - 0s 739us/sample - loss: 0.1907\n",
      "Epoch 35/100\n",
      "4/4 [==============================] - 0s 970us/sample - loss: 0.1905\n",
      "Epoch 36/100\n",
      "4/4 [==============================] - 0s 1ms/sample - loss: 0.1903\n",
      "Epoch 37/100\n",
      "4/4 [==============================] - 0s 3ms/sample - loss: 0.1901\n",
      "Epoch 38/100\n",
      "4/4 [==============================] - 0s 3ms/sample - loss: 0.1899\n",
      "Epoch 39/100\n",
      "4/4 [==============================] - 0s 3ms/sample - loss: 0.1897\n",
      "Epoch 40/100\n",
      "4/4 [==============================] - 0s 3ms/sample - loss: 0.1895\n",
      "Epoch 41/100\n",
      "4/4 [==============================] - 0s 1ms/sample - loss: 0.1893\n",
      "Epoch 42/100\n",
      "4/4 [==============================] - 0s 2ms/sample - loss: 0.1891\n",
      "Epoch 43/100\n",
      "4/4 [==============================] - 0s 3ms/sample - loss: 0.1889\n",
      "Epoch 44/100\n",
      "4/4 [==============================] - 0s 864us/sample - loss: 0.1886\n",
      "Epoch 45/100\n",
      "4/4 [==============================] - 0s 2ms/sample - loss: 0.1884\n",
      "Epoch 46/100\n",
      "4/4 [==============================] - 0s 3ms/sample - loss: 0.1882\n",
      "Epoch 47/100\n",
      "4/4 [==============================] - 0s 2ms/sample - loss: 0.1880\n",
      "Epoch 48/100\n",
      "4/4 [==============================] - 0s 1ms/sample - loss: 0.1878\n",
      "Epoch 49/100\n",
      "4/4 [==============================] - 0s 871us/sample - loss: 0.1876\n",
      "Epoch 50/100\n",
      "4/4 [==============================] - 0s 1ms/sample - loss: 0.1874\n",
      "Epoch 51/100\n",
      "4/4 [==============================] - 0s 3ms/sample - loss: 0.1872\n",
      "Epoch 52/100\n",
      "4/4 [==============================] - 0s 2ms/sample - loss: 0.1870\n",
      "Epoch 53/100\n",
      "4/4 [==============================] - 0s 1ms/sample - loss: 0.1868\n",
      "Epoch 54/100\n",
      "4/4 [==============================] - 0s 2ms/sample - loss: 0.1866\n",
      "Epoch 55/100\n",
      "4/4 [==============================] - 0s 2ms/sample - loss: 0.1864\n",
      "Epoch 56/100\n",
      "4/4 [==============================] - 0s 2ms/sample - loss: 0.1862\n",
      "Epoch 57/100\n",
      "4/4 [==============================] - 0s 2ms/sample - loss: 0.1860\n",
      "Epoch 58/100\n",
      "4/4 [==============================] - 0s 1ms/sample - loss: 0.1858\n",
      "Epoch 59/100\n",
      "4/4 [==============================] - 0s 1ms/sample - loss: 0.1856\n",
      "Epoch 60/100\n",
      "4/4 [==============================] - 0s 1ms/sample - loss: 0.1854\n",
      "Epoch 61/100\n",
      "4/4 [==============================] - 0s 1ms/sample - loss: 0.1852\n",
      "Epoch 62/100\n",
      "4/4 [==============================] - 0s 1ms/sample - loss: 0.1850\n",
      "Epoch 63/100\n",
      "4/4 [==============================] - 0s 2ms/sample - loss: 0.1848\n",
      "Epoch 64/100\n",
      "4/4 [==============================] - 0s 1ms/sample - loss: 0.1846\n",
      "Epoch 65/100\n",
      "4/4 [==============================] - 0s 692us/sample - loss: 0.1844\n",
      "Epoch 66/100\n",
      "4/4 [==============================] - 0s 2ms/sample - loss: 0.1842\n",
      "Epoch 67/100\n",
      "4/4 [==============================] - 0s 3ms/sample - loss: 0.1840\n",
      "Epoch 68/100\n",
      "4/4 [==============================] - 0s 1ms/sample - loss: 0.1838\n",
      "Epoch 69/100\n",
      "4/4 [==============================] - 0s 1ms/sample - loss: 0.1836\n",
      "Epoch 70/100\n",
      "4/4 [==============================] - 0s 2ms/sample - loss: 0.1834\n",
      "Epoch 71/100\n",
      "4/4 [==============================] - 0s 2ms/sample - loss: 0.1832\n",
      "Epoch 72/100\n",
      "4/4 [==============================] - 0s 1ms/sample - loss: 0.1830\n",
      "Epoch 73/100\n",
      "4/4 [==============================] - 0s 1ms/sample - loss: 0.1828\n",
      "Epoch 74/100\n",
      "4/4 [==============================] - 0s 1ms/sample - loss: 0.1826\n",
      "Epoch 75/100\n",
      "4/4 [==============================] - 0s 2ms/sample - loss: 0.1824\n",
      "Epoch 76/100\n",
      "4/4 [==============================] - 0s 3ms/sample - loss: 0.1822\n",
      "Epoch 77/100\n",
      "4/4 [==============================] - 0s 1ms/sample - loss: 0.1820\n",
      "Epoch 78/100\n",
      "4/4 [==============================] - 0s 2ms/sample - loss: 0.1818\n",
      "Epoch 79/100\n",
      "4/4 [==============================] - 0s 1ms/sample - loss: 0.1816\n",
      "Epoch 80/100\n",
      "4/4 [==============================] - 0s 776us/sample - loss: 0.1814\n",
      "Epoch 81/100\n",
      "4/4 [==============================] - 0s 2ms/sample - loss: 0.1812\n",
      "Epoch 82/100\n",
      "4/4 [==============================] - 0s 1ms/sample - loss: 0.1810\n",
      "Epoch 83/100\n",
      "4/4 [==============================] - 0s 1ms/sample - loss: 0.1808\n",
      "Epoch 84/100\n",
      "4/4 [==============================] - 0s 1ms/sample - loss: 0.1807\n",
      "Epoch 85/100\n",
      "4/4 [==============================] - 0s 875us/sample - loss: 0.1805\n",
      "Epoch 86/100\n",
      "4/4 [==============================] - 0s 2ms/sample - loss: 0.1803\n",
      "Epoch 87/100\n",
      "4/4 [==============================] - 0s 1ms/sample - loss: 0.1801\n",
      "Epoch 88/100\n",
      "4/4 [==============================] - 0s 2ms/sample - loss: 0.1799\n",
      "Epoch 89/100\n",
      "4/4 [==============================] - 0s 2ms/sample - loss: 0.1797\n",
      "Epoch 90/100\n",
      "4/4 [==============================] - 0s 1ms/sample - loss: 0.1795\n",
      "Epoch 91/100\n",
      "4/4 [==============================] - 0s 2ms/sample - loss: 0.1793\n",
      "Epoch 92/100\n",
      "4/4 [==============================] - 0s 2ms/sample - loss: 0.1791\n",
      "Epoch 93/100\n",
      "4/4 [==============================] - 0s 1ms/sample - loss: 0.1789\n",
      "Epoch 94/100\n",
      "4/4 [==============================] - 0s 2ms/sample - loss: 0.1787\n",
      "Epoch 95/100\n",
      "4/4 [==============================] - 0s 1ms/sample - loss: 0.1786\n",
      "Epoch 96/100\n",
      "4/4 [==============================] - 0s 1ms/sample - loss: 0.1784\n",
      "Epoch 97/100\n",
      "4/4 [==============================] - 0s 2ms/sample - loss: 0.1782\n",
      "Epoch 98/100\n",
      "4/4 [==============================] - 0s 2ms/sample - loss: 0.1780\n",
      "Epoch 99/100\n",
      "4/4 [==============================] - 0s 2ms/sample - loss: 0.1778\n",
      "Epoch 100/100\n",
      "4/4 [==============================] - 0s 1ms/sample - loss: 0.1776\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "import numpy as np\n",
    "\n",
    "data = np.genfromtxt('data_lir.csv', delimiter=',')\n",
    "X = data[:, 0:1]\n",
    "y = data[:, 1:]\n",
    "\n",
    "#create model\n",
    "lir_model = keras.Sequential([keras.layers.Dense(units=1, input_shape=[1])])\n",
    "\n",
    "#initial weight \n",
    "lir_model.layers[0].set_weights([np.array([[0.5]]), np.array([1.0])])\n",
    "\n",
    "#compile model\n",
    "opt = keras.optimizers.SGD(learning_rate=0.01)\n",
    "lir_model.compile(optimizer=opt, loss='mse')\n",
    "\n",
    "#training\n",
    "batch_size = 4\n",
    "epochs = 100\n",
    "history = lir_model.fit(X, y, batch_size, epochs)\n",
    "\n",
    "# # parameters after one epoch\n",
    "# print('weight-bias: \\n', model.layers[0].get_weights())\n",
    "\n",
    "#save model\n",
    "checkpoint_path = \"lir_model.ckpt\"\n",
    "lir_model.save_weights(checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[6.5581503]]\n"
     ]
    }
   ],
   "source": [
    "# create model\n",
    "lir_model = tf.keras.Sequential(\n",
    "    [tf.keras.layers.Dense(units=1, input_shape=[1])])\n",
    "\n",
    "# load model\n",
    "lir_model.load_weights('lir_model.ckpt')\n",
    "\n",
    "X_testing = [[5.0]]\n",
    "y_hat = lir_model.predict(X_testing)\n",
    "print(y_hat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sử dụng load toàn bộ model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4 samples\n",
      "Epoch 1/100\n",
      "4/4 [==============================] - 0s 72ms/sample - loss: 10.3619\n",
      "Epoch 2/100\n",
      "4/4 [==============================] - 0s 740us/sample - loss: 2.1497\n",
      "Epoch 3/100\n",
      "4/4 [==============================] - 0s 2ms/sample - loss: 0.5724\n",
      "Epoch 4/100\n",
      "4/4 [==============================] - 0s 962us/sample - loss: 0.2693\n",
      "Epoch 5/100\n",
      "4/4 [==============================] - 0s 1ms/sample - loss: 0.2109\n",
      "Epoch 6/100\n",
      "4/4 [==============================] - 0s 953us/sample - loss: 0.1995\n",
      "Epoch 7/100\n",
      "4/4 [==============================] - 0s 995us/sample - loss: 0.1971\n",
      "Epoch 8/100\n",
      "4/4 [==============================] - 0s 1ms/sample - loss: 0.1965\n",
      "Epoch 9/100\n",
      "4/4 [==============================] - 0s 1ms/sample - loss: 0.1962\n",
      "Epoch 10/100\n",
      "4/4 [==============================] - 0s 928us/sample - loss: 0.1960\n",
      "Epoch 11/100\n",
      "4/4 [==============================] - 0s 1ms/sample - loss: 0.1957\n",
      "Epoch 12/100\n",
      "4/4 [==============================] - 0s 2ms/sample - loss: 0.1955\n",
      "Epoch 13/100\n",
      "4/4 [==============================] - 0s 2ms/sample - loss: 0.1953\n",
      "Epoch 14/100\n",
      "4/4 [==============================] - 0s 1ms/sample - loss: 0.1951\n",
      "Epoch 15/100\n",
      "4/4 [==============================] - 0s 2ms/sample - loss: 0.1949\n",
      "Epoch 16/100\n",
      "4/4 [==============================] - 0s 3ms/sample - loss: 0.1946\n",
      "Epoch 17/100\n",
      "4/4 [==============================] - 0s 1ms/sample - loss: 0.1944\n",
      "Epoch 18/100\n",
      "4/4 [==============================] - 0s 1ms/sample - loss: 0.1942\n",
      "Epoch 19/100\n",
      "4/4 [==============================] - 0s 1ms/sample - loss: 0.1940\n",
      "Epoch 20/100\n",
      "4/4 [==============================] - 0s 4ms/sample - loss: 0.1938\n",
      "Epoch 21/100\n",
      "4/4 [==============================] - 0s 998us/sample - loss: 0.1935\n",
      "Epoch 22/100\n",
      "4/4 [==============================] - 0s 1ms/sample - loss: 0.1933\n",
      "Epoch 23/100\n",
      "4/4 [==============================] - 0s 1ms/sample - loss: 0.1931\n",
      "Epoch 24/100\n",
      "4/4 [==============================] - 0s 896us/sample - loss: 0.1929\n",
      "Epoch 25/100\n",
      "4/4 [==============================] - 0s 2ms/sample - loss: 0.1927\n",
      "Epoch 26/100\n",
      "4/4 [==============================] - 0s 1ms/sample - loss: 0.1925\n",
      "Epoch 27/100\n",
      "4/4 [==============================] - 0s 1ms/sample - loss: 0.1922\n",
      "Epoch 28/100\n",
      "4/4 [==============================] - 0s 1ms/sample - loss: 0.1920\n",
      "Epoch 29/100\n",
      "4/4 [==============================] - 0s 1ms/sample - loss: 0.1918\n",
      "Epoch 30/100\n",
      "4/4 [==============================] - 0s 2ms/sample - loss: 0.1916\n",
      "Epoch 31/100\n",
      "4/4 [==============================] - 0s 1ms/sample - loss: 0.1914\n",
      "Epoch 32/100\n",
      "4/4 [==============================] - 0s 1ms/sample - loss: 0.1912\n",
      "Epoch 33/100\n",
      "4/4 [==============================] - 0s 2ms/sample - loss: 0.1910\n",
      "Epoch 34/100\n",
      "4/4 [==============================] - 0s 1ms/sample - loss: 0.1907\n",
      "Epoch 35/100\n",
      "4/4 [==============================] - 0s 1ms/sample - loss: 0.1905\n",
      "Epoch 36/100\n",
      "4/4 [==============================] - 0s 914us/sample - loss: 0.1903\n",
      "Epoch 37/100\n",
      "4/4 [==============================] - 0s 1ms/sample - loss: 0.1901\n",
      "Epoch 38/100\n",
      "4/4 [==============================] - 0s 797us/sample - loss: 0.1899\n",
      "Epoch 39/100\n",
      "4/4 [==============================] - 0s 4ms/sample - loss: 0.1897\n",
      "Epoch 40/100\n",
      "4/4 [==============================] - 0s 1ms/sample - loss: 0.1895\n",
      "Epoch 41/100\n",
      "4/4 [==============================] - 0s 1ms/sample - loss: 0.1893\n",
      "Epoch 42/100\n",
      "4/4 [==============================] - 0s 1ms/sample - loss: 0.1891\n",
      "Epoch 43/100\n",
      "4/4 [==============================] - 0s 1ms/sample - loss: 0.1889\n",
      "Epoch 44/100\n",
      "4/4 [==============================] - 0s 833us/sample - loss: 0.1886\n",
      "Epoch 45/100\n",
      "4/4 [==============================] - 0s 4ms/sample - loss: 0.1884\n",
      "Epoch 46/100\n",
      "4/4 [==============================] - 0s 1000us/sample - loss: 0.1882\n",
      "Epoch 47/100\n",
      "4/4 [==============================] - 0s 613us/sample - loss: 0.1880\n",
      "Epoch 48/100\n",
      "4/4 [==============================] - 0s 2ms/sample - loss: 0.1878\n",
      "Epoch 49/100\n",
      "4/4 [==============================] - 0s 1ms/sample - loss: 0.1876\n",
      "Epoch 50/100\n",
      "4/4 [==============================] - 0s 1ms/sample - loss: 0.1874\n",
      "Epoch 51/100\n",
      "4/4 [==============================] - 0s 824us/sample - loss: 0.1872\n",
      "Epoch 52/100\n",
      "4/4 [==============================] - 0s 890us/sample - loss: 0.1870\n",
      "Epoch 53/100\n",
      "4/4 [==============================] - 0s 2ms/sample - loss: 0.1868\n",
      "Epoch 54/100\n",
      "4/4 [==============================] - 0s 1ms/sample - loss: 0.1866\n",
      "Epoch 55/100\n",
      "4/4 [==============================] - 0s 2ms/sample - loss: 0.1864\n",
      "Epoch 56/100\n",
      "4/4 [==============================] - 0s 817us/sample - loss: 0.1862\n",
      "Epoch 57/100\n",
      "4/4 [==============================] - 0s 785us/sample - loss: 0.1860\n",
      "Epoch 58/100\n",
      "4/4 [==============================] - 0s 682us/sample - loss: 0.1858\n",
      "Epoch 59/100\n",
      "4/4 [==============================] - 0s 1ms/sample - loss: 0.1856\n",
      "Epoch 60/100\n",
      "4/4 [==============================] - 0s 2ms/sample - loss: 0.1854\n",
      "Epoch 61/100\n",
      "4/4 [==============================] - 0s 1ms/sample - loss: 0.1852\n",
      "Epoch 62/100\n",
      "4/4 [==============================] - 0s 1ms/sample - loss: 0.1850\n",
      "Epoch 63/100\n",
      "4/4 [==============================] - 0s 1ms/sample - loss: 0.1848\n",
      "Epoch 64/100\n",
      "4/4 [==============================] - 0s 731us/sample - loss: 0.1846\n",
      "Epoch 65/100\n",
      "4/4 [==============================] - 0s 6ms/sample - loss: 0.1844\n",
      "Epoch 66/100\n",
      "4/4 [==============================] - 0s 3ms/sample - loss: 0.1842\n",
      "Epoch 67/100\n",
      "4/4 [==============================] - 0s 1ms/sample - loss: 0.1840\n",
      "Epoch 68/100\n",
      "4/4 [==============================] - 0s 3ms/sample - loss: 0.1838\n",
      "Epoch 69/100\n",
      "4/4 [==============================] - 0s 1ms/sample - loss: 0.1836\n",
      "Epoch 70/100\n",
      "4/4 [==============================] - 0s 2ms/sample - loss: 0.1834\n",
      "Epoch 71/100\n",
      "4/4 [==============================] - 0s 1ms/sample - loss: 0.1832\n",
      "Epoch 72/100\n",
      "4/4 [==============================] - 0s 3ms/sample - loss: 0.1830\n",
      "Epoch 73/100\n",
      "4/4 [==============================] - 0s 2ms/sample - loss: 0.1828\n",
      "Epoch 74/100\n",
      "4/4 [==============================] - 0s 1ms/sample - loss: 0.1826\n",
      "Epoch 75/100\n",
      "4/4 [==============================] - 0s 1ms/sample - loss: 0.1824\n",
      "Epoch 76/100\n",
      "4/4 [==============================] - 0s 2ms/sample - loss: 0.1822\n",
      "Epoch 77/100\n",
      "4/4 [==============================] - 0s 2ms/sample - loss: 0.1820\n",
      "Epoch 78/100\n",
      "4/4 [==============================] - 0s 2ms/sample - loss: 0.1818\n",
      "Epoch 79/100\n",
      "4/4 [==============================] - 0s 1ms/sample - loss: 0.1816\n",
      "Epoch 80/100\n",
      "4/4 [==============================] - 0s 935us/sample - loss: 0.1814\n",
      "Epoch 81/100\n",
      "4/4 [==============================] - 0s 1ms/sample - loss: 0.1812\n",
      "Epoch 82/100\n",
      "4/4 [==============================] - 0s 1ms/sample - loss: 0.1810\n",
      "Epoch 83/100\n",
      "4/4 [==============================] - 0s 858us/sample - loss: 0.1808\n",
      "Epoch 84/100\n",
      "4/4 [==============================] - 0s 4ms/sample - loss: 0.1807\n",
      "Epoch 85/100\n",
      "4/4 [==============================] - 0s 1ms/sample - loss: 0.1805\n",
      "Epoch 86/100\n",
      "4/4 [==============================] - 0s 1ms/sample - loss: 0.1803\n",
      "Epoch 87/100\n",
      "4/4 [==============================] - 0s 2ms/sample - loss: 0.1801\n",
      "Epoch 88/100\n",
      "4/4 [==============================] - 0s 3ms/sample - loss: 0.1799\n",
      "Epoch 89/100\n",
      "4/4 [==============================] - 0s 1ms/sample - loss: 0.1797\n",
      "Epoch 90/100\n",
      "4/4 [==============================] - 0s 2ms/sample - loss: 0.1795\n",
      "Epoch 91/100\n",
      "4/4 [==============================] - 0s 2ms/sample - loss: 0.1793\n",
      "Epoch 92/100\n",
      "4/4 [==============================] - 0s 1ms/sample - loss: 0.1791\n",
      "Epoch 93/100\n",
      "4/4 [==============================] - 0s 3ms/sample - loss: 0.1789\n",
      "Epoch 94/100\n",
      "4/4 [==============================] - 0s 759us/sample - loss: 0.1787\n",
      "Epoch 95/100\n",
      "4/4 [==============================] - 0s 3ms/sample - loss: 0.1786\n",
      "Epoch 96/100\n",
      "4/4 [==============================] - 0s 3ms/sample - loss: 0.1784\n",
      "Epoch 97/100\n",
      "4/4 [==============================] - 0s 4ms/sample - loss: 0.1782\n",
      "Epoch 98/100\n",
      "4/4 [==============================] - 0s 2ms/sample - loss: 0.1780\n",
      "Epoch 99/100\n",
      "4/4 [==============================] - 0s 4ms/sample - loss: 0.1778\n",
      "Epoch 100/100\n",
      "4/4 [==============================] - 0s 1ms/sample - loss: 0.1776\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "import numpy as np\n",
    "\n",
    "data = np.genfromtxt('data_lir.csv', delimiter=',')\n",
    "X = data[:, 0:1]\n",
    "y = data[:, 1:]\n",
    "\n",
    "#create model\n",
    "lir_model = keras.Sequential([keras.layers.Dense(units=1, input_shape=[1])])\n",
    "\n",
    "#initial weight \n",
    "lir_model.layers[0].set_weights([np.array([[0.5]]), np.array([1.0])])\n",
    "\n",
    "#compile model\n",
    "opt = keras.optimizers.SGD(learning_rate=0.01)\n",
    "lir_model.compile(optimizer=opt, loss='mse')\n",
    "\n",
    "#training\n",
    "batch_size = 4\n",
    "epochs = 100\n",
    "history = lir_model.fit(X, y, batch_size, epochs)\n",
    "\n",
    "#save model\n",
    "lir_model_path = \"lir_model.h5\"\n",
    "lir_model.save(lir_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[6.5581503]]\n"
     ]
    }
   ],
   "source": [
    "# load model\n",
    "lir_model_path = tf.keras.models.load_model('lir_model.h5')\n",
    "\n",
    "# testing\n",
    "X_testing = [[5.0]]\n",
    "y_hat = lir_model_path.predict(X_testing)\n",
    "print(y_hat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b) Logistic-Engineer-Iris.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 100 samples\n",
      "Epoch 1/100\n",
      "100/100 [==============================] - 0s 3ms/sample - loss: 1.0535\n",
      "Epoch 2/100\n",
      "100/100 [==============================] - 0s 512us/sample - loss: 0.2738\n",
      "Epoch 3/100\n",
      "100/100 [==============================] - 0s 483us/sample - loss: 0.2190\n",
      "Epoch 4/100\n",
      "100/100 [==============================] - 0s 486us/sample - loss: 0.2024\n",
      "Epoch 5/100\n",
      "100/100 [==============================] - 0s 527us/sample - loss: 0.1892\n",
      "Epoch 6/100\n",
      "100/100 [==============================] - 0s 517us/sample - loss: 0.1775\n",
      "Epoch 7/100\n",
      "100/100 [==============================] - 0s 452us/sample - loss: 0.1670\n",
      "Epoch 8/100\n",
      "100/100 [==============================] - 0s 515us/sample - loss: 0.1577\n",
      "Epoch 9/100\n",
      "100/100 [==============================] - 0s 454us/sample - loss: 0.1502\n",
      "Epoch 10/100\n",
      "100/100 [==============================] - 0s 590us/sample - loss: 0.1424\n",
      "Epoch 11/100\n",
      "100/100 [==============================] - 0s 831us/sample - loss: 0.1357\n",
      "Epoch 12/100\n",
      "100/100 [==============================] - 0s 1ms/sample - loss: 0.1295\n",
      "Epoch 13/100\n",
      "100/100 [==============================] - 0s 843us/sample - loss: 0.1237\n",
      "Epoch 14/100\n",
      "100/100 [==============================] - 0s 487us/sample - loss: 0.1187\n",
      "Epoch 15/100\n",
      "100/100 [==============================] - 0s 450us/sample - loss: 0.1138\n",
      "Epoch 16/100\n",
      "100/100 [==============================] - 0s 489us/sample - loss: 0.1094\n",
      "Epoch 17/100\n",
      "100/100 [==============================] - 0s 509us/sample - loss: 0.1057\n",
      "Epoch 18/100\n",
      "100/100 [==============================] - 0s 681us/sample - loss: 0.1018\n",
      "Epoch 19/100\n",
      "100/100 [==============================] - 0s 540us/sample - loss: 0.0982\n",
      "Epoch 20/100\n",
      "100/100 [==============================] - 0s 493us/sample - loss: 0.0950\n",
      "Epoch 21/100\n",
      "100/100 [==============================] - 0s 546us/sample - loss: 0.0918\n",
      "Epoch 22/100\n",
      "100/100 [==============================] - 0s 459us/sample - loss: 0.0889\n",
      "Epoch 23/100\n",
      "100/100 [==============================] - 0s 427us/sample - loss: 0.0863\n",
      "Epoch 24/100\n",
      "100/100 [==============================] - 0s 447us/sample - loss: 0.0839\n",
      "Epoch 25/100\n",
      "100/100 [==============================] - 0s 524us/sample - loss: 0.0815\n",
      "Epoch 26/100\n",
      "100/100 [==============================] - 0s 489us/sample - loss: 0.0792\n",
      "Epoch 27/100\n",
      "100/100 [==============================] - 0s 621us/sample - loss: 0.0770\n",
      "Epoch 28/100\n",
      "100/100 [==============================] - 0s 660us/sample - loss: 0.0749\n",
      "Epoch 29/100\n",
      "100/100 [==============================] - 0s 474us/sample - loss: 0.0730\n",
      "Epoch 30/100\n",
      "100/100 [==============================] - 0s 637us/sample - loss: 0.0713\n",
      "Epoch 31/100\n",
      "100/100 [==============================] - 0s 511us/sample - loss: 0.0695\n",
      "Epoch 32/100\n",
      "100/100 [==============================] - 0s 478us/sample - loss: 0.0680\n",
      "Epoch 33/100\n",
      "100/100 [==============================] - 0s 427us/sample - loss: 0.0664\n",
      "Epoch 34/100\n",
      "100/100 [==============================] - 0s 464us/sample - loss: 0.0648\n",
      "Epoch 35/100\n",
      "100/100 [==============================] - 0s 448us/sample - loss: 0.0634\n",
      "Epoch 36/100\n",
      "100/100 [==============================] - 0s 449us/sample - loss: 0.0622\n",
      "Epoch 37/100\n",
      "100/100 [==============================] - 0s 512us/sample - loss: 0.0607\n",
      "Epoch 38/100\n",
      "100/100 [==============================] - 0s 480us/sample - loss: 0.0596\n",
      "Epoch 39/100\n",
      "100/100 [==============================] - 0s 504us/sample - loss: 0.0583\n",
      "Epoch 40/100\n",
      "100/100 [==============================] - 0s 523us/sample - loss: 0.0571\n",
      "Epoch 41/100\n",
      "100/100 [==============================] - 0s 600us/sample - loss: 0.0560\n",
      "Epoch 42/100\n",
      "100/100 [==============================] - 0s 441us/sample - loss: 0.0549\n",
      "Epoch 43/100\n",
      "100/100 [==============================] - 0s 615us/sample - loss: 0.0540\n",
      "Epoch 44/100\n",
      "100/100 [==============================] - 0s 445us/sample - loss: 0.0530\n",
      "Epoch 45/100\n",
      "100/100 [==============================] - 0s 519us/sample - loss: 0.0520\n",
      "Epoch 46/100\n",
      "100/100 [==============================] - 0s 516us/sample - loss: 0.0512\n",
      "Epoch 47/100\n",
      "100/100 [==============================] - 0s 513us/sample - loss: 0.0502\n",
      "Epoch 48/100\n",
      "100/100 [==============================] - 0s 500us/sample - loss: 0.0494\n",
      "Epoch 49/100\n",
      "100/100 [==============================] - 0s 399us/sample - loss: 0.0486\n",
      "Epoch 50/100\n",
      "100/100 [==============================] - 0s 583us/sample - loss: 0.0477\n",
      "Epoch 51/100\n",
      "100/100 [==============================] - 0s 742us/sample - loss: 0.0470\n",
      "Epoch 52/100\n",
      "100/100 [==============================] - 0s 493us/sample - loss: 0.0463\n",
      "Epoch 53/100\n",
      "100/100 [==============================] - 0s 514us/sample - loss: 0.0455\n",
      "Epoch 54/100\n",
      "100/100 [==============================] - 0s 632us/sample - loss: 0.0448\n",
      "Epoch 55/100\n",
      "100/100 [==============================] - 0s 609us/sample - loss: 0.0442\n",
      "Epoch 56/100\n",
      "100/100 [==============================] - 0s 626us/sample - loss: 0.0435\n",
      "Epoch 57/100\n",
      "100/100 [==============================] - 0s 799us/sample - loss: 0.0429\n",
      "Epoch 58/100\n",
      "100/100 [==============================] - 0s 981us/sample - loss: 0.0423\n",
      "Epoch 59/100\n",
      "100/100 [==============================] - 0s 505us/sample - loss: 0.0416\n",
      "Epoch 60/100\n",
      "100/100 [==============================] - 0s 451us/sample - loss: 0.0411\n",
      "Epoch 61/100\n",
      "100/100 [==============================] - 0s 437us/sample - loss: 0.0405\n",
      "Epoch 62/100\n",
      "100/100 [==============================] - 0s 729us/sample - loss: 0.0399\n",
      "Epoch 63/100\n",
      "100/100 [==============================] - 0s 914us/sample - loss: 0.0394\n",
      "Epoch 64/100\n",
      "100/100 [==============================] - 0s 767us/sample - loss: 0.0389\n",
      "Epoch 65/100\n",
      "100/100 [==============================] - 0s 914us/sample - loss: 0.0384\n",
      "Epoch 66/100\n",
      "100/100 [==============================] - 0s 577us/sample - loss: 0.0379\n",
      "Epoch 67/100\n",
      "100/100 [==============================] - 0s 674us/sample - loss: 0.0375\n",
      "Epoch 68/100\n",
      "100/100 [==============================] - 0s 444us/sample - loss: 0.0370\n",
      "Epoch 69/100\n",
      "100/100 [==============================] - 0s 562us/sample - loss: 0.0365\n",
      "Epoch 70/100\n",
      "100/100 [==============================] - 0s 538us/sample - loss: 0.0361\n",
      "Epoch 71/100\n",
      "100/100 [==============================] - 0s 428us/sample - loss: 0.0356\n",
      "Epoch 72/100\n",
      "100/100 [==============================] - 0s 656us/sample - loss: 0.0352\n",
      "Epoch 73/100\n",
      "100/100 [==============================] - 0s 637us/sample - loss: 0.0349\n",
      "Epoch 74/100\n",
      "100/100 [==============================] - 0s 468us/sample - loss: 0.0344\n",
      "Epoch 75/100\n",
      "100/100 [==============================] - 0s 581us/sample - loss: 0.0340\n",
      "Epoch 76/100\n",
      "100/100 [==============================] - 0s 743us/sample - loss: 0.0336\n",
      "Epoch 77/100\n",
      "100/100 [==============================] - 0s 790us/sample - loss: 0.0332\n",
      "Epoch 78/100\n",
      "100/100 [==============================] - 0s 671us/sample - loss: 0.0329\n",
      "Epoch 79/100\n",
      "100/100 [==============================] - 0s 816us/sample - loss: 0.0325\n",
      "Epoch 80/100\n",
      "100/100 [==============================] - 0s 739us/sample - loss: 0.0322\n",
      "Epoch 81/100\n",
      "100/100 [==============================] - 0s 705us/sample - loss: 0.0318\n",
      "Epoch 82/100\n",
      "100/100 [==============================] - 0s 637us/sample - loss: 0.0315\n",
      "Epoch 83/100\n",
      "100/100 [==============================] - 0s 616us/sample - loss: 0.0312\n",
      "Epoch 84/100\n",
      "100/100 [==============================] - 0s 569us/sample - loss: 0.0309\n",
      "Epoch 85/100\n",
      "100/100 [==============================] - 0s 633us/sample - loss: 0.0306\n",
      "Epoch 86/100\n",
      "100/100 [==============================] - 0s 629us/sample - loss: 0.0302\n",
      "Epoch 87/100\n",
      "100/100 [==============================] - 0s 650us/sample - loss: 0.0299\n",
      "Epoch 88/100\n",
      "100/100 [==============================] - 0s 589us/sample - loss: 0.0296\n",
      "Epoch 89/100\n",
      "100/100 [==============================] - 0s 500us/sample - loss: 0.0293\n",
      "Epoch 90/100\n",
      "100/100 [==============================] - 0s 457us/sample - loss: 0.0291\n",
      "Epoch 91/100\n",
      "100/100 [==============================] - 0s 680us/sample - loss: 0.0288\n",
      "Epoch 92/100\n",
      "100/100 [==============================] - 0s 644us/sample - loss: 0.0285\n",
      "Epoch 93/100\n",
      "100/100 [==============================] - 0s 841us/sample - loss: 0.0283\n",
      "Epoch 94/100\n",
      "100/100 [==============================] - 0s 1ms/sample - loss: 0.0280\n",
      "Epoch 95/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/100 [==============================] - 0s 447us/sample - loss: 0.0277\n",
      "Epoch 96/100\n",
      "100/100 [==============================] - 0s 526us/sample - loss: 0.0275\n",
      "Epoch 97/100\n",
      "100/100 [==============================] - 0s 489us/sample - loss: 0.0272\n",
      "Epoch 98/100\n",
      "100/100 [==============================] - 0s 357us/sample - loss: 0.0270\n",
      "Epoch 99/100\n",
      "100/100 [==============================] - 0s 416us/sample - loss: 0.0268\n",
      "Epoch 100/100\n",
      "100/100 [==============================] - 0s 628us/sample - loss: 0.0265\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "import numpy as np\n",
    "\n",
    "data = np.genfromtxt('iris_full_lor.csv', delimiter=',', skip_header=1)\n",
    "X = data[:, 0:4]\n",
    "y = data[:, 4:]\n",
    "\n",
    "#create model\n",
    "lor_model = keras.Sequential([keras.layers.Dense(units=1, input_shape=[4], activation='sigmoid')])\n",
    "\n",
    "#initial parameters\n",
    "# lor_model.layers[0].set_weights([np.array([[-0.1], [-0.1], [-0.1], [-0.1]]), np.array([0.1])])\n",
    "lor_model.layers[0].set_weights([tf.random.uniform((4,1)), np.array([0.1])])\n",
    "\n",
    "#compile model\n",
    "opt = keras.optimizers.SGD(learning_rate=0.01)\n",
    "lor_model.compile(optimizer=opt, loss='binary_crossentropy')\n",
    "\n",
    "#training\n",
    "batch_size = 4\n",
    "epochs = 100\n",
    "history = lor_model.fit(X, y, batch_size, epochs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(lor_model.predict(X).round() == y).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(history.history['loss'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## c) Softmax-Engineer-Iris.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Có 2 cách để thực hiện là sử dụng label măc định dạng catgeorical thì dùng loss: sparse_categorical_crossentropy, còn nếu như label đã được convert sang one-hot-endcoding thì dung loss: categorical_crossentropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 150 samples\n",
      "Epoch 1/500\n",
      "150/150 [==============================] - 1s 5ms/sample - loss: 1.3168\n",
      "Epoch 2/500\n",
      "150/150 [==============================] - 0s 134us/sample - loss: 1.2657\n",
      "Epoch 3/500\n",
      "150/150 [==============================] - 0s 147us/sample - loss: 1.2179\n",
      "Epoch 4/500\n",
      "150/150 [==============================] - 0s 98us/sample - loss: 1.1734\n",
      "Epoch 5/500\n",
      "150/150 [==============================] - 0s 166us/sample - loss: 1.1317\n",
      "Epoch 6/500\n",
      "150/150 [==============================] - 0s 73us/sample - loss: 1.0937\n",
      "Epoch 7/500\n",
      "150/150 [==============================] - 0s 158us/sample - loss: 1.0575\n",
      "Epoch 8/500\n",
      "150/150 [==============================] - 0s 146us/sample - loss: 1.0240\n",
      "Epoch 9/500\n",
      "150/150 [==============================] - 0s 124us/sample - loss: 0.9927\n",
      "Epoch 10/500\n",
      "150/150 [==============================] - 0s 200us/sample - loss: 0.9637\n",
      "Epoch 11/500\n",
      "150/150 [==============================] - 0s 150us/sample - loss: 0.9363\n",
      "Epoch 12/500\n",
      "150/150 [==============================] - 0s 129us/sample - loss: 0.9110\n",
      "Epoch 13/500\n",
      "150/150 [==============================] - 0s 107us/sample - loss: 0.8874\n",
      "Epoch 14/500\n",
      "150/150 [==============================] - 0s 66us/sample - loss: 0.8652\n",
      "Epoch 15/500\n",
      "150/150 [==============================] - 0s 87us/sample - loss: 0.8444\n",
      "Epoch 16/500\n",
      "150/150 [==============================] - 0s 119us/sample - loss: 0.8252\n",
      "Epoch 17/500\n",
      "150/150 [==============================] - 0s 104us/sample - loss: 0.8067\n",
      "Epoch 18/500\n",
      "150/150 [==============================] - 0s 88us/sample - loss: 0.7893\n",
      "Epoch 19/500\n",
      "150/150 [==============================] - 0s 141us/sample - loss: 0.7726\n",
      "Epoch 20/500\n",
      "150/150 [==============================] - 0s 134us/sample - loss: 0.7573\n",
      "Epoch 21/500\n",
      "150/150 [==============================] - 0s 152us/sample - loss: 0.7428\n",
      "Epoch 22/500\n",
      "150/150 [==============================] - 0s 141us/sample - loss: 0.7291\n",
      "Epoch 23/500\n",
      "150/150 [==============================] - 0s 87us/sample - loss: 0.7157\n",
      "Epoch 24/500\n",
      "150/150 [==============================] - 0s 94us/sample - loss: 0.7036\n",
      "Epoch 25/500\n",
      "150/150 [==============================] - 0s 124us/sample - loss: 0.6920\n",
      "Epoch 26/500\n",
      "150/150 [==============================] - 0s 76us/sample - loss: 0.6808\n",
      "Epoch 27/500\n",
      "150/150 [==============================] - 0s 88us/sample - loss: 0.6704\n",
      "Epoch 28/500\n",
      "150/150 [==============================] - 0s 118us/sample - loss: 0.6604\n",
      "Epoch 29/500\n",
      "150/150 [==============================] - 0s 131us/sample - loss: 0.6509\n",
      "Epoch 30/500\n",
      "150/150 [==============================] - 0s 138us/sample - loss: 0.6418\n",
      "Epoch 31/500\n",
      "150/150 [==============================] - 0s 131us/sample - loss: 0.6333\n",
      "Epoch 32/500\n",
      "150/150 [==============================] - 0s 126us/sample - loss: 0.6256\n",
      "Epoch 33/500\n",
      "150/150 [==============================] - 0s 168us/sample - loss: 0.6175\n",
      "Epoch 34/500\n",
      "150/150 [==============================] - 0s 115us/sample - loss: 0.6102\n",
      "Epoch 35/500\n",
      "150/150 [==============================] - 0s 230us/sample - loss: 0.6031\n",
      "Epoch 36/500\n",
      "150/150 [==============================] - 0s 211us/sample - loss: 0.5965\n",
      "Epoch 37/500\n",
      "150/150 [==============================] - 0s 214us/sample - loss: 0.5902\n",
      "Epoch 38/500\n",
      "150/150 [==============================] - 0s 220us/sample - loss: 0.5840\n",
      "Epoch 39/500\n",
      "150/150 [==============================] - 0s 186us/sample - loss: 0.5780\n",
      "Epoch 40/500\n",
      "150/150 [==============================] - 0s 96us/sample - loss: 0.5724\n",
      "Epoch 41/500\n",
      "150/150 [==============================] - 0s 138us/sample - loss: 0.5668\n",
      "Epoch 42/500\n",
      "150/150 [==============================] - 0s 152us/sample - loss: 0.5615\n",
      "Epoch 43/500\n",
      "150/150 [==============================] - 0s 81us/sample - loss: 0.5566\n",
      "Epoch 44/500\n",
      "150/150 [==============================] - 0s 141us/sample - loss: 0.5518\n",
      "Epoch 45/500\n",
      "150/150 [==============================] - 0s 97us/sample - loss: 0.5473\n",
      "Epoch 46/500\n",
      "150/150 [==============================] - 0s 150us/sample - loss: 0.5427\n",
      "Epoch 47/500\n",
      "150/150 [==============================] - 0s 135us/sample - loss: 0.5385\n",
      "Epoch 48/500\n",
      "150/150 [==============================] - 0s 133us/sample - loss: 0.5344\n",
      "Epoch 49/500\n",
      "150/150 [==============================] - 0s 112us/sample - loss: 0.5303\n",
      "Epoch 50/500\n",
      "150/150 [==============================] - 0s 135us/sample - loss: 0.5266\n",
      "Epoch 51/500\n",
      "150/150 [==============================] - 0s 82us/sample - loss: 0.5228\n",
      "Epoch 52/500\n",
      "150/150 [==============================] - 0s 182us/sample - loss: 0.5192\n",
      "Epoch 53/500\n",
      "150/150 [==============================] - 0s 128us/sample - loss: 0.5156\n",
      "Epoch 54/500\n",
      "150/150 [==============================] - 0s 117us/sample - loss: 0.5123\n",
      "Epoch 55/500\n",
      "150/150 [==============================] - 0s 141us/sample - loss: 0.5090\n",
      "Epoch 56/500\n",
      "150/150 [==============================] - 0s 104us/sample - loss: 0.5058\n",
      "Epoch 57/500\n",
      "150/150 [==============================] - 0s 101us/sample - loss: 0.5028\n",
      "Epoch 58/500\n",
      "150/150 [==============================] - 0s 118us/sample - loss: 0.4996\n",
      "Epoch 59/500\n",
      "150/150 [==============================] - 0s 125us/sample - loss: 0.4967\n",
      "Epoch 60/500\n",
      "150/150 [==============================] - 0s 114us/sample - loss: 0.4940\n",
      "Epoch 61/500\n",
      "150/150 [==============================] - 0s 76us/sample - loss: 0.4911\n",
      "Epoch 62/500\n",
      "150/150 [==============================] - 0s 76us/sample - loss: 0.4884\n",
      "Epoch 63/500\n",
      "150/150 [==============================] - 0s 81us/sample - loss: 0.4857\n",
      "Epoch 64/500\n",
      "150/150 [==============================] - 0s 87us/sample - loss: 0.4833\n",
      "Epoch 65/500\n",
      "150/150 [==============================] - 0s 114us/sample - loss: 0.4806\n",
      "Epoch 66/500\n",
      "150/150 [==============================] - 0s 132us/sample - loss: 0.4781\n",
      "Epoch 67/500\n",
      "150/150 [==============================] - 0s 144us/sample - loss: 0.4758\n",
      "Epoch 68/500\n",
      "150/150 [==============================] - 0s 135us/sample - loss: 0.4734\n",
      "Epoch 69/500\n",
      "150/150 [==============================] - 0s 118us/sample - loss: 0.4711\n",
      "Epoch 70/500\n",
      "150/150 [==============================] - 0s 133us/sample - loss: 0.4689\n",
      "Epoch 71/500\n",
      "150/150 [==============================] - ETA: 0s - loss: 0.383 - 0s 115us/sample - loss: 0.4668\n",
      "Epoch 72/500\n",
      "150/150 [==============================] - 0s 230us/sample - loss: 0.4646\n",
      "Epoch 73/500\n",
      "150/150 [==============================] - 0s 115us/sample - loss: 0.4625\n",
      "Epoch 74/500\n",
      "150/150 [==============================] - 0s 105us/sample - loss: 0.4605\n",
      "Epoch 75/500\n",
      "150/150 [==============================] - 0s 108us/sample - loss: 0.4585\n",
      "Epoch 76/500\n",
      "150/150 [==============================] - 0s 224us/sample - loss: 0.4566\n",
      "Epoch 77/500\n",
      "150/150 [==============================] - 0s 131us/sample - loss: 0.4546\n",
      "Epoch 78/500\n",
      "150/150 [==============================] - 0s 125us/sample - loss: 0.4528\n",
      "Epoch 79/500\n",
      "150/150 [==============================] - 0s 142us/sample - loss: 0.4509\n",
      "Epoch 80/500\n",
      "150/150 [==============================] - 0s 156us/sample - loss: 0.4491\n",
      "Epoch 81/500\n",
      "150/150 [==============================] - 0s 176us/sample - loss: 0.4474\n",
      "Epoch 82/500\n",
      "150/150 [==============================] - 0s 141us/sample - loss: 0.4457\n",
      "Epoch 83/500\n",
      "150/150 [==============================] - 0s 87us/sample - loss: 0.4439\n",
      "Epoch 84/500\n",
      "150/150 [==============================] - 0s 136us/sample - loss: 0.4422\n",
      "Epoch 85/500\n",
      "150/150 [==============================] - 0s 100us/sample - loss: 0.4406\n",
      "Epoch 86/500\n",
      "150/150 [==============================] - 0s 142us/sample - loss: 0.4390\n",
      "Epoch 87/500\n",
      "150/150 [==============================] - 0s 104us/sample - loss: 0.4373\n",
      "Epoch 88/500\n",
      "150/150 [==============================] - 0s 111us/sample - loss: 0.4358\n",
      "Epoch 89/500\n",
      "150/150 [==============================] - 0s 90us/sample - loss: 0.4343\n",
      "Epoch 90/500\n",
      "150/150 [==============================] - 0s 102us/sample - loss: 0.4328\n",
      "Epoch 91/500\n",
      "150/150 [==============================] - 0s 95us/sample - loss: 0.4313\n",
      "Epoch 92/500\n",
      "150/150 [==============================] - 0s 85us/sample - loss: 0.4300\n",
      "Epoch 93/500\n",
      "150/150 [==============================] - 0s 124us/sample - loss: 0.4284\n",
      "Epoch 94/500\n",
      "150/150 [==============================] - 0s 120us/sample - loss: 0.4270\n",
      "Epoch 95/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "150/150 [==============================] - 0s 91us/sample - loss: 0.4256\n",
      "Epoch 96/500\n",
      "150/150 [==============================] - 0s 119us/sample - loss: 0.4244\n",
      "Epoch 97/500\n",
      "150/150 [==============================] - 0s 101us/sample - loss: 0.4229\n",
      "Epoch 98/500\n",
      "150/150 [==============================] - 0s 111us/sample - loss: 0.4215\n",
      "Epoch 99/500\n",
      "150/150 [==============================] - 0s 106us/sample - loss: 0.4202\n",
      "Epoch 100/500\n",
      "150/150 [==============================] - 0s 95us/sample - loss: 0.4189\n",
      "Epoch 101/500\n",
      "150/150 [==============================] - 0s 122us/sample - loss: 0.4178\n",
      "Epoch 102/500\n",
      "150/150 [==============================] - 0s 140us/sample - loss: 0.4164\n",
      "Epoch 103/500\n",
      "150/150 [==============================] - 0s 105us/sample - loss: 0.4152\n",
      "Epoch 104/500\n",
      "150/150 [==============================] - 0s 86us/sample - loss: 0.4140\n",
      "Epoch 105/500\n",
      "150/150 [==============================] - 0s 133us/sample - loss: 0.4127\n",
      "Epoch 106/500\n",
      "150/150 [==============================] - 0s 112us/sample - loss: 0.4116\n",
      "Epoch 107/500\n",
      "150/150 [==============================] - 0s 103us/sample - loss: 0.4104\n",
      "Epoch 108/500\n",
      "150/150 [==============================] - 0s 220us/sample - loss: 0.4092\n",
      "Epoch 109/500\n",
      "150/150 [==============================] - 0s 140us/sample - loss: 0.4082\n",
      "Epoch 110/500\n",
      "150/150 [==============================] - 0s 232us/sample - loss: 0.4070\n",
      "Epoch 111/500\n",
      "150/150 [==============================] - 0s 165us/sample - loss: 0.4059\n",
      "Epoch 112/500\n",
      "150/150 [==============================] - 0s 248us/sample - loss: 0.4048\n",
      "Epoch 113/500\n",
      "150/150 [==============================] - 0s 156us/sample - loss: 0.4038\n",
      "Epoch 114/500\n",
      "150/150 [==============================] - 0s 162us/sample - loss: 0.4026\n",
      "Epoch 115/500\n",
      "150/150 [==============================] - 0s 174us/sample - loss: 0.4016\n",
      "Epoch 116/500\n",
      "150/150 [==============================] - 0s 227us/sample - loss: 0.4006\n",
      "Epoch 117/500\n",
      "150/150 [==============================] - 0s 174us/sample - loss: 0.3995\n",
      "Epoch 118/500\n",
      "150/150 [==============================] - 0s 143us/sample - loss: 0.3985\n",
      "Epoch 119/500\n",
      "150/150 [==============================] - 0s 137us/sample - loss: 0.3976\n",
      "Epoch 120/500\n",
      "150/150 [==============================] - 0s 158us/sample - loss: 0.3965\n",
      "Epoch 121/500\n",
      "150/150 [==============================] - 0s 175us/sample - loss: 0.3955\n",
      "Epoch 122/500\n",
      "150/150 [==============================] - 0s 233us/sample - loss: 0.3945\n",
      "Epoch 123/500\n",
      "150/150 [==============================] - 0s 115us/sample - loss: 0.3935\n",
      "Epoch 124/500\n",
      "150/150 [==============================] - 0s 198us/sample - loss: 0.3926\n",
      "Epoch 125/500\n",
      "150/150 [==============================] - 0s 109us/sample - loss: 0.3917\n",
      "Epoch 126/500\n",
      "150/150 [==============================] - ETA: 0s - loss: 0.371 - 0s 90us/sample - loss: 0.3908\n",
      "Epoch 127/500\n",
      "150/150 [==============================] - 0s 115us/sample - loss: 0.3899\n",
      "Epoch 128/500\n",
      "150/150 [==============================] - 0s 124us/sample - loss: 0.3890\n",
      "Epoch 129/500\n",
      "150/150 [==============================] - 0s 77us/sample - loss: 0.3881\n",
      "Epoch 130/500\n",
      "150/150 [==============================] - 0s 112us/sample - loss: 0.3872\n",
      "Epoch 131/500\n",
      "150/150 [==============================] - 0s 100us/sample - loss: 0.3863\n",
      "Epoch 132/500\n",
      "150/150 [==============================] - 0s 87us/sample - loss: 0.3854\n",
      "Epoch 133/500\n",
      "150/150 [==============================] - 0s 116us/sample - loss: 0.3847\n",
      "Epoch 134/500\n",
      "150/150 [==============================] - 0s 113us/sample - loss: 0.3837\n",
      "Epoch 135/500\n",
      "150/150 [==============================] - 0s 88us/sample - loss: 0.3829\n",
      "Epoch 136/500\n",
      "150/150 [==============================] - 0s 153us/sample - loss: 0.3821\n",
      "Epoch 137/500\n",
      "150/150 [==============================] - 0s 131us/sample - loss: 0.3813\n",
      "Epoch 138/500\n",
      "150/150 [==============================] - 0s 130us/sample - loss: 0.3804\n",
      "Epoch 139/500\n",
      "150/150 [==============================] - 0s 136us/sample - loss: 0.3796\n",
      "Epoch 140/500\n",
      "150/150 [==============================] - 0s 107us/sample - loss: 0.3789\n",
      "Epoch 141/500\n",
      "150/150 [==============================] - 0s 171us/sample - loss: 0.3780\n",
      "Epoch 142/500\n",
      "150/150 [==============================] - 0s 275us/sample - loss: 0.3772\n",
      "Epoch 143/500\n",
      "150/150 [==============================] - 0s 88us/sample - loss: 0.3765\n",
      "Epoch 144/500\n",
      "150/150 [==============================] - 0s 142us/sample - loss: 0.3757\n",
      "Epoch 145/500\n",
      "150/150 [==============================] - 0s 188us/sample - loss: 0.3749\n",
      "Epoch 146/500\n",
      "150/150 [==============================] - 0s 153us/sample - loss: 0.3742\n",
      "Epoch 147/500\n",
      "150/150 [==============================] - 0s 120us/sample - loss: 0.3734\n",
      "Epoch 148/500\n",
      "150/150 [==============================] - 0s 107us/sample - loss: 0.3727\n",
      "Epoch 149/500\n",
      "150/150 [==============================] - 0s 105us/sample - loss: 0.3720\n",
      "Epoch 150/500\n",
      "150/150 [==============================] - 0s 88us/sample - loss: 0.3712\n",
      "Epoch 151/500\n",
      "150/150 [==============================] - 0s 92us/sample - loss: 0.3706\n",
      "Epoch 152/500\n",
      "150/150 [==============================] - 0s 96us/sample - loss: 0.3699\n",
      "Epoch 153/500\n",
      "150/150 [==============================] - 0s 98us/sample - loss: 0.3691\n",
      "Epoch 154/500\n",
      "150/150 [==============================] - 0s 94us/sample - loss: 0.3685\n",
      "Epoch 155/500\n",
      "150/150 [==============================] - 0s 81us/sample - loss: 0.3677\n",
      "Epoch 156/500\n",
      "150/150 [==============================] - 0s 87us/sample - loss: 0.3671\n",
      "Epoch 157/500\n",
      "150/150 [==============================] - 0s 114us/sample - loss: 0.3664\n",
      "Epoch 158/500\n",
      "150/150 [==============================] - 0s 76us/sample - loss: 0.3657\n",
      "Epoch 159/500\n",
      "150/150 [==============================] - 0s 83us/sample - loss: 0.3650\n",
      "Epoch 160/500\n",
      "150/150 [==============================] - 0s 97us/sample - loss: 0.3643\n",
      "Epoch 161/500\n",
      "150/150 [==============================] - 0s 72us/sample - loss: 0.3637\n",
      "Epoch 162/500\n",
      "150/150 [==============================] - 0s 99us/sample - loss: 0.3631\n",
      "Epoch 163/500\n",
      "150/150 [==============================] - 0s 104us/sample - loss: 0.3624\n",
      "Epoch 164/500\n",
      "150/150 [==============================] - 0s 66us/sample - loss: 0.3617\n",
      "Epoch 165/500\n",
      "150/150 [==============================] - 0s 88us/sample - loss: 0.3611\n",
      "Epoch 166/500\n",
      "150/150 [==============================] - 0s 59us/sample - loss: 0.3604\n",
      "Epoch 167/500\n",
      "150/150 [==============================] - 0s 73us/sample - loss: 0.3598\n",
      "Epoch 168/500\n",
      "150/150 [==============================] - 0s 125us/sample - loss: 0.3592\n",
      "Epoch 169/500\n",
      "150/150 [==============================] - 0s 96us/sample - loss: 0.3586\n",
      "Epoch 170/500\n",
      "150/150 [==============================] - 0s 94us/sample - loss: 0.3580\n",
      "Epoch 171/500\n",
      "150/150 [==============================] - 0s 85us/sample - loss: 0.3573\n",
      "Epoch 172/500\n",
      "150/150 [==============================] - 0s 94us/sample - loss: 0.3567\n",
      "Epoch 173/500\n",
      "150/150 [==============================] - 0s 121us/sample - loss: 0.3561\n",
      "Epoch 174/500\n",
      "150/150 [==============================] - 0s 109us/sample - loss: 0.3555\n",
      "Epoch 175/500\n",
      "150/150 [==============================] - 0s 114us/sample - loss: 0.3550\n",
      "Epoch 176/500\n",
      "150/150 [==============================] - 0s 106us/sample - loss: 0.3544\n",
      "Epoch 177/500\n",
      "150/150 [==============================] - 0s 108us/sample - loss: 0.3538\n",
      "Epoch 178/500\n",
      "150/150 [==============================] - 0s 102us/sample - loss: 0.3532\n",
      "Epoch 179/500\n",
      "150/150 [==============================] - 0s 101us/sample - loss: 0.3527\n",
      "Epoch 180/500\n",
      "150/150 [==============================] - 0s 100us/sample - loss: 0.3520\n",
      "Epoch 181/500\n",
      "150/150 [==============================] - 0s 113us/sample - loss: 0.3515\n",
      "Epoch 182/500\n",
      "150/150 [==============================] - 0s 87us/sample - loss: 0.3509\n",
      "Epoch 183/500\n",
      "150/150 [==============================] - 0s 70us/sample - loss: 0.3504\n",
      "Epoch 184/500\n",
      "150/150 [==============================] - 0s 70us/sample - loss: 0.3499\n",
      "Epoch 185/500\n",
      "150/150 [==============================] - 0s 71us/sample - loss: 0.3492\n",
      "Epoch 186/500\n",
      "150/150 [==============================] - 0s 107us/sample - loss: 0.3487\n",
      "Epoch 187/500\n",
      "150/150 [==============================] - 0s 82us/sample - loss: 0.3481\n",
      "Epoch 188/500\n",
      "150/150 [==============================] - 0s 61us/sample - loss: 0.3476\n",
      "Epoch 189/500\n",
      "150/150 [==============================] - 0s 67us/sample - loss: 0.3471\n",
      "Epoch 190/500\n",
      "150/150 [==============================] - 0s 108us/sample - loss: 0.3465\n",
      "Epoch 191/500\n",
      "150/150 [==============================] - 0s 104us/sample - loss: 0.3461\n",
      "Epoch 192/500\n",
      "150/150 [==============================] - 0s 97us/sample - loss: 0.3455\n",
      "Epoch 193/500\n",
      "150/150 [==============================] - 0s 244us/sample - loss: 0.3450\n",
      "Epoch 194/500\n",
      "150/150 [==============================] - 0s 110us/sample - loss: 0.3445\n",
      "Epoch 195/500\n",
      "150/150 [==============================] - 0s 118us/sample - loss: 0.3440\n",
      "Epoch 196/500\n",
      "150/150 [==============================] - 0s 96us/sample - loss: 0.3435\n",
      "Epoch 197/500\n",
      "150/150 [==============================] - 0s 94us/sample - loss: 0.3429\n",
      "Epoch 198/500\n",
      "150/150 [==============================] - 0s 82us/sample - loss: 0.3424\n",
      "Epoch 199/500\n",
      "150/150 [==============================] - 0s 114us/sample - loss: 0.3420\n",
      "Epoch 200/500\n",
      "150/150 [==============================] - 0s 96us/sample - loss: 0.3415\n",
      "Epoch 201/500\n",
      "150/150 [==============================] - 0s 95us/sample - loss: 0.3409\n",
      "Epoch 202/500\n",
      "150/150 [==============================] - 0s 115us/sample - loss: 0.3404\n",
      "Epoch 203/500\n",
      "150/150 [==============================] - 0s 133us/sample - loss: 0.3399\n",
      "Epoch 204/500\n",
      "150/150 [==============================] - 0s 103us/sample - loss: 0.3395\n",
      "Epoch 205/500\n",
      "150/150 [==============================] - 0s 102us/sample - loss: 0.3390\n",
      "Epoch 206/500\n",
      "150/150 [==============================] - 0s 99us/sample - loss: 0.3385\n",
      "Epoch 207/500\n",
      "150/150 [==============================] - 0s 118us/sample - loss: 0.3380\n",
      "Epoch 208/500\n",
      "150/150 [==============================] - 0s 127us/sample - loss: 0.3375\n",
      "Epoch 209/500\n",
      "150/150 [==============================] - 0s 125us/sample - loss: 0.3370\n",
      "Epoch 210/500\n",
      "150/150 [==============================] - 0s 112us/sample - loss: 0.3366\n",
      "Epoch 211/500\n",
      "150/150 [==============================] - 0s 136us/sample - loss: 0.3361\n",
      "Epoch 212/500\n",
      "150/150 [==============================] - 0s 134us/sample - loss: 0.3357\n",
      "Epoch 213/500\n",
      "150/150 [==============================] - 0s 153us/sample - loss: 0.3352\n",
      "Epoch 214/500\n",
      "150/150 [==============================] - 0s 123us/sample - loss: 0.3347\n",
      "Epoch 215/500\n",
      "150/150 [==============================] - 0s 97us/sample - loss: 0.3343\n",
      "Epoch 216/500\n",
      "150/150 [==============================] - 0s 92us/sample - loss: 0.3338\n",
      "Epoch 217/500\n",
      "150/150 [==============================] - 0s 89us/sample - loss: 0.3334\n",
      "Epoch 218/500\n",
      "150/150 [==============================] - 0s 102us/sample - loss: 0.3329\n",
      "Epoch 219/500\n",
      "150/150 [==============================] - 0s 105us/sample - loss: 0.3324\n",
      "Epoch 220/500\n",
      "150/150 [==============================] - 0s 105us/sample - loss: 0.3320\n",
      "Epoch 221/500\n",
      "150/150 [==============================] - 0s 99us/sample - loss: 0.3315\n",
      "Epoch 222/500\n",
      "150/150 [==============================] - 0s 120us/sample - loss: 0.3311\n",
      "Epoch 223/500\n",
      "150/150 [==============================] - 0s 110us/sample - loss: 0.3307\n",
      "Epoch 224/500\n",
      "150/150 [==============================] - 0s 104us/sample - loss: 0.3303\n",
      "Epoch 225/500\n",
      "150/150 [==============================] - 0s 100us/sample - loss: 0.3299\n",
      "Epoch 226/500\n",
      "150/150 [==============================] - 0s 119us/sample - loss: 0.3294\n",
      "Epoch 227/500\n",
      "150/150 [==============================] - 0s 103us/sample - loss: 0.3289\n",
      "Epoch 228/500\n",
      "150/150 [==============================] - 0s 111us/sample - loss: 0.3285\n",
      "Epoch 229/500\n",
      "150/150 [==============================] - 0s 109us/sample - loss: 0.3282\n",
      "Epoch 230/500\n",
      "150/150 [==============================] - 0s 137us/sample - loss: 0.3277\n",
      "Epoch 231/500\n",
      "150/150 [==============================] - 0s 112us/sample - loss: 0.3273\n",
      "Epoch 232/500\n",
      "150/150 [==============================] - 0s 93us/sample - loss: 0.3269\n",
      "Epoch 233/500\n",
      "150/150 [==============================] - 0s 121us/sample - loss: 0.3265\n",
      "Epoch 234/500\n",
      "150/150 [==============================] - 0s 121us/sample - loss: 0.3261\n",
      "Epoch 235/500\n",
      "150/150 [==============================] - 0s 136us/sample - loss: 0.3257\n",
      "Epoch 236/500\n",
      "150/150 [==============================] - 0s 129us/sample - loss: 0.3253\n",
      "Epoch 237/500\n",
      "150/150 [==============================] - 0s 149us/sample - loss: 0.3249\n",
      "Epoch 238/500\n",
      "150/150 [==============================] - 0s 94us/sample - loss: 0.3245\n",
      "Epoch 239/500\n",
      "150/150 [==============================] - 0s 99us/sample - loss: 0.3240\n",
      "Epoch 240/500\n",
      "150/150 [==============================] - 0s 120us/sample - loss: 0.3236\n",
      "Epoch 241/500\n",
      "150/150 [==============================] - 0s 91us/sample - loss: 0.3232\n",
      "Epoch 242/500\n",
      "150/150 [==============================] - 0s 92us/sample - loss: 0.3229\n",
      "Epoch 243/500\n",
      "150/150 [==============================] - 0s 88us/sample - loss: 0.3224\n",
      "Epoch 244/500\n",
      "150/150 [==============================] - 0s 80us/sample - loss: 0.3221\n",
      "Epoch 245/500\n",
      "150/150 [==============================] - 0s 165us/sample - loss: 0.3217\n",
      "Epoch 246/500\n",
      "150/150 [==============================] - 0s 104us/sample - loss: 0.3213\n",
      "Epoch 247/500\n",
      "150/150 [==============================] - 0s 92us/sample - loss: 0.3209\n",
      "Epoch 248/500\n",
      "150/150 [==============================] - 0s 83us/sample - loss: 0.3205\n",
      "Epoch 249/500\n",
      "150/150 [==============================] - 0s 90us/sample - loss: 0.3201\n",
      "Epoch 250/500\n",
      "150/150 [==============================] - 0s 119us/sample - loss: 0.3198\n",
      "Epoch 251/500\n",
      "150/150 [==============================] - 0s 85us/sample - loss: 0.3194\n",
      "Epoch 252/500\n",
      "150/150 [==============================] - 0s 101us/sample - loss: 0.3190\n",
      "Epoch 253/500\n",
      "150/150 [==============================] - 0s 115us/sample - loss: 0.3186\n",
      "Epoch 254/500\n",
      "150/150 [==============================] - 0s 113us/sample - loss: 0.3182\n",
      "Epoch 255/500\n",
      "150/150 [==============================] - 0s 97us/sample - loss: 0.3179\n",
      "Epoch 256/500\n",
      "150/150 [==============================] - 0s 101us/sample - loss: 0.3175\n",
      "Epoch 257/500\n",
      "150/150 [==============================] - 0s 87us/sample - loss: 0.3171\n",
      "Epoch 258/500\n",
      "150/150 [==============================] - 0s 104us/sample - loss: 0.3168\n",
      "Epoch 259/500\n",
      "150/150 [==============================] - 0s 105us/sample - loss: 0.3164\n",
      "Epoch 260/500\n",
      "150/150 [==============================] - 0s 111us/sample - loss: 0.3160\n",
      "Epoch 261/500\n",
      "150/150 [==============================] - 0s 107us/sample - loss: 0.3156\n",
      "Epoch 262/500\n",
      "150/150 [==============================] - 0s 110us/sample - loss: 0.3153\n",
      "Epoch 263/500\n",
      "150/150 [==============================] - 0s 196us/sample - loss: 0.3149\n",
      "Epoch 264/500\n",
      "150/150 [==============================] - 0s 103us/sample - loss: 0.3146\n",
      "Epoch 265/500\n",
      "150/150 [==============================] - 0s 182us/sample - loss: 0.3142\n",
      "Epoch 266/500\n",
      "150/150 [==============================] - 0s 109us/sample - loss: 0.3139\n",
      "Epoch 267/500\n",
      "150/150 [==============================] - 0s 122us/sample - loss: 0.3135\n",
      "Epoch 268/500\n",
      "150/150 [==============================] - 0s 123us/sample - loss: 0.3131\n",
      "Epoch 269/500\n",
      "150/150 [==============================] - 0s 83us/sample - loss: 0.3128\n",
      "Epoch 270/500\n",
      "150/150 [==============================] - 0s 79us/sample - loss: 0.3125\n",
      "Epoch 271/500\n",
      "150/150 [==============================] - 0s 70us/sample - loss: 0.3121\n",
      "Epoch 272/500\n",
      "150/150 [==============================] - 0s 105us/sample - loss: 0.3117\n",
      "Epoch 273/500\n",
      "150/150 [==============================] - 0s 134us/sample - loss: 0.3114\n",
      "Epoch 274/500\n",
      "150/150 [==============================] - 0s 74us/sample - loss: 0.3111\n",
      "Epoch 275/500\n",
      "150/150 [==============================] - 0s 76us/sample - loss: 0.3107\n",
      "Epoch 276/500\n",
      "150/150 [==============================] - 0s 119us/sample - loss: 0.3104\n",
      "Epoch 277/500\n",
      "150/150 [==============================] - 0s 130us/sample - loss: 0.3101\n",
      "Epoch 278/500\n",
      "150/150 [==============================] - 0s 81us/sample - loss: 0.3097\n",
      "Epoch 279/500\n",
      "150/150 [==============================] - 0s 80us/sample - loss: 0.3094\n",
      "Epoch 280/500\n",
      "150/150 [==============================] - 0s 98us/sample - loss: 0.3091\n",
      "Epoch 281/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "150/150 [==============================] - 0s 106us/sample - loss: 0.3087\n",
      "Epoch 282/500\n",
      "150/150 [==============================] - 0s 83us/sample - loss: 0.3084\n",
      "Epoch 283/500\n",
      "150/150 [==============================] - 0s 104us/sample - loss: 0.3081\n",
      "Epoch 284/500\n",
      "150/150 [==============================] - 0s 86us/sample - loss: 0.3077\n",
      "Epoch 285/500\n",
      "150/150 [==============================] - 0s 94us/sample - loss: 0.3074\n",
      "Epoch 286/500\n",
      "150/150 [==============================] - 0s 107us/sample - loss: 0.3071\n",
      "Epoch 287/500\n",
      "150/150 [==============================] - 0s 86us/sample - loss: 0.3068\n",
      "Epoch 288/500\n",
      "150/150 [==============================] - 0s 85us/sample - loss: 0.3064\n",
      "Epoch 289/500\n",
      "150/150 [==============================] - 0s 97us/sample - loss: 0.3062\n",
      "Epoch 290/500\n",
      "150/150 [==============================] - 0s 90us/sample - loss: 0.3058\n",
      "Epoch 291/500\n",
      "150/150 [==============================] - 0s 74us/sample - loss: 0.3055\n",
      "Epoch 292/500\n",
      "150/150 [==============================] - 0s 88us/sample - loss: 0.3052\n",
      "Epoch 293/500\n",
      "150/150 [==============================] - 0s 93us/sample - loss: 0.3048\n",
      "Epoch 294/500\n",
      "150/150 [==============================] - 0s 89us/sample - loss: 0.3045\n",
      "Epoch 295/500\n",
      "150/150 [==============================] - 0s 59us/sample - loss: 0.3042\n",
      "Epoch 296/500\n",
      "150/150 [==============================] - 0s 52us/sample - loss: 0.3039\n",
      "Epoch 297/500\n",
      "150/150 [==============================] - 0s 52us/sample - loss: 0.3036\n",
      "Epoch 298/500\n",
      "150/150 [==============================] - 0s 64us/sample - loss: 0.3032\n",
      "Epoch 299/500\n",
      "150/150 [==============================] - 0s 61us/sample - loss: 0.3029\n",
      "Epoch 300/500\n",
      "150/150 [==============================] - 0s 45us/sample - loss: 0.3027\n",
      "Epoch 301/500\n",
      "150/150 [==============================] - 0s 46us/sample - loss: 0.3023\n",
      "Epoch 302/500\n",
      "150/150 [==============================] - 0s 51us/sample - loss: 0.3020\n",
      "Epoch 303/500\n",
      "150/150 [==============================] - 0s 67us/sample - loss: 0.3017\n",
      "Epoch 304/500\n",
      "150/150 [==============================] - 0s 50us/sample - loss: 0.3014\n",
      "Epoch 305/500\n",
      "150/150 [==============================] - 0s 58us/sample - loss: 0.3011\n",
      "Epoch 306/500\n",
      "150/150 [==============================] - 0s 52us/sample - loss: 0.3008\n",
      "Epoch 307/500\n",
      "150/150 [==============================] - 0s 43us/sample - loss: 0.3005\n",
      "Epoch 308/500\n",
      "150/150 [==============================] - 0s 43us/sample - loss: 0.3002\n",
      "Epoch 309/500\n",
      "150/150 [==============================] - 0s 49us/sample - loss: 0.2999\n",
      "Epoch 310/500\n",
      "150/150 [==============================] - 0s 46us/sample - loss: 0.2996\n",
      "Epoch 311/500\n",
      "150/150 [==============================] - 0s 77us/sample - loss: 0.2993\n",
      "Epoch 312/500\n",
      "150/150 [==============================] - 0s 46us/sample - loss: 0.2990\n",
      "Epoch 313/500\n",
      "150/150 [==============================] - 0s 369us/sample - loss: 0.2987\n",
      "Epoch 314/500\n",
      "150/150 [==============================] - 0s 121us/sample - loss: 0.2984\n",
      "Epoch 315/500\n",
      "150/150 [==============================] - 0s 179us/sample - loss: 0.2981\n",
      "Epoch 316/500\n",
      "150/150 [==============================] - 0s 91us/sample - loss: 0.2978\n",
      "Epoch 317/500\n",
      "150/150 [==============================] - 0s 110us/sample - loss: 0.2975\n",
      "Epoch 318/500\n",
      "150/150 [==============================] - 0s 96us/sample - loss: 0.2972\n",
      "Epoch 319/500\n",
      "150/150 [==============================] - 0s 178us/sample - loss: 0.2969\n",
      "Epoch 320/500\n",
      "150/150 [==============================] - 0s 129us/sample - loss: 0.2966\n",
      "Epoch 321/500\n",
      "150/150 [==============================] - 0s 155us/sample - loss: 0.2964\n",
      "Epoch 322/500\n",
      "150/150 [==============================] - 0s 138us/sample - loss: 0.2960\n",
      "Epoch 323/500\n",
      "150/150 [==============================] - 0s 109us/sample - loss: 0.2958\n",
      "Epoch 324/500\n",
      "150/150 [==============================] - 0s 111us/sample - loss: 0.2955\n",
      "Epoch 325/500\n",
      "150/150 [==============================] - 0s 95us/sample - loss: 0.2952\n",
      "Epoch 326/500\n",
      "150/150 [==============================] - 0s 156us/sample - loss: 0.2949\n",
      "Epoch 327/500\n",
      "150/150 [==============================] - 0s 189us/sample - loss: 0.2946\n",
      "Epoch 328/500\n",
      "150/150 [==============================] - 0s 127us/sample - loss: 0.2943\n",
      "Epoch 329/500\n",
      "150/150 [==============================] - 0s 193us/sample - loss: 0.2941\n",
      "Epoch 330/500\n",
      "150/150 [==============================] - 0s 128us/sample - loss: 0.2938\n",
      "Epoch 331/500\n",
      "150/150 [==============================] - 0s 111us/sample - loss: 0.2935\n",
      "Epoch 332/500\n",
      "150/150 [==============================] - 0s 82us/sample - loss: 0.2932\n",
      "Epoch 333/500\n",
      "150/150 [==============================] - 0s 95us/sample - loss: 0.2929\n",
      "Epoch 334/500\n",
      "150/150 [==============================] - 0s 122us/sample - loss: 0.2927\n",
      "Epoch 335/500\n",
      "150/150 [==============================] - 0s 79us/sample - loss: 0.2924\n",
      "Epoch 336/500\n",
      "150/150 [==============================] - 0s 82us/sample - loss: 0.2921\n",
      "Epoch 337/500\n",
      "150/150 [==============================] - 0s 67us/sample - loss: 0.2919\n",
      "Epoch 338/500\n",
      "150/150 [==============================] - 0s 69us/sample - loss: 0.2916\n",
      "Epoch 339/500\n",
      "150/150 [==============================] - 0s 99us/sample - loss: 0.2913\n",
      "Epoch 340/500\n",
      "150/150 [==============================] - 0s 104us/sample - loss: 0.2911\n",
      "Epoch 341/500\n",
      "150/150 [==============================] - 0s 113us/sample - loss: 0.2907\n",
      "Epoch 342/500\n",
      "150/150 [==============================] - 0s 91us/sample - loss: 0.2905\n",
      "Epoch 343/500\n",
      "150/150 [==============================] - 0s 124us/sample - loss: 0.2902\n",
      "Epoch 344/500\n",
      "150/150 [==============================] - 0s 94us/sample - loss: 0.2900\n",
      "Epoch 345/500\n",
      "150/150 [==============================] - 0s 69us/sample - loss: 0.2897\n",
      "Epoch 346/500\n",
      "150/150 [==============================] - 0s 80us/sample - loss: 0.2894\n",
      "Epoch 347/500\n",
      "150/150 [==============================] - 0s 101us/sample - loss: 0.2891\n",
      "Epoch 348/500\n",
      "150/150 [==============================] - 0s 95us/sample - loss: 0.2889\n",
      "Epoch 349/500\n",
      "150/150 [==============================] - 0s 106us/sample - loss: 0.2887\n",
      "Epoch 350/500\n",
      "150/150 [==============================] - 0s 108us/sample - loss: 0.2883\n",
      "Epoch 351/500\n",
      "150/150 [==============================] - 0s 110us/sample - loss: 0.2882\n",
      "Epoch 352/500\n",
      "150/150 [==============================] - 0s 105us/sample - loss: 0.2878\n",
      "Epoch 353/500\n",
      "150/150 [==============================] - 0s 84us/sample - loss: 0.2875\n",
      "Epoch 354/500\n",
      "150/150 [==============================] - 0s 108us/sample - loss: 0.2873\n",
      "Epoch 355/500\n",
      "150/150 [==============================] - 0s 89us/sample - loss: 0.2871\n",
      "Epoch 356/500\n",
      "150/150 [==============================] - 0s 100us/sample - loss: 0.2868\n",
      "Epoch 357/500\n",
      "150/150 [==============================] - 0s 101us/sample - loss: 0.2865\n",
      "Epoch 358/500\n",
      "150/150 [==============================] - 0s 74us/sample - loss: 0.2863\n",
      "Epoch 359/500\n",
      "150/150 [==============================] - 0s 100us/sample - loss: 0.2860\n",
      "Epoch 360/500\n",
      "150/150 [==============================] - 0s 66us/sample - loss: 0.2858\n",
      "Epoch 361/500\n",
      "150/150 [==============================] - 0s 105us/sample - loss: 0.2855\n",
      "Epoch 362/500\n",
      "150/150 [==============================] - 0s 97us/sample - loss: 0.2852\n",
      "Epoch 363/500\n",
      "150/150 [==============================] - 0s 110us/sample - loss: 0.2850\n",
      "Epoch 364/500\n",
      "150/150 [==============================] - 0s 100us/sample - loss: 0.2847\n",
      "Epoch 365/500\n",
      "150/150 [==============================] - 0s 101us/sample - loss: 0.2845\n",
      "Epoch 366/500\n",
      "150/150 [==============================] - 0s 57us/sample - loss: 0.2842\n",
      "Epoch 367/500\n",
      "150/150 [==============================] - 0s 81us/sample - loss: 0.2840\n",
      "Epoch 368/500\n",
      "150/150 [==============================] - 0s 76us/sample - loss: 0.2837\n",
      "Epoch 369/500\n",
      "150/150 [==============================] - 0s 72us/sample - loss: 0.2834\n",
      "Epoch 370/500\n",
      "150/150 [==============================] - 0s 94us/sample - loss: 0.2832\n",
      "Epoch 371/500\n",
      "150/150 [==============================] - 0s 56us/sample - loss: 0.2830\n",
      "Epoch 372/500\n",
      "150/150 [==============================] - 0s 76us/sample - loss: 0.2827\n",
      "Epoch 373/500\n",
      "150/150 [==============================] - 0s 98us/sample - loss: 0.2825\n",
      "Epoch 374/500\n",
      "150/150 [==============================] - 0s 118us/sample - loss: 0.2822\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 375/500\n",
      "150/150 [==============================] - 0s 86us/sample - loss: 0.2820\n",
      "Epoch 376/500\n",
      "150/150 [==============================] - 0s 68us/sample - loss: 0.2817\n",
      "Epoch 377/500\n",
      "150/150 [==============================] - 0s 53us/sample - loss: 0.2814\n",
      "Epoch 378/500\n",
      "150/150 [==============================] - 0s 119us/sample - loss: 0.2812\n",
      "Epoch 379/500\n",
      "150/150 [==============================] - 0s 88us/sample - loss: 0.2810\n",
      "Epoch 380/500\n",
      "150/150 [==============================] - 0s 93us/sample - loss: 0.2807\n",
      "Epoch 381/500\n",
      "150/150 [==============================] - 0s 98us/sample - loss: 0.2805\n",
      "Epoch 382/500\n",
      "150/150 [==============================] - 0s 111us/sample - loss: 0.2803\n",
      "Epoch 383/500\n",
      "150/150 [==============================] - 0s 106us/sample - loss: 0.2800\n",
      "Epoch 384/500\n",
      "150/150 [==============================] - 0s 102us/sample - loss: 0.2798\n",
      "Epoch 385/500\n",
      "150/150 [==============================] - 0s 101us/sample - loss: 0.2795\n",
      "Epoch 386/500\n",
      "150/150 [==============================] - 0s 94us/sample - loss: 0.2793\n",
      "Epoch 387/500\n",
      "150/150 [==============================] - 0s 78us/sample - loss: 0.2791\n",
      "Epoch 388/500\n",
      "150/150 [==============================] - 0s 104us/sample - loss: 0.2788\n",
      "Epoch 389/500\n",
      "150/150 [==============================] - 0s 92us/sample - loss: 0.2786\n",
      "Epoch 390/500\n",
      "150/150 [==============================] - 0s 79us/sample - loss: 0.2784\n",
      "Epoch 391/500\n",
      "150/150 [==============================] - 0s 113us/sample - loss: 0.2781\n",
      "Epoch 392/500\n",
      "150/150 [==============================] - 0s 126us/sample - loss: 0.2779\n",
      "Epoch 393/500\n",
      "150/150 [==============================] - 0s 119us/sample - loss: 0.2776\n",
      "Epoch 394/500\n",
      "150/150 [==============================] - 0s 122us/sample - loss: 0.2774\n",
      "Epoch 395/500\n",
      "150/150 [==============================] - 0s 96us/sample - loss: 0.2772\n",
      "Epoch 396/500\n",
      "150/150 [==============================] - 0s 74us/sample - loss: 0.2770\n",
      "Epoch 397/500\n",
      "150/150 [==============================] - 0s 90us/sample - loss: 0.2767\n",
      "Epoch 398/500\n",
      "150/150 [==============================] - 0s 88us/sample - loss: 0.2765\n",
      "Epoch 399/500\n",
      "150/150 [==============================] - 0s 119us/sample - loss: 0.2763\n",
      "Epoch 400/500\n",
      "150/150 [==============================] - 0s 104us/sample - loss: 0.2760\n",
      "Epoch 401/500\n",
      "150/150 [==============================] - 0s 87us/sample - loss: 0.2759\n",
      "Epoch 402/500\n",
      "150/150 [==============================] - 0s 112us/sample - loss: 0.2756\n",
      "Epoch 403/500\n",
      "150/150 [==============================] - 0s 84us/sample - loss: 0.2753\n",
      "Epoch 404/500\n",
      "150/150 [==============================] - 0s 79us/sample - loss: 0.2751\n",
      "Epoch 405/500\n",
      "150/150 [==============================] - 0s 96us/sample - loss: 0.2749\n",
      "Epoch 406/500\n",
      "150/150 [==============================] - 0s 92us/sample - loss: 0.2746\n",
      "Epoch 407/500\n",
      "150/150 [==============================] - 0s 88us/sample - loss: 0.2744\n",
      "Epoch 408/500\n",
      "150/150 [==============================] - 0s 79us/sample - loss: 0.2742\n",
      "Epoch 409/500\n",
      "150/150 [==============================] - 0s 83us/sample - loss: 0.2740\n",
      "Epoch 410/500\n",
      "150/150 [==============================] - 0s 108us/sample - loss: 0.2737\n",
      "Epoch 411/500\n",
      "150/150 [==============================] - 0s 120us/sample - loss: 0.2736\n",
      "Epoch 412/500\n",
      "150/150 [==============================] - 0s 89us/sample - loss: 0.2733\n",
      "Epoch 413/500\n",
      "150/150 [==============================] - 0s 62us/sample - loss: 0.2731\n",
      "Epoch 414/500\n",
      "150/150 [==============================] - 0s 113us/sample - loss: 0.2729\n",
      "Epoch 415/500\n",
      "150/150 [==============================] - 0s 110us/sample - loss: 0.2726\n",
      "Epoch 416/500\n",
      "150/150 [==============================] - 0s 95us/sample - loss: 0.2724\n",
      "Epoch 417/500\n",
      "150/150 [==============================] - 0s 89us/sample - loss: 0.2722\n",
      "Epoch 418/500\n",
      "150/150 [==============================] - 0s 78us/sample - loss: 0.2720\n",
      "Epoch 419/500\n",
      "150/150 [==============================] - 0s 92us/sample - loss: 0.2717\n",
      "Epoch 420/500\n",
      "150/150 [==============================] - 0s 78us/sample - loss: 0.2715\n",
      "Epoch 421/500\n",
      "150/150 [==============================] - 0s 88us/sample - loss: 0.2713\n",
      "Epoch 422/500\n",
      "150/150 [==============================] - 0s 96us/sample - loss: 0.2711\n",
      "Epoch 423/500\n",
      "150/150 [==============================] - 0s 85us/sample - loss: 0.2708\n",
      "Epoch 424/500\n",
      "150/150 [==============================] - 0s 78us/sample - loss: 0.2707\n",
      "Epoch 425/500\n",
      "150/150 [==============================] - 0s 108us/sample - loss: 0.2705\n",
      "Epoch 426/500\n",
      "150/150 [==============================] - 0s 104us/sample - loss: 0.2702\n",
      "Epoch 427/500\n",
      "150/150 [==============================] - 0s 108us/sample - loss: 0.2700\n",
      "Epoch 428/500\n",
      "150/150 [==============================] - 0s 86us/sample - loss: 0.2698\n",
      "Epoch 429/500\n",
      "150/150 [==============================] - 0s 96us/sample - loss: 0.2696\n",
      "Epoch 430/500\n",
      "150/150 [==============================] - 0s 119us/sample - loss: 0.2693\n",
      "Epoch 431/500\n",
      "150/150 [==============================] - 0s 69us/sample - loss: 0.2691\n",
      "Epoch 432/500\n",
      "150/150 [==============================] - 0s 102us/sample - loss: 0.2689\n",
      "Epoch 433/500\n",
      "150/150 [==============================] - 0s 60us/sample - loss: 0.2688\n",
      "Epoch 434/500\n",
      "150/150 [==============================] - 0s 92us/sample - loss: 0.2685\n",
      "Epoch 435/500\n",
      "150/150 [==============================] - 0s 83us/sample - loss: 0.2683\n",
      "Epoch 436/500\n",
      "150/150 [==============================] - 0s 85us/sample - loss: 0.2681\n",
      "Epoch 437/500\n",
      "150/150 [==============================] - 0s 60us/sample - loss: 0.2679\n",
      "Epoch 438/500\n",
      "150/150 [==============================] - 0s 93us/sample - loss: 0.2677\n",
      "Epoch 439/500\n",
      "150/150 [==============================] - 0s 101us/sample - loss: 0.2675\n",
      "Epoch 440/500\n",
      "150/150 [==============================] - 0s 106us/sample - loss: 0.2673\n",
      "Epoch 441/500\n",
      "150/150 [==============================] - 0s 96us/sample - loss: 0.2670\n",
      "Epoch 442/500\n",
      "150/150 [==============================] - 0s 101us/sample - loss: 0.2668\n",
      "Epoch 443/500\n",
      "150/150 [==============================] - 0s 103us/sample - loss: 0.2666\n",
      "Epoch 444/500\n",
      "150/150 [==============================] - 0s 94us/sample - loss: 0.2664\n",
      "Epoch 445/500\n",
      "150/150 [==============================] - 0s 101us/sample - loss: 0.2662\n",
      "Epoch 446/500\n",
      "150/150 [==============================] - 0s 99us/sample - loss: 0.2660\n",
      "Epoch 447/500\n",
      "150/150 [==============================] - 0s 88us/sample - loss: 0.2658\n",
      "Epoch 448/500\n",
      "150/150 [==============================] - 0s 95us/sample - loss: 0.2656\n",
      "Epoch 449/500\n",
      "150/150 [==============================] - 0s 93us/sample - loss: 0.2654\n",
      "Epoch 450/500\n",
      "150/150 [==============================] - 0s 76us/sample - loss: 0.2652\n",
      "Epoch 451/500\n",
      "150/150 [==============================] - 0s 102us/sample - loss: 0.2650\n",
      "Epoch 452/500\n",
      "150/150 [==============================] - 0s 96us/sample - loss: 0.2647\n",
      "Epoch 453/500\n",
      "150/150 [==============================] - 0s 91us/sample - loss: 0.2645\n",
      "Epoch 454/500\n",
      "150/150 [==============================] - 0s 76us/sample - loss: 0.2643\n",
      "Epoch 455/500\n",
      "150/150 [==============================] - 0s 90us/sample - loss: 0.2641\n",
      "Epoch 456/500\n",
      "150/150 [==============================] - 0s 107us/sample - loss: 0.2640\n",
      "Epoch 457/500\n",
      "150/150 [==============================] - 0s 94us/sample - loss: 0.2637\n",
      "Epoch 458/500\n",
      "150/150 [==============================] - 0s 104us/sample - loss: 0.2635\n",
      "Epoch 459/500\n",
      "150/150 [==============================] - 0s 94us/sample - loss: 0.2633\n",
      "Epoch 460/500\n",
      "150/150 [==============================] - 0s 103us/sample - loss: 0.2631\n",
      "Epoch 461/500\n",
      "150/150 [==============================] - 0s 83us/sample - loss: 0.2629\n",
      "Epoch 462/500\n",
      "150/150 [==============================] - 0s 103us/sample - loss: 0.2627\n",
      "Epoch 463/500\n",
      "150/150 [==============================] - 0s 110us/sample - loss: 0.2625\n",
      "Epoch 464/500\n",
      "150/150 [==============================] - 0s 94us/sample - loss: 0.2623\n",
      "Epoch 465/500\n",
      "150/150 [==============================] - 0s 108us/sample - loss: 0.2621\n",
      "Epoch 466/500\n",
      "150/150 [==============================] - 0s 111us/sample - loss: 0.2619\n",
      "Epoch 467/500\n",
      "150/150 [==============================] - 0s 95us/sample - loss: 0.2617\n",
      "Epoch 468/500\n",
      "150/150 [==============================] - 0s 94us/sample - loss: 0.2615\n",
      "Epoch 469/500\n",
      "150/150 [==============================] - 0s 105us/sample - loss: 0.2613\n",
      "Epoch 470/500\n",
      "150/150 [==============================] - 0s 98us/sample - loss: 0.2612\n",
      "Epoch 471/500\n",
      "150/150 [==============================] - 0s 90us/sample - loss: 0.2609\n",
      "Epoch 472/500\n",
      "150/150 [==============================] - 0s 72us/sample - loss: 0.2607\n",
      "Epoch 473/500\n",
      "150/150 [==============================] - 0s 84us/sample - loss: 0.2606\n",
      "Epoch 474/500\n",
      "150/150 [==============================] - 0s 101us/sample - loss: 0.2603\n",
      "Epoch 475/500\n",
      "150/150 [==============================] - 0s 84us/sample - loss: 0.2601\n",
      "Epoch 476/500\n",
      "150/150 [==============================] - 0s 112us/sample - loss: 0.2600\n",
      "Epoch 477/500\n",
      "150/150 [==============================] - 0s 114us/sample - loss: 0.2598\n",
      "Epoch 478/500\n",
      "150/150 [==============================] - 0s 80us/sample - loss: 0.2595\n",
      "Epoch 479/500\n",
      "150/150 [==============================] - 0s 94us/sample - loss: 0.2594\n",
      "Epoch 480/500\n",
      "150/150 [==============================] - 0s 107us/sample - loss: 0.2592\n",
      "Epoch 481/500\n",
      "150/150 [==============================] - 0s 83us/sample - loss: 0.2590\n",
      "Epoch 482/500\n",
      "150/150 [==============================] - 0s 92us/sample - loss: 0.2588\n",
      "Epoch 483/500\n",
      "150/150 [==============================] - 0s 86us/sample - loss: 0.2586\n",
      "Epoch 484/500\n",
      "150/150 [==============================] - 0s 90us/sample - loss: 0.2584\n",
      "Epoch 485/500\n",
      "150/150 [==============================] - 0s 86us/sample - loss: 0.2582\n",
      "Epoch 486/500\n",
      "150/150 [==============================] - 0s 86us/sample - loss: 0.2580\n",
      "Epoch 487/500\n",
      "150/150 [==============================] - 0s 92us/sample - loss: 0.2579\n",
      "Epoch 488/500\n",
      "150/150 [==============================] - 0s 80us/sample - loss: 0.2576\n",
      "Epoch 489/500\n",
      "150/150 [==============================] - 0s 100us/sample - loss: 0.2574\n",
      "Epoch 490/500\n",
      "150/150 [==============================] - 0s 137us/sample - loss: 0.2573\n",
      "Epoch 491/500\n",
      "150/150 [==============================] - 0s 99us/sample - loss: 0.2571\n",
      "Epoch 492/500\n",
      "150/150 [==============================] - 0s 115us/sample - loss: 0.2569\n",
      "Epoch 493/500\n",
      "150/150 [==============================] - 0s 103us/sample - loss: 0.2567\n",
      "Epoch 494/500\n",
      "150/150 [==============================] - 0s 119us/sample - loss: 0.2565\n",
      "Epoch 495/500\n",
      "150/150 [==============================] - 0s 106us/sample - loss: 0.2563\n",
      "Epoch 496/500\n",
      "150/150 [==============================] - 0s 98us/sample - loss: 0.2561\n",
      "Epoch 497/500\n",
      "150/150 [==============================] - 0s 82us/sample - loss: 0.2560\n",
      "Epoch 498/500\n",
      "150/150 [==============================] - 0s 97us/sample - loss: 0.2558\n",
      "Epoch 499/500\n",
      "150/150 [==============================] - 0s 87us/sample - loss: 0.2556\n",
      "Epoch 500/500\n",
      "150/150 [==============================] - 0s 90us/sample - loss: 0.2554\n"
     ]
    }
   ],
   "source": [
    "# pre-processing data\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "from sklearn import preprocessing\n",
    "\n",
    "tf.keras.backend.set_floatx('float64')\n",
    "\n",
    "\n",
    "batch_size = 40\n",
    "epochs = 500\n",
    "\n",
    "# Data Preparation\n",
    "data = np.genfromtxt('iris_full_sm.csv', delimiter=',', skip_header=1)\n",
    "X = data[:,0:4]\n",
    "y = data[:,4:]\n",
    "X = preprocessing.scale(X)\n",
    "\n",
    "# create model\n",
    "model = tf.keras.Sequential()\n",
    "model.add(tf.keras.Input(shape=(4,)))\n",
    "model.add(tf.keras.layers.Dense(units=3))\n",
    "model.add(keras.layers.Activation(keras.activations.softmax))\n",
    "\n",
    "# declare optimization method and loss function\n",
    "model.compile(optimizer='sgd', loss='sparse_categorical_crossentropy')\n",
    "\n",
    "# training\n",
    "history = model.fit(X, y, batch_size, epochs)\n",
    "\n",
    "# model.save('model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 150 samples\n",
      "Epoch 1/500\n",
      "150/150 [==============================] - 0s 2ms/sample - loss: 2.9605\n",
      "Epoch 2/500\n",
      "150/150 [==============================] - 0s 73us/sample - loss: 2.7386\n",
      "Epoch 3/500\n",
      "150/150 [==============================] - 0s 142us/sample - loss: 2.6326\n",
      "Epoch 4/500\n",
      "150/150 [==============================] - 0s 129us/sample - loss: 2.5613\n",
      "Epoch 5/500\n",
      "150/150 [==============================] - 0s 147us/sample - loss: 2.4866\n",
      "Epoch 6/500\n",
      "150/150 [==============================] - 0s 113us/sample - loss: 2.4087\n",
      "Epoch 7/500\n",
      "150/150 [==============================] - 0s 95us/sample - loss: 2.3336\n",
      "Epoch 8/500\n",
      "150/150 [==============================] - 0s 102us/sample - loss: 2.2604\n",
      "Epoch 9/500\n",
      "150/150 [==============================] - 0s 134us/sample - loss: 2.1903\n",
      "Epoch 10/500\n",
      "150/150 [==============================] - 0s 84us/sample - loss: 2.1199\n",
      "Epoch 11/500\n",
      "150/150 [==============================] - 0s 128us/sample - loss: 2.0495\n",
      "Epoch 12/500\n",
      "150/150 [==============================] - 0s 163us/sample - loss: 1.9794\n",
      "Epoch 13/500\n",
      "150/150 [==============================] - 0s 113us/sample - loss: 1.9139\n",
      "Epoch 14/500\n",
      "150/150 [==============================] - 0s 121us/sample - loss: 1.8558\n",
      "Epoch 15/500\n",
      "150/150 [==============================] - 0s 144us/sample - loss: 1.7879\n",
      "Epoch 16/500\n",
      "150/150 [==============================] - 0s 192us/sample - loss: 1.7249\n",
      "Epoch 17/500\n",
      "150/150 [==============================] - 0s 172us/sample - loss: 1.6721\n",
      "Epoch 18/500\n",
      "150/150 [==============================] - 0s 181us/sample - loss: 1.6060\n",
      "Epoch 19/500\n",
      "150/150 [==============================] - 0s 181us/sample - loss: 1.5616\n",
      "Epoch 20/500\n",
      "150/150 [==============================] - 0s 175us/sample - loss: 1.4963\n",
      "Epoch 21/500\n",
      "150/150 [==============================] - 0s 151us/sample - loss: 1.4459\n",
      "Epoch 22/500\n",
      "150/150 [==============================] - 0s 85us/sample - loss: 1.3939\n",
      "Epoch 23/500\n",
      "150/150 [==============================] - 0s 105us/sample - loss: 1.3506\n",
      "Epoch 24/500\n",
      "150/150 [==============================] - 0s 105us/sample - loss: 1.3063\n",
      "Epoch 25/500\n",
      "150/150 [==============================] - 0s 108us/sample - loss: 1.2663\n",
      "Epoch 26/500\n",
      "150/150 [==============================] - 0s 100us/sample - loss: 1.2210\n",
      "Epoch 27/500\n",
      "150/150 [==============================] - 0s 112us/sample - loss: 1.1827\n",
      "Epoch 28/500\n",
      "150/150 [==============================] - 0s 110us/sample - loss: 1.1484\n",
      "Epoch 29/500\n",
      "150/150 [==============================] - 0s 90us/sample - loss: 1.1180\n",
      "Epoch 30/500\n",
      "150/150 [==============================] - 0s 110us/sample - loss: 1.0885\n",
      "Epoch 31/500\n",
      "150/150 [==============================] - 0s 99us/sample - loss: 1.0568\n",
      "Epoch 32/500\n",
      "150/150 [==============================] - 0s 79us/sample - loss: 1.0262\n",
      "Epoch 33/500\n",
      "150/150 [==============================] - 0s 139us/sample - loss: 1.0005\n",
      "Epoch 34/500\n",
      "150/150 [==============================] - 0s 92us/sample - loss: 0.9819\n",
      "Epoch 35/500\n",
      "150/150 [==============================] - 0s 164us/sample - loss: 0.9558\n",
      "Epoch 36/500\n",
      "150/150 [==============================] - 0s 125us/sample - loss: 0.9345\n",
      "Epoch 37/500\n",
      "150/150 [==============================] - 0s 117us/sample - loss: 0.9166\n",
      "Epoch 38/500\n",
      "150/150 [==============================] - 0s 130us/sample - loss: 0.8940\n",
      "Epoch 39/500\n",
      "150/150 [==============================] - 0s 81us/sample - loss: 0.8777\n",
      "Epoch 40/500\n",
      "150/150 [==============================] - 0s 130us/sample - loss: 0.8632\n",
      "Epoch 41/500\n",
      "150/150 [==============================] - 0s 123us/sample - loss: 0.8448\n",
      "Epoch 42/500\n",
      "150/150 [==============================] - 0s 99us/sample - loss: 0.8297\n",
      "Epoch 43/500\n",
      "150/150 [==============================] - 0s 192us/sample - loss: 0.8217\n",
      "Epoch 44/500\n",
      "150/150 [==============================] - 0s 88us/sample - loss: 0.8019\n",
      "Epoch 45/500\n",
      "150/150 [==============================] - 0s 88us/sample - loss: 0.7892\n",
      "Epoch 46/500\n",
      "150/150 [==============================] - 0s 104us/sample - loss: 0.7804\n",
      "Epoch 47/500\n",
      "150/150 [==============================] - 0s 93us/sample - loss: 0.7653\n",
      "Epoch 48/500\n",
      "150/150 [==============================] - 0s 120us/sample - loss: 0.7540\n",
      "Epoch 49/500\n",
      "150/150 [==============================] - 0s 70us/sample - loss: 0.7432\n",
      "Epoch 50/500\n",
      "150/150 [==============================] - 0s 99us/sample - loss: 0.7354\n",
      "Epoch 51/500\n",
      "150/150 [==============================] - 0s 125us/sample - loss: 0.7343\n",
      "Epoch 52/500\n",
      "150/150 [==============================] - 0s 119us/sample - loss: 0.7184\n",
      "Epoch 53/500\n",
      "150/150 [==============================] - 0s 95us/sample - loss: 0.7086\n",
      "Epoch 54/500\n",
      "150/150 [==============================] - 0s 100us/sample - loss: 0.7049\n",
      "Epoch 55/500\n",
      "150/150 [==============================] - 0s 105us/sample - loss: 0.6930\n",
      "Epoch 56/500\n",
      "150/150 [==============================] - 0s 140us/sample - loss: 0.6903\n",
      "Epoch 57/500\n",
      "150/150 [==============================] - 0s 124us/sample - loss: 0.6816\n",
      "Epoch 58/500\n",
      "150/150 [==============================] - 0s 199us/sample - loss: 0.6722\n",
      "Epoch 59/500\n",
      "150/150 [==============================] - 0s 184us/sample - loss: 0.6662\n",
      "Epoch 60/500\n",
      "150/150 [==============================] - 0s 87us/sample - loss: 0.6604\n",
      "Epoch 61/500\n",
      "150/150 [==============================] - 0s 166us/sample - loss: 0.6576\n",
      "Epoch 62/500\n",
      "150/150 [==============================] - 0s 163us/sample - loss: 0.6481\n",
      "Epoch 63/500\n",
      "150/150 [==============================] - 0s 87us/sample - loss: 0.6421\n",
      "Epoch 64/500\n",
      "150/150 [==============================] - 0s 81us/sample - loss: 0.6379\n",
      "Epoch 65/500\n",
      "150/150 [==============================] - 0s 82us/sample - loss: 0.6335\n",
      "Epoch 66/500\n",
      "150/150 [==============================] - 0s 96us/sample - loss: 0.6273\n",
      "Epoch 67/500\n",
      "150/150 [==============================] - 0s 81us/sample - loss: 0.6221\n",
      "Epoch 68/500\n",
      "150/150 [==============================] - 0s 68us/sample - loss: 0.6169\n",
      "Epoch 69/500\n",
      "150/150 [==============================] - 0s 140us/sample - loss: 0.6138\n",
      "Epoch 70/500\n",
      "150/150 [==============================] - 0s 74us/sample - loss: 0.6088\n",
      "Epoch 71/500\n",
      "150/150 [==============================] - 0s 85us/sample - loss: 0.6077\n",
      "Epoch 72/500\n",
      "150/150 [==============================] - 0s 73us/sample - loss: 0.6033\n",
      "Epoch 73/500\n",
      "150/150 [==============================] - 0s 105us/sample - loss: 0.5956\n",
      "Epoch 74/500\n",
      "150/150 [==============================] - 0s 101us/sample - loss: 0.5923\n",
      "Epoch 75/500\n",
      "150/150 [==============================] - 0s 104us/sample - loss: 0.5903\n",
      "Epoch 76/500\n",
      "150/150 [==============================] - 0s 103us/sample - loss: 0.5851\n",
      "Epoch 77/500\n",
      "150/150 [==============================] - 0s 117us/sample - loss: 0.5807\n",
      "Epoch 78/500\n",
      "150/150 [==============================] - 0s 92us/sample - loss: 0.5778\n",
      "Epoch 79/500\n",
      "150/150 [==============================] - 0s 109us/sample - loss: 0.5754\n",
      "Epoch 80/500\n",
      "150/150 [==============================] - 0s 86us/sample - loss: 0.5741\n",
      "Epoch 81/500\n",
      "150/150 [==============================] - 0s 110us/sample - loss: 0.5703\n",
      "Epoch 82/500\n",
      "150/150 [==============================] - 0s 79us/sample - loss: 0.5650\n",
      "Epoch 83/500\n",
      "150/150 [==============================] - 0s 109us/sample - loss: 0.5619\n",
      "Epoch 84/500\n",
      "150/150 [==============================] - 0s 86us/sample - loss: 0.5591\n",
      "Epoch 85/500\n",
      "150/150 [==============================] - 0s 68us/sample - loss: 0.5571\n",
      "Epoch 86/500\n",
      "150/150 [==============================] - 0s 80us/sample - loss: 0.5605\n",
      "Epoch 87/500\n",
      "150/150 [==============================] - 0s 76us/sample - loss: 0.5551\n",
      "Epoch 88/500\n",
      "150/150 [==============================] - 0s 113us/sample - loss: 0.5488\n",
      "Epoch 89/500\n",
      "150/150 [==============================] - 0s 64us/sample - loss: 0.5442\n",
      "Epoch 90/500\n",
      "150/150 [==============================] - 0s 97us/sample - loss: 0.5450\n",
      "Epoch 91/500\n",
      "150/150 [==============================] - 0s 110us/sample - loss: 0.5401\n",
      "Epoch 92/500\n",
      "150/150 [==============================] - 0s 102us/sample - loss: 0.5383\n",
      "Epoch 93/500\n",
      "150/150 [==============================] - 0s 90us/sample - loss: 0.5446\n",
      "Epoch 94/500\n",
      "150/150 [==============================] - 0s 106us/sample - loss: 0.5337\n",
      "Epoch 95/500\n",
      "150/150 [==============================] - 0s 100us/sample - loss: 0.5317\n",
      "Epoch 96/500\n",
      "150/150 [==============================] - 0s 101us/sample - loss: 0.5291\n",
      "Epoch 97/500\n",
      "150/150 [==============================] - 0s 100us/sample - loss: 0.5263\n",
      "Epoch 98/500\n",
      "150/150 [==============================] - 0s 76us/sample - loss: 0.5234\n",
      "Epoch 99/500\n",
      "150/150 [==============================] - 0s 65us/sample - loss: 0.5228\n",
      "Epoch 100/500\n",
      "150/150 [==============================] - 0s 63us/sample - loss: 0.5200\n",
      "Epoch 101/500\n",
      "150/150 [==============================] - 0s 138us/sample - loss: 0.5166\n",
      "Epoch 102/500\n",
      "150/150 [==============================] - 0s 112us/sample - loss: 0.5187\n",
      "Epoch 103/500\n",
      "150/150 [==============================] - 0s 98us/sample - loss: 0.5124\n",
      "Epoch 104/500\n",
      "150/150 [==============================] - 0s 111us/sample - loss: 0.5105\n",
      "Epoch 105/500\n",
      "150/150 [==============================] - 0s 89us/sample - loss: 0.5098\n",
      "Epoch 106/500\n",
      "150/150 [==============================] - 0s 91us/sample - loss: 0.5072\n",
      "Epoch 107/500\n",
      "150/150 [==============================] - 0s 107us/sample - loss: 0.5054\n",
      "Epoch 108/500\n",
      "150/150 [==============================] - 0s 90us/sample - loss: 0.5036\n",
      "Epoch 109/500\n",
      "150/150 [==============================] - 0s 88us/sample - loss: 0.5016\n",
      "Epoch 110/500\n",
      "150/150 [==============================] - 0s 98us/sample - loss: 0.5010\n",
      "Epoch 111/500\n",
      "150/150 [==============================] - 0s 108us/sample - loss: 0.4979\n",
      "Epoch 112/500\n",
      "150/150 [==============================] - 0s 96us/sample - loss: 0.4995\n",
      "Epoch 113/500\n",
      "150/150 [==============================] - 0s 141us/sample - loss: 0.4964\n",
      "Epoch 114/500\n",
      "150/150 [==============================] - 0s 87us/sample - loss: 0.4935\n",
      "Epoch 115/500\n",
      "150/150 [==============================] - 0s 110us/sample - loss: 0.4927\n",
      "Epoch 116/500\n",
      "150/150 [==============================] - 0s 88us/sample - loss: 0.4909\n",
      "Epoch 117/500\n",
      "150/150 [==============================] - 0s 85us/sample - loss: 0.4893\n",
      "Epoch 118/500\n",
      "150/150 [==============================] - 0s 117us/sample - loss: 0.4868\n",
      "Epoch 119/500\n",
      "150/150 [==============================] - 0s 81us/sample - loss: 0.4866\n",
      "Epoch 120/500\n",
      "150/150 [==============================] - 0s 71us/sample - loss: 0.4834\n",
      "Epoch 121/500\n",
      "150/150 [==============================] - 0s 74us/sample - loss: 0.4829\n",
      "Epoch 122/500\n",
      "150/150 [==============================] - 0s 59us/sample - loss: 0.4808\n",
      "Epoch 123/500\n",
      "150/150 [==============================] - 0s 127us/sample - loss: 0.4786\n",
      "Epoch 124/500\n",
      "150/150 [==============================] - 0s 92us/sample - loss: 0.4775\n",
      "Epoch 125/500\n",
      "150/150 [==============================] - 0s 132us/sample - loss: 0.4763\n",
      "Epoch 126/500\n",
      "150/150 [==============================] - 0s 58us/sample - loss: 0.4754\n",
      "Epoch 127/500\n",
      "150/150 [==============================] - 0s 137us/sample - loss: 0.4744\n",
      "Epoch 128/500\n",
      "150/150 [==============================] - 0s 105us/sample - loss: 0.4776\n",
      "Epoch 129/500\n",
      "150/150 [==============================] - 0s 129us/sample - loss: 0.4721\n",
      "Epoch 130/500\n",
      "150/150 [==============================] - 0s 121us/sample - loss: 0.4688\n",
      "Epoch 131/500\n",
      "150/150 [==============================] - 0s 97us/sample - loss: 0.4689\n",
      "Epoch 132/500\n",
      "150/150 [==============================] - 0s 93us/sample - loss: 0.4672\n",
      "Epoch 133/500\n",
      "150/150 [==============================] - 0s 136us/sample - loss: 0.4646\n",
      "Epoch 134/500\n",
      "150/150 [==============================] - 0s 139us/sample - loss: 0.4641\n",
      "Epoch 135/500\n",
      "150/150 [==============================] - 0s 95us/sample - loss: 0.4623\n",
      "Epoch 136/500\n",
      "150/150 [==============================] - 0s 113us/sample - loss: 0.4611\n",
      "Epoch 137/500\n",
      "150/150 [==============================] - 0s 92us/sample - loss: 0.4601\n",
      "Epoch 138/500\n",
      "150/150 [==============================] - 0s 93us/sample - loss: 0.4617\n",
      "Epoch 139/500\n",
      "150/150 [==============================] - 0s 89us/sample - loss: 0.4576\n",
      "Epoch 140/500\n",
      "150/150 [==============================] - 0s 71us/sample - loss: 0.4578\n",
      "Epoch 141/500\n",
      "150/150 [==============================] - 0s 97us/sample - loss: 0.4558\n",
      "Epoch 142/500\n",
      "150/150 [==============================] - 0s 106us/sample - loss: 0.4557\n",
      "Epoch 143/500\n",
      "150/150 [==============================] - 0s 90us/sample - loss: 0.4526\n",
      "Epoch 144/500\n",
      "150/150 [==============================] - 0s 82us/sample - loss: 0.4539\n",
      "Epoch 145/500\n",
      "150/150 [==============================] - 0s 88us/sample - loss: 0.4557\n",
      "Epoch 146/500\n",
      "150/150 [==============================] - 0s 84us/sample - loss: 0.4500\n",
      "Epoch 147/500\n",
      "150/150 [==============================] - 0s 116us/sample - loss: 0.4481\n",
      "Epoch 148/500\n",
      "150/150 [==============================] - 0s 67us/sample - loss: 0.4473\n",
      "Epoch 149/500\n",
      "150/150 [==============================] - 0s 79us/sample - loss: 0.4466\n",
      "Epoch 150/500\n",
      "150/150 [==============================] - 0s 92us/sample - loss: 0.4445\n",
      "Epoch 151/500\n",
      "150/150 [==============================] - 0s 110us/sample - loss: 0.4468\n",
      "Epoch 152/500\n",
      "150/150 [==============================] - 0s 115us/sample - loss: 0.4432\n",
      "Epoch 153/500\n",
      "150/150 [==============================] - 0s 69us/sample - loss: 0.4430\n",
      "Epoch 154/500\n",
      "150/150 [==============================] - 0s 88us/sample - loss: 0.4411\n",
      "Epoch 155/500\n",
      "150/150 [==============================] - 0s 89us/sample - loss: 0.4393\n",
      "Epoch 156/500\n",
      "150/150 [==============================] - 0s 115us/sample - loss: 0.4401\n",
      "Epoch 157/500\n",
      "150/150 [==============================] - 0s 109us/sample - loss: 0.4388\n",
      "Epoch 158/500\n",
      "150/150 [==============================] - 0s 94us/sample - loss: 0.4373\n",
      "Epoch 159/500\n",
      "150/150 [==============================] - 0s 91us/sample - loss: 0.4358\n",
      "Epoch 160/500\n",
      "150/150 [==============================] - 0s 98us/sample - loss: 0.4345\n",
      "Epoch 161/500\n",
      "150/150 [==============================] - 0s 102us/sample - loss: 0.4376\n",
      "Epoch 162/500\n",
      "150/150 [==============================] - 0s 129us/sample - loss: 0.4323\n",
      "Epoch 163/500\n",
      "150/150 [==============================] - 0s 89us/sample - loss: 0.4316\n",
      "Epoch 164/500\n",
      "150/150 [==============================] - 0s 79us/sample - loss: 0.4304\n",
      "Epoch 165/500\n",
      "150/150 [==============================] - 0s 91us/sample - loss: 0.4305\n",
      "Epoch 166/500\n",
      "150/150 [==============================] - 0s 92us/sample - loss: 0.4300\n",
      "Epoch 167/500\n",
      "150/150 [==============================] - 0s 84us/sample - loss: 0.4291\n",
      "Epoch 168/500\n",
      "150/150 [==============================] - 0s 121us/sample - loss: 0.4269\n",
      "Epoch 169/500\n",
      "150/150 [==============================] - 0s 86us/sample - loss: 0.4332\n",
      "Epoch 170/500\n",
      "150/150 [==============================] - 0s 80us/sample - loss: 0.4285\n",
      "Epoch 171/500\n",
      "150/150 [==============================] - 0s 128us/sample - loss: 0.4249\n",
      "Epoch 172/500\n",
      "150/150 [==============================] - 0s 102us/sample - loss: 0.4231\n",
      "Epoch 173/500\n",
      "150/150 [==============================] - 0s 201us/sample - loss: 0.4236\n",
      "Epoch 174/500\n",
      "150/150 [==============================] - 0s 64us/sample - loss: 0.4245\n",
      "Epoch 175/500\n",
      "150/150 [==============================] - 0s 109us/sample - loss: 0.4234\n",
      "Epoch 176/500\n",
      "150/150 [==============================] - 0s 120us/sample - loss: 0.4195\n",
      "Epoch 177/500\n",
      "150/150 [==============================] - 0s 89us/sample - loss: 0.4187\n",
      "Epoch 178/500\n",
      "150/150 [==============================] - 0s 95us/sample - loss: 0.4175\n",
      "Epoch 179/500\n",
      "150/150 [==============================] - 0s 116us/sample - loss: 0.4190\n",
      "Epoch 180/500\n",
      "150/150 [==============================] - 0s 79us/sample - loss: 0.4166\n",
      "Epoch 181/500\n",
      "150/150 [==============================] - 0s 81us/sample - loss: 0.4155\n",
      "Epoch 182/500\n",
      "150/150 [==============================] - 0s 73us/sample - loss: 0.4145\n",
      "Epoch 183/500\n",
      "150/150 [==============================] - 0s 59us/sample - loss: 0.4142\n",
      "Epoch 184/500\n",
      "150/150 [==============================] - 0s 86us/sample - loss: 0.4126\n",
      "Epoch 185/500\n",
      "150/150 [==============================] - 0s 121us/sample - loss: 0.4160\n",
      "Epoch 186/500\n",
      "150/150 [==============================] - 0s 101us/sample - loss: 0.4141\n",
      "Epoch 187/500\n",
      "150/150 [==============================] - 0s 140us/sample - loss: 0.4121\n",
      "Epoch 188/500\n",
      "150/150 [==============================] - 0s 101us/sample - loss: 0.4109\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 189/500\n",
      "150/150 [==============================] - 0s 63us/sample - loss: 0.4096\n",
      "Epoch 190/500\n",
      "150/150 [==============================] - 0s 114us/sample - loss: 0.4081\n",
      "Epoch 191/500\n",
      "150/150 [==============================] - 0s 68us/sample - loss: 0.4102\n",
      "Epoch 192/500\n",
      "150/150 [==============================] - 0s 103us/sample - loss: 0.4065\n",
      "Epoch 193/500\n",
      "150/150 [==============================] - 0s 107us/sample - loss: 0.4083\n",
      "Epoch 194/500\n",
      "150/150 [==============================] - 0s 73us/sample - loss: 0.4061\n",
      "Epoch 195/500\n",
      "150/150 [==============================] - 0s 84us/sample - loss: 0.4054\n",
      "Epoch 196/500\n",
      "150/150 [==============================] - 0s 76us/sample - loss: 0.4033\n",
      "Epoch 197/500\n",
      "150/150 [==============================] - 0s 75us/sample - loss: 0.4043\n",
      "Epoch 198/500\n",
      "150/150 [==============================] - 0s 91us/sample - loss: 0.4017\n",
      "Epoch 199/500\n",
      "150/150 [==============================] - 0s 103us/sample - loss: 0.4020\n",
      "Epoch 200/500\n",
      "150/150 [==============================] - 0s 71us/sample - loss: 0.4005\n",
      "Epoch 201/500\n",
      "150/150 [==============================] - 0s 83us/sample - loss: 0.3996\n",
      "Epoch 202/500\n",
      "150/150 [==============================] - 0s 104us/sample - loss: 0.3986\n",
      "Epoch 203/500\n",
      "150/150 [==============================] - 0s 92us/sample - loss: 0.3997\n",
      "Epoch 204/500\n",
      "150/150 [==============================] - 0s 89us/sample - loss: 0.3985\n",
      "Epoch 205/500\n",
      "150/150 [==============================] - 0s 79us/sample - loss: 0.3983\n",
      "Epoch 206/500\n",
      "150/150 [==============================] - 0s 109us/sample - loss: 0.3968\n",
      "Epoch 207/500\n",
      "150/150 [==============================] - 0s 89us/sample - loss: 0.3977\n",
      "Epoch 208/500\n",
      "150/150 [==============================] - 0s 73us/sample - loss: 0.3941\n",
      "Epoch 209/500\n",
      "150/150 [==============================] - 0s 132us/sample - loss: 0.3968\n",
      "Epoch 210/500\n",
      "150/150 [==============================] - 0s 89us/sample - loss: 0.3936\n",
      "Epoch 211/500\n",
      "150/150 [==============================] - 0s 91us/sample - loss: 0.3925\n",
      "Epoch 212/500\n",
      "150/150 [==============================] - 0s 99us/sample - loss: 0.3919\n",
      "Epoch 213/500\n",
      "150/150 [==============================] - 0s 108us/sample - loss: 0.3913\n",
      "Epoch 214/500\n",
      "150/150 [==============================] - 0s 103us/sample - loss: 0.3923\n",
      "Epoch 215/500\n",
      "150/150 [==============================] - 0s 101us/sample - loss: 0.3898\n",
      "Epoch 216/500\n",
      "150/150 [==============================] - 0s 116us/sample - loss: 0.3889\n",
      "Epoch 217/500\n",
      "150/150 [==============================] - 0s 95us/sample - loss: 0.3917\n",
      "Epoch 218/500\n",
      "150/150 [==============================] - 0s 99us/sample - loss: 0.3891\n",
      "Epoch 219/500\n",
      "150/150 [==============================] - 0s 84us/sample - loss: 0.3887\n",
      "Epoch 220/500\n",
      "150/150 [==============================] - 0s 85us/sample - loss: 0.3879\n",
      "Epoch 221/500\n",
      "150/150 [==============================] - 0s 78us/sample - loss: 0.3878\n",
      "Epoch 222/500\n",
      "150/150 [==============================] - 0s 133us/sample - loss: 0.3877\n",
      "Epoch 223/500\n",
      "150/150 [==============================] - 0s 85us/sample - loss: 0.3854\n",
      "Epoch 224/500\n",
      "150/150 [==============================] - 0s 84us/sample - loss: 0.3838\n",
      "Epoch 225/500\n",
      "150/150 [==============================] - 0s 103us/sample - loss: 0.3832\n",
      "Epoch 226/500\n",
      "150/150 [==============================] - 0s 104us/sample - loss: 0.3865\n",
      "Epoch 227/500\n",
      "150/150 [==============================] - 0s 95us/sample - loss: 0.3819\n",
      "Epoch 228/500\n",
      "150/150 [==============================] - 0s 89us/sample - loss: 0.3824\n",
      "Epoch 229/500\n",
      "150/150 [==============================] - 0s 124us/sample - loss: 0.3849\n",
      "Epoch 230/500\n",
      "150/150 [==============================] - 0s 137us/sample - loss: 0.3799\n",
      "Epoch 231/500\n",
      "150/150 [==============================] - 0s 110us/sample - loss: 0.3810\n",
      "Epoch 232/500\n",
      "150/150 [==============================] - 0s 69us/sample - loss: 0.3791\n",
      "Epoch 233/500\n",
      "150/150 [==============================] - 0s 91us/sample - loss: 0.3776\n",
      "Epoch 234/500\n",
      "150/150 [==============================] - 0s 106us/sample - loss: 0.3770\n",
      "Epoch 235/500\n",
      "150/150 [==============================] - 0s 110us/sample - loss: 0.3762\n",
      "Epoch 236/500\n",
      "150/150 [==============================] - 0s 135us/sample - loss: 0.3792\n",
      "Epoch 237/500\n",
      "150/150 [==============================] - 0s 111us/sample - loss: 0.3781\n",
      "Epoch 238/500\n",
      "150/150 [==============================] - 0s 103us/sample - loss: 0.3759\n",
      "Epoch 239/500\n",
      "150/150 [==============================] - 0s 95us/sample - loss: 0.3767\n",
      "Epoch 240/500\n",
      "150/150 [==============================] - 0s 109us/sample - loss: 0.3738\n",
      "Epoch 241/500\n",
      "150/150 [==============================] - 0s 93us/sample - loss: 0.3731\n",
      "Epoch 242/500\n",
      "150/150 [==============================] - 0s 94us/sample - loss: 0.3728\n",
      "Epoch 243/500\n",
      "150/150 [==============================] - 0s 131us/sample - loss: 0.3738\n",
      "Epoch 244/500\n",
      "150/150 [==============================] - 0s 109us/sample - loss: 0.3710\n",
      "Epoch 245/500\n",
      "150/150 [==============================] - 0s 196us/sample - loss: 0.3729\n",
      "Epoch 246/500\n",
      "150/150 [==============================] - 0s 176us/sample - loss: 0.3719\n",
      "Epoch 247/500\n",
      "150/150 [==============================] - 0s 111us/sample - loss: 0.3698\n",
      "Epoch 248/500\n",
      "150/150 [==============================] - 0s 200us/sample - loss: 0.3687\n",
      "Epoch 249/500\n",
      "150/150 [==============================] - 0s 134us/sample - loss: 0.3682\n",
      "Epoch 250/500\n",
      "150/150 [==============================] - 0s 209us/sample - loss: 0.3687\n",
      "Epoch 251/500\n",
      "150/150 [==============================] - 0s 202us/sample - loss: 0.3679\n",
      "Epoch 252/500\n",
      "150/150 [==============================] - 0s 115us/sample - loss: 0.3678\n",
      "Epoch 253/500\n",
      "150/150 [==============================] - 0s 124us/sample - loss: 0.3679\n",
      "Epoch 254/500\n",
      "150/150 [==============================] - 0s 85us/sample - loss: 0.3656\n",
      "Epoch 255/500\n",
      "150/150 [==============================] - 0s 94us/sample - loss: 0.3648\n",
      "Epoch 256/500\n",
      "150/150 [==============================] - 0s 163us/sample - loss: 0.3666\n",
      "Epoch 257/500\n",
      "150/150 [==============================] - 0s 127us/sample - loss: 0.3637\n",
      "Epoch 258/500\n",
      "150/150 [==============================] - 0s 128us/sample - loss: 0.3632\n",
      "Epoch 259/500\n",
      "150/150 [==============================] - 0s 122us/sample - loss: 0.3640\n",
      "Epoch 260/500\n",
      "150/150 [==============================] - 0s 92us/sample - loss: 0.3621\n",
      "Epoch 261/500\n",
      "150/150 [==============================] - 0s 117us/sample - loss: 0.3613\n",
      "Epoch 262/500\n",
      "150/150 [==============================] - 0s 103us/sample - loss: 0.3618\n",
      "Epoch 263/500\n",
      "150/150 [==============================] - 0s 143us/sample - loss: 0.3604\n",
      "Epoch 264/500\n",
      "150/150 [==============================] - 0s 125us/sample - loss: 0.3612\n",
      "Epoch 265/500\n",
      "150/150 [==============================] - 0s 109us/sample - loss: 0.3595\n",
      "Epoch 266/500\n",
      "150/150 [==============================] - 0s 94us/sample - loss: 0.3593\n",
      "Epoch 267/500\n",
      "150/150 [==============================] - 0s 107us/sample - loss: 0.3590\n",
      "Epoch 268/500\n",
      "150/150 [==============================] - 0s 128us/sample - loss: 0.3578\n",
      "Epoch 269/500\n",
      "150/150 [==============================] - 0s 80us/sample - loss: 0.3585\n",
      "Epoch 270/500\n",
      "150/150 [==============================] - 0s 63us/sample - loss: 0.3585\n",
      "Epoch 271/500\n",
      "150/150 [==============================] - 0s 125us/sample - loss: 0.3584\n",
      "Epoch 272/500\n",
      "150/150 [==============================] - 0s 96us/sample - loss: 0.3562\n",
      "Epoch 273/500\n",
      "150/150 [==============================] - 0s 122us/sample - loss: 0.3562\n",
      "Epoch 274/500\n",
      "150/150 [==============================] - 0s 73us/sample - loss: 0.3553\n",
      "Epoch 275/500\n",
      "150/150 [==============================] - 0s 58us/sample - loss: 0.3549\n",
      "Epoch 276/500\n",
      "150/150 [==============================] - 0s 115us/sample - loss: 0.3548\n",
      "Epoch 277/500\n",
      "150/150 [==============================] - 0s 110us/sample - loss: 0.3536\n",
      "Epoch 278/500\n",
      "150/150 [==============================] - 0s 108us/sample - loss: 0.3531\n",
      "Epoch 279/500\n",
      "150/150 [==============================] - 0s 84us/sample - loss: 0.3521\n",
      "Epoch 280/500\n",
      "150/150 [==============================] - 0s 109us/sample - loss: 0.3520\n",
      "Epoch 281/500\n",
      "150/150 [==============================] - 0s 82us/sample - loss: 0.3514\n",
      "Epoch 282/500\n",
      "150/150 [==============================] - 0s 99us/sample - loss: 0.3506\n",
      "Epoch 283/500\n",
      "150/150 [==============================] - 0s 109us/sample - loss: 0.3535\n",
      "Epoch 284/500\n",
      "150/150 [==============================] - 0s 77us/sample - loss: 0.3509\n",
      "Epoch 285/500\n",
      "150/150 [==============================] - 0s 64us/sample - loss: 0.3504\n",
      "Epoch 286/500\n",
      "150/150 [==============================] - 0s 92us/sample - loss: 0.3492\n",
      "Epoch 287/500\n",
      "150/150 [==============================] - 0s 96us/sample - loss: 0.3485\n",
      "Epoch 288/500\n",
      "150/150 [==============================] - 0s 91us/sample - loss: 0.3488\n",
      "Epoch 289/500\n",
      "150/150 [==============================] - 0s 66us/sample - loss: 0.3475\n",
      "Epoch 290/500\n",
      "150/150 [==============================] - 0s 86us/sample - loss: 0.3473\n",
      "Epoch 291/500\n",
      "150/150 [==============================] - 0s 111us/sample - loss: 0.3469\n",
      "Epoch 292/500\n",
      "150/150 [==============================] - 0s 99us/sample - loss: 0.3461\n",
      "Epoch 293/500\n",
      "150/150 [==============================] - 0s 104us/sample - loss: 0.3452\n",
      "Epoch 294/500\n",
      "150/150 [==============================] - 0s 98us/sample - loss: 0.3446\n",
      "Epoch 295/500\n",
      "150/150 [==============================] - 0s 102us/sample - loss: 0.3461\n",
      "Epoch 296/500\n",
      "150/150 [==============================] - 0s 101us/sample - loss: 0.3447\n",
      "Epoch 297/500\n",
      "150/150 [==============================] - 0s 98us/sample - loss: 0.3435\n",
      "Epoch 298/500\n",
      "150/150 [==============================] - 0s 96us/sample - loss: 0.3433\n",
      "Epoch 299/500\n",
      "150/150 [==============================] - 0s 88us/sample - loss: 0.3422\n",
      "Epoch 300/500\n",
      "150/150 [==============================] - 0s 105us/sample - loss: 0.3424\n",
      "Epoch 301/500\n",
      "150/150 [==============================] - 0s 78us/sample - loss: 0.3419\n",
      "Epoch 302/500\n",
      "150/150 [==============================] - 0s 98us/sample - loss: 0.3408\n",
      "Epoch 303/500\n",
      "150/150 [==============================] - 0s 117us/sample - loss: 0.3436\n",
      "Epoch 304/500\n",
      "150/150 [==============================] - 0s 97us/sample - loss: 0.3439\n",
      "Epoch 305/500\n",
      "150/150 [==============================] - 0s 94us/sample - loss: 0.3401\n",
      "Epoch 306/500\n",
      "150/150 [==============================] - 0s 90us/sample - loss: 0.3420\n",
      "Epoch 307/500\n",
      "150/150 [==============================] - 0s 86us/sample - loss: 0.3397\n",
      "Epoch 308/500\n",
      "150/150 [==============================] - 0s 115us/sample - loss: 0.3391\n",
      "Epoch 309/500\n",
      "150/150 [==============================] - 0s 87us/sample - loss: 0.3384\n",
      "Epoch 310/500\n",
      "150/150 [==============================] - 0s 104us/sample - loss: 0.3381\n",
      "Epoch 311/500\n",
      "150/150 [==============================] - 0s 101us/sample - loss: 0.3406\n",
      "Epoch 312/500\n",
      "150/150 [==============================] - 0s 110us/sample - loss: 0.3365\n",
      "Epoch 313/500\n",
      "150/150 [==============================] - 0s 104us/sample - loss: 0.3366\n",
      "Epoch 314/500\n",
      "150/150 [==============================] - 0s 105us/sample - loss: 0.3390\n",
      "Epoch 315/500\n",
      "150/150 [==============================] - 0s 118us/sample - loss: 0.3356\n",
      "Epoch 316/500\n",
      "150/150 [==============================] - 0s 112us/sample - loss: 0.3350\n",
      "Epoch 317/500\n",
      "150/150 [==============================] - 0s 167us/sample - loss: 0.3350\n",
      "Epoch 318/500\n",
      "150/150 [==============================] - 0s 116us/sample - loss: 0.3341\n",
      "Epoch 319/500\n",
      "150/150 [==============================] - 0s 202us/sample - loss: 0.3334\n",
      "Epoch 320/500\n",
      "150/150 [==============================] - 0s 118us/sample - loss: 0.3335\n",
      "Epoch 321/500\n",
      "150/150 [==============================] - 0s 64us/sample - loss: 0.3347\n",
      "Epoch 322/500\n",
      "150/150 [==============================] - 0s 105us/sample - loss: 0.3332\n",
      "Epoch 323/500\n",
      "150/150 [==============================] - 0s 88us/sample - loss: 0.3318\n",
      "Epoch 324/500\n",
      "150/150 [==============================] - 0s 81us/sample - loss: 0.3311\n",
      "Epoch 325/500\n",
      "150/150 [==============================] - 0s 80us/sample - loss: 0.3313\n",
      "Epoch 326/500\n",
      "150/150 [==============================] - 0s 96us/sample - loss: 0.3302\n",
      "Epoch 327/500\n",
      "150/150 [==============================] - 0s 126us/sample - loss: 0.3311\n",
      "Epoch 328/500\n",
      "150/150 [==============================] - 0s 125us/sample - loss: 0.3310\n",
      "Epoch 329/500\n",
      "150/150 [==============================] - 0s 91us/sample - loss: 0.3315\n",
      "Epoch 330/500\n",
      "150/150 [==============================] - 0s 143us/sample - loss: 0.3297\n",
      "Epoch 331/500\n",
      "150/150 [==============================] - 0s 109us/sample - loss: 0.3295\n",
      "Epoch 332/500\n",
      "150/150 [==============================] - 0s 141us/sample - loss: 0.3284\n",
      "Epoch 333/500\n",
      "150/150 [==============================] - 0s 179us/sample - loss: 0.3297\n",
      "Epoch 334/500\n",
      "150/150 [==============================] - 0s 245us/sample - loss: 0.3274\n",
      "Epoch 335/500\n",
      "150/150 [==============================] - 0s 125us/sample - loss: 0.3278\n",
      "Epoch 336/500\n",
      "150/150 [==============================] - 0s 161us/sample - loss: 0.3260\n",
      "Epoch 337/500\n",
      "150/150 [==============================] - 0s 116us/sample - loss: 0.3264\n",
      "Epoch 338/500\n",
      "150/150 [==============================] - 0s 84us/sample - loss: 0.3262\n",
      "Epoch 339/500\n",
      "150/150 [==============================] - 0s 123us/sample - loss: 0.3256\n",
      "Epoch 340/500\n",
      "150/150 [==============================] - 0s 116us/sample - loss: 0.3263\n",
      "Epoch 341/500\n",
      "150/150 [==============================] - 0s 165us/sample - loss: 0.3253\n",
      "Epoch 342/500\n",
      "150/150 [==============================] - 0s 105us/sample - loss: 0.3273\n",
      "Epoch 343/500\n",
      "150/150 [==============================] - 0s 120us/sample - loss: 0.3240\n",
      "Epoch 344/500\n",
      "150/150 [==============================] - 0s 117us/sample - loss: 0.3229\n",
      "Epoch 345/500\n",
      "150/150 [==============================] - 0s 130us/sample - loss: 0.3223\n",
      "Epoch 346/500\n",
      "150/150 [==============================] - 0s 130us/sample - loss: 0.3231\n",
      "Epoch 347/500\n",
      "150/150 [==============================] - 0s 153us/sample - loss: 0.3219\n",
      "Epoch 348/500\n",
      "150/150 [==============================] - 0s 167us/sample - loss: 0.3237\n",
      "Epoch 349/500\n",
      "150/150 [==============================] - 0s 140us/sample - loss: 0.3215\n",
      "Epoch 350/500\n",
      "150/150 [==============================] - 0s 121us/sample - loss: 0.3205\n",
      "Epoch 351/500\n",
      "150/150 [==============================] - 0s 105us/sample - loss: 0.3209\n",
      "Epoch 352/500\n",
      "150/150 [==============================] - 0s 131us/sample - loss: 0.3200\n",
      "Epoch 353/500\n",
      "150/150 [==============================] - 0s 111us/sample - loss: 0.3198\n",
      "Epoch 354/500\n",
      "150/150 [==============================] - 0s 119us/sample - loss: 0.3202\n",
      "Epoch 355/500\n",
      "150/150 [==============================] - 0s 110us/sample - loss: 0.3193\n",
      "Epoch 356/500\n",
      "150/150 [==============================] - 0s 69us/sample - loss: 0.3203\n",
      "Epoch 357/500\n",
      "150/150 [==============================] - 0s 53us/sample - loss: 0.3202\n",
      "Epoch 358/500\n",
      "150/150 [==============================] - 0s 186us/sample - loss: 0.3195\n",
      "Epoch 359/500\n",
      "150/150 [==============================] - 0s 180us/sample - loss: 0.3174\n",
      "Epoch 360/500\n",
      "150/150 [==============================] - 0s 79us/sample - loss: 0.3182\n",
      "Epoch 361/500\n",
      "150/150 [==============================] - 0s 101us/sample - loss: 0.3191\n",
      "Epoch 362/500\n",
      "150/150 [==============================] - 0s 65us/sample - loss: 0.3163\n",
      "Epoch 363/500\n",
      "150/150 [==============================] - 0s 139us/sample - loss: 0.3172\n",
      "Epoch 364/500\n",
      "150/150 [==============================] - 0s 84us/sample - loss: 0.3171\n",
      "Epoch 365/500\n",
      "150/150 [==============================] - 0s 97us/sample - loss: 0.3150\n",
      "Epoch 366/500\n",
      "150/150 [==============================] - 0s 91us/sample - loss: 0.3163\n",
      "Epoch 367/500\n",
      "150/150 [==============================] - 0s 129us/sample - loss: 0.3156\n",
      "Epoch 368/500\n",
      "150/150 [==============================] - 0s 115us/sample - loss: 0.3142\n",
      "Epoch 369/500\n",
      "150/150 [==============================] - 0s 129us/sample - loss: 0.3149\n",
      "Epoch 370/500\n",
      "150/150 [==============================] - 0s 80us/sample - loss: 0.3136\n",
      "Epoch 371/500\n",
      "150/150 [==============================] - 0s 118us/sample - loss: 0.3128\n",
      "Epoch 372/500\n",
      "150/150 [==============================] - 0s 91us/sample - loss: 0.3124\n",
      "Epoch 373/500\n",
      "150/150 [==============================] - 0s 67us/sample - loss: 0.3125\n",
      "Epoch 374/500\n",
      "150/150 [==============================] - 0s 73us/sample - loss: 0.3129\n",
      "Epoch 375/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "150/150 [==============================] - 0s 71us/sample - loss: 0.3127\n",
      "Epoch 376/500\n",
      "150/150 [==============================] - 0s 72us/sample - loss: 0.3124\n",
      "Epoch 377/500\n",
      "150/150 [==============================] - 0s 139us/sample - loss: 0.3106\n",
      "Epoch 378/500\n",
      "150/150 [==============================] - 0s 163us/sample - loss: 0.3112\n",
      "Epoch 379/500\n",
      "150/150 [==============================] - 0s 240us/sample - loss: 0.3098\n",
      "Epoch 380/500\n",
      "150/150 [==============================] - ETA: 0s - loss: 0.316 - 0s 157us/sample - loss: 0.3098\n",
      "Epoch 381/500\n",
      "150/150 [==============================] - 0s 170us/sample - loss: 0.3094\n",
      "Epoch 382/500\n",
      "150/150 [==============================] - 0s 77us/sample - loss: 0.3100\n",
      "Epoch 383/500\n",
      "150/150 [==============================] - 0s 95us/sample - loss: 0.3091\n",
      "Epoch 384/500\n",
      "150/150 [==============================] - 0s 106us/sample - loss: 0.3083\n",
      "Epoch 385/500\n",
      "150/150 [==============================] - 0s 182us/sample - loss: 0.3080\n",
      "Epoch 386/500\n",
      "150/150 [==============================] - 0s 76us/sample - loss: 0.3079\n",
      "Epoch 387/500\n",
      "150/150 [==============================] - 0s 89us/sample - loss: 0.3082\n",
      "Epoch 388/500\n",
      "150/150 [==============================] - 0s 104us/sample - loss: 0.3072\n",
      "Epoch 389/500\n",
      "150/150 [==============================] - 0s 87us/sample - loss: 0.3066\n",
      "Epoch 390/500\n",
      "150/150 [==============================] - 0s 107us/sample - loss: 0.3069\n",
      "Epoch 391/500\n",
      "150/150 [==============================] - ETA: 0s - loss: 0.369 - 0s 118us/sample - loss: 0.3077\n",
      "Epoch 392/500\n",
      "150/150 [==============================] - 0s 81us/sample - loss: 0.3054\n",
      "Epoch 393/500\n",
      "150/150 [==============================] - 0s 135us/sample - loss: 0.3069\n",
      "Epoch 394/500\n",
      "150/150 [==============================] - 0s 134us/sample - loss: 0.3047\n",
      "Epoch 395/500\n",
      "150/150 [==============================] - 0s 85us/sample - loss: 0.3052\n",
      "Epoch 396/500\n",
      "150/150 [==============================] - 0s 136us/sample - loss: 0.3042\n",
      "Epoch 397/500\n",
      "150/150 [==============================] - 0s 107us/sample - loss: 0.3054\n",
      "Epoch 398/500\n",
      "150/150 [==============================] - 0s 109us/sample - loss: 0.3039\n",
      "Epoch 399/500\n",
      "150/150 [==============================] - 0s 103us/sample - loss: 0.3032\n",
      "Epoch 400/500\n",
      "150/150 [==============================] - 0s 98us/sample - loss: 0.3037\n",
      "Epoch 401/500\n",
      "150/150 [==============================] - 0s 75us/sample - loss: 0.3032\n",
      "Epoch 402/500\n",
      "150/150 [==============================] - 0s 71us/sample - loss: 0.3017\n",
      "Epoch 403/500\n",
      "150/150 [==============================] - 0s 96us/sample - loss: 0.3035\n",
      "Epoch 404/500\n",
      "150/150 [==============================] - 0s 77us/sample - loss: 0.3032\n",
      "Epoch 405/500\n",
      "150/150 [==============================] - 0s 72us/sample - loss: 0.3032\n",
      "Epoch 406/500\n",
      "150/150 [==============================] - 0s 99us/sample - loss: 0.3009\n",
      "Epoch 407/500\n",
      "150/150 [==============================] - 0s 112us/sample - loss: 0.3007\n",
      "Epoch 408/500\n",
      "150/150 [==============================] - 0s 94us/sample - loss: 0.2999\n",
      "Epoch 409/500\n",
      "150/150 [==============================] - 0s 92us/sample - loss: 0.2999\n",
      "Epoch 410/500\n",
      "150/150 [==============================] - 0s 89us/sample - loss: 0.2994\n",
      "Epoch 411/500\n",
      "150/150 [==============================] - 0s 102us/sample - loss: 0.2997\n",
      "Epoch 412/500\n",
      "150/150 [==============================] - 0s 87us/sample - loss: 0.2988\n",
      "Epoch 413/500\n",
      "150/150 [==============================] - 0s 87us/sample - loss: 0.2992\n",
      "Epoch 414/500\n",
      "150/150 [==============================] - 0s 96us/sample - loss: 0.3000\n",
      "Epoch 415/500\n",
      "150/150 [==============================] - 0s 85us/sample - loss: 0.3005\n",
      "Epoch 416/500\n",
      "150/150 [==============================] - 0s 148us/sample - loss: 0.2978\n",
      "Epoch 417/500\n",
      "150/150 [==============================] - 0s 111us/sample - loss: 0.2974\n",
      "Epoch 418/500\n",
      "150/150 [==============================] - 0s 108us/sample - loss: 0.2992\n",
      "Epoch 419/500\n",
      "150/150 [==============================] - 0s 127us/sample - loss: 0.2971\n",
      "Epoch 420/500\n",
      "150/150 [==============================] - 0s 152us/sample - loss: 0.2971\n",
      "Epoch 421/500\n",
      "150/150 [==============================] - 0s 140us/sample - loss: 0.2962\n",
      "Epoch 422/500\n",
      "150/150 [==============================] - 0s 169us/sample - loss: 0.2962\n",
      "Epoch 423/500\n",
      "150/150 [==============================] - 0s 154us/sample - loss: 0.2956\n",
      "Epoch 424/500\n",
      "150/150 [==============================] - 0s 76us/sample - loss: 0.2962\n",
      "Epoch 425/500\n",
      "150/150 [==============================] - 0s 95us/sample - loss: 0.2963\n",
      "Epoch 426/500\n",
      "150/150 [==============================] - 0s 77us/sample - loss: 0.2946\n",
      "Epoch 427/500\n",
      "150/150 [==============================] - 0s 78us/sample - loss: 0.2949\n",
      "Epoch 428/500\n",
      "150/150 [==============================] - 0s 88us/sample - loss: 0.2953\n",
      "Epoch 429/500\n",
      "150/150 [==============================] - 0s 88us/sample - loss: 0.2944\n",
      "Epoch 430/500\n",
      "150/150 [==============================] - 0s 101us/sample - loss: 0.2927\n",
      "Epoch 431/500\n",
      "150/150 [==============================] - 0s 104us/sample - loss: 0.2928\n",
      "Epoch 432/500\n",
      "150/150 [==============================] - 0s 96us/sample - loss: 0.2927\n",
      "Epoch 433/500\n",
      "150/150 [==============================] - 0s 96us/sample - loss: 0.2923\n",
      "Epoch 434/500\n",
      "150/150 [==============================] - 0s 134us/sample - loss: 0.2932\n",
      "Epoch 435/500\n",
      "150/150 [==============================] - 0s 90us/sample - loss: 0.2924\n",
      "Epoch 436/500\n",
      "150/150 [==============================] - 0s 108us/sample - loss: 0.2925\n",
      "Epoch 437/500\n",
      "150/150 [==============================] - 0s 119us/sample - loss: 0.2908\n",
      "Epoch 438/500\n",
      "150/150 [==============================] - 0s 99us/sample - loss: 0.2906\n",
      "Epoch 439/500\n",
      "150/150 [==============================] - 0s 78us/sample - loss: 0.2902\n",
      "Epoch 440/500\n",
      "150/150 [==============================] - 0s 85us/sample - loss: 0.2902\n",
      "Epoch 441/500\n",
      "150/150 [==============================] - 0s 111us/sample - loss: 0.2901\n",
      "Epoch 442/500\n",
      "150/150 [==============================] - 0s 95us/sample - loss: 0.2893\n",
      "Epoch 443/500\n",
      "150/150 [==============================] - 0s 53us/sample - loss: 0.2904\n",
      "Epoch 444/500\n",
      "150/150 [==============================] - 0s 59us/sample - loss: 0.2890\n",
      "Epoch 445/500\n",
      "150/150 [==============================] - 0s 98us/sample - loss: 0.2887\n",
      "Epoch 446/500\n",
      "150/150 [==============================] - 0s 83us/sample - loss: 0.2912\n",
      "Epoch 447/500\n",
      "150/150 [==============================] - 0s 72us/sample - loss: 0.2884\n",
      "Epoch 448/500\n",
      "150/150 [==============================] - 0s 52us/sample - loss: 0.2889\n",
      "Epoch 449/500\n",
      "150/150 [==============================] - 0s 108us/sample - loss: 0.2877\n",
      "Epoch 450/500\n",
      "150/150 [==============================] - 0s 108us/sample - loss: 0.2870\n",
      "Epoch 451/500\n",
      "150/150 [==============================] - 0s 118us/sample - loss: 0.2868\n",
      "Epoch 452/500\n",
      "150/150 [==============================] - 0s 103us/sample - loss: 0.2869\n",
      "Epoch 453/500\n",
      "150/150 [==============================] - 0s 96us/sample - loss: 0.2874\n",
      "Epoch 454/500\n",
      "150/150 [==============================] - 0s 97us/sample - loss: 0.2859\n",
      "Epoch 455/500\n",
      "150/150 [==============================] - 0s 79us/sample - loss: 0.2878\n",
      "Epoch 456/500\n",
      "150/150 [==============================] - 0s 114us/sample - loss: 0.2854\n",
      "Epoch 457/500\n",
      "150/150 [==============================] - 0s 78us/sample - loss: 0.2863\n",
      "Epoch 458/500\n",
      "150/150 [==============================] - 0s 107us/sample - loss: 0.2847\n",
      "Epoch 459/500\n",
      "150/150 [==============================] - 0s 83us/sample - loss: 0.2853\n",
      "Epoch 460/500\n",
      "150/150 [==============================] - 0s 100us/sample - loss: 0.2845\n",
      "Epoch 461/500\n",
      "150/150 [==============================] - 0s 100us/sample - loss: 0.2839\n",
      "Epoch 462/500\n",
      "150/150 [==============================] - 0s 106us/sample - loss: 0.2838\n",
      "Epoch 463/500\n",
      "150/150 [==============================] - 0s 120us/sample - loss: 0.2851\n",
      "Epoch 464/500\n",
      "150/150 [==============================] - 0s 105us/sample - loss: 0.2830\n",
      "Epoch 465/500\n",
      "150/150 [==============================] - 0s 79us/sample - loss: 0.2835\n",
      "Epoch 466/500\n",
      "150/150 [==============================] - 0s 93us/sample - loss: 0.2825\n",
      "Epoch 467/500\n",
      "150/150 [==============================] - 0s 111us/sample - loss: 0.2839\n",
      "Epoch 468/500\n",
      "150/150 [==============================] - 0s 109us/sample - loss: 0.2822\n",
      "Epoch 469/500\n",
      "150/150 [==============================] - 0s 86us/sample - loss: 0.2827\n",
      "Epoch 470/500\n",
      "150/150 [==============================] - 0s 90us/sample - loss: 0.2818\n",
      "Epoch 471/500\n",
      "150/150 [==============================] - 0s 104us/sample - loss: 0.2823\n",
      "Epoch 472/500\n",
      "150/150 [==============================] - 0s 100us/sample - loss: 0.2810\n",
      "Epoch 473/500\n",
      "150/150 [==============================] - 0s 87us/sample - loss: 0.2823\n",
      "Epoch 474/500\n",
      "150/150 [==============================] - 0s 69us/sample - loss: 0.2819\n",
      "Epoch 475/500\n",
      "150/150 [==============================] - 0s 76us/sample - loss: 0.2803\n",
      "Epoch 476/500\n",
      "150/150 [==============================] - 0s 106us/sample - loss: 0.2820\n",
      "Epoch 477/500\n",
      "150/150 [==============================] - 0s 90us/sample - loss: 0.2798\n",
      "Epoch 478/500\n",
      "150/150 [==============================] - 0s 72us/sample - loss: 0.2816\n",
      "Epoch 479/500\n",
      "150/150 [==============================] - 0s 91us/sample - loss: 0.2790\n",
      "Epoch 480/500\n",
      "150/150 [==============================] - 0s 94us/sample - loss: 0.2788\n",
      "Epoch 481/500\n",
      "150/150 [==============================] - 0s 96us/sample - loss: 0.2793\n",
      "Epoch 482/500\n",
      "150/150 [==============================] - 0s 100us/sample - loss: 0.2791\n",
      "Epoch 483/500\n",
      "150/150 [==============================] - 0s 124us/sample - loss: 0.2792\n",
      "Epoch 484/500\n",
      "150/150 [==============================] - 0s 94us/sample - loss: 0.2780\n",
      "Epoch 485/500\n",
      "150/150 [==============================] - 0s 112us/sample - loss: 0.2772\n",
      "Epoch 486/500\n",
      "150/150 [==============================] - 0s 173us/sample - loss: 0.2770\n",
      "Epoch 487/500\n",
      "150/150 [==============================] - 0s 82us/sample - loss: 0.2775\n",
      "Epoch 488/500\n",
      "150/150 [==============================] - 0s 124us/sample - loss: 0.2769\n",
      "Epoch 489/500\n",
      "150/150 [==============================] - 0s 112us/sample - loss: 0.2772\n",
      "Epoch 490/500\n",
      "150/150 [==============================] - 0s 99us/sample - loss: 0.2771\n",
      "Epoch 491/500\n",
      "150/150 [==============================] - 0s 113us/sample - loss: 0.2767\n",
      "Epoch 492/500\n",
      "150/150 [==============================] - 0s 119us/sample - loss: 0.2755\n",
      "Epoch 493/500\n",
      "150/150 [==============================] - 0s 169us/sample - loss: 0.2768\n",
      "Epoch 494/500\n",
      "150/150 [==============================] - 0s 174us/sample - loss: 0.2750\n",
      "Epoch 495/500\n",
      "150/150 [==============================] - 0s 88us/sample - loss: 0.2746\n",
      "Epoch 496/500\n",
      "150/150 [==============================] - 0s 135us/sample - loss: 0.2763\n",
      "Epoch 497/500\n",
      "150/150 [==============================] - 0s 174us/sample - loss: 0.2749\n",
      "Epoch 498/500\n",
      "150/150 [==============================] - 0s 139us/sample - loss: 0.2761\n",
      "Epoch 499/500\n",
      "150/150 [==============================] - 0s 79us/sample - loss: 0.2739\n",
      "Epoch 500/500\n",
      "150/150 [==============================] - 0s 108us/sample - loss: 0.2734\n"
     ]
    }
   ],
   "source": [
    "# one-hot encoding\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "tf.keras.backend.set_floatx('float64')\n",
    "\n",
    "\n",
    "batch_size = 40\n",
    "epochs = 500\n",
    "\n",
    "# Data Preparation\n",
    "data = np.genfromtxt('iris_full_sm.csv', delimiter=',', skip_header=1)\n",
    "X = data[:,0:4]\n",
    "y = data[:,4]\n",
    "y = y.astype('uint8')\n",
    "\n",
    "classes = 3\n",
    "onehots = tf.one_hot(y, classes)\n",
    "\n",
    "# create model\n",
    "model = tf.keras.Sequential()\n",
    "model.add(tf.keras.Input(shape=(4,)))\n",
    "model.add(tf.keras.layers.Dense(units=3))\n",
    "model.add(keras.layers.Activation(keras.activations.softmax))\n",
    "\n",
    "# declare optimization method and loss function\n",
    "model.compile(optimizer='sgd', loss='categorical_crossentropy')\n",
    "\n",
    "# training\n",
    "history = model.fit(X, onehots, batch_size, epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5) (Optional) Thử chạy lại các ví dụ trên với giá trị khởi tạo cho theta lớn. Ví dụ như theta = tf.Variable(tf.random.normal((5, 3), dtype=tf.float64)*5)\n",
    "\n",
    "## Lúc này, quá trình train không còn ổn định nữa\n",
    "\n",
    "## - Tìm hiểu nguyên nhân tại sao lại có hiện tượng này\n",
    "## - Đề xuất cách giải quyết."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]], shape=(4, 3), dtype=float64)\n",
      "tf.Tensor(\n",
      "[[0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]], shape=(4, 3), dtype=float64)\n",
      "tf.Tensor(\n",
      "[[0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]], shape=(4, 3), dtype=float64)\n",
      "tf.Tensor(\n",
      "[[0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]], shape=(4, 3), dtype=float64)\n",
      "tf.Tensor(\n",
      "[[0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]], shape=(4, 3), dtype=float64)\n",
      "tf.Tensor(\n",
      "[[0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]], shape=(4, 3), dtype=float64)\n",
      "tf.Tensor(\n",
      "[[0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]], shape=(4, 3), dtype=float64)\n",
      "tf.Tensor(\n",
      "[[0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]], shape=(4, 3), dtype=float64)\n",
      "tf.Tensor(\n",
      "[[0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]], shape=(4, 3), dtype=float64)\n",
      "tf.Tensor(\n",
      "[[0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]], shape=(4, 3), dtype=float64)\n",
      "tf.Tensor(\n",
      "[[0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]], shape=(4, 3), dtype=float64)\n",
      "tf.Tensor(\n",
      "[[0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]], shape=(4, 3), dtype=float64)\n",
      "tf.Tensor(\n",
      "[[0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]], shape=(4, 3), dtype=float64)\n",
      "tf.Tensor(\n",
      "[[0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]], shape=(4, 3), dtype=float64)\n",
      "tf.Tensor(\n",
      "[[0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]], shape=(4, 3), dtype=float64)\n",
      "tf.Tensor(\n",
      "[[0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]], shape=(4, 3), dtype=float64)\n",
      "tf.Tensor(\n",
      "[[0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]], shape=(4, 3), dtype=float64)\n",
      "tf.Tensor(\n",
      "[[0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]], shape=(4, 3), dtype=float64)\n",
      "tf.Tensor(\n",
      "[[0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]], shape=(4, 3), dtype=float64)\n",
      "tf.Tensor(\n",
      "[[0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]], shape=(4, 3), dtype=float64)\n",
      "tf.Tensor(\n",
      "[[0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]], shape=(4, 3), dtype=float64)\n",
      "tf.Tensor(\n",
      "[[0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]], shape=(4, 3), dtype=float64)\n",
      "tf.Tensor(\n",
      "[[0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]], shape=(4, 3), dtype=float64)\n",
      "tf.Tensor(\n",
      "[[0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]], shape=(4, 3), dtype=float64)\n",
      "tf.Tensor(\n",
      "[[0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]], shape=(4, 3), dtype=float64)\n",
      "tf.Tensor(\n",
      "[[0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]], shape=(4, 3), dtype=float64)\n",
      "tf.Tensor(\n",
      "[[0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]], shape=(4, 3), dtype=float64)\n",
      "tf.Tensor(\n",
      "[[0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]], shape=(4, 3), dtype=float64)\n",
      "tf.Tensor(\n",
      "[[0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]], shape=(4, 3), dtype=float64)\n",
      "tf.Tensor(\n",
      "[[0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]], shape=(4, 3), dtype=float64)\n",
      "tf.Tensor(\n",
      "[[0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]], shape=(4, 3), dtype=float64)\n",
      "tf.Tensor(\n",
      "[[0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]], shape=(4, 3), dtype=float64)\n",
      "tf.Tensor(\n",
      "[[0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]], shape=(4, 3), dtype=float64)\n",
      "tf.Tensor(\n",
      "[[0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]], shape=(4, 3), dtype=float64)\n",
      "tf.Tensor(\n",
      "[[0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]], shape=(4, 3), dtype=float64)\n",
      "tf.Tensor(\n",
      "[[0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]], shape=(4, 3), dtype=float64)\n",
      "tf.Tensor(\n",
      "[[0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]], shape=(4, 3), dtype=float64)\n",
      "tf.Tensor(\n",
      "[[0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]], shape=(4, 3), dtype=float64)\n",
      "tf.Tensor(\n",
      "[[0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]], shape=(4, 3), dtype=float64)\n",
      "tf.Tensor(\n",
      "[[0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]], shape=(4, 3), dtype=float64)\n",
      "tf.Tensor(\n",
      "[[0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]], shape=(4, 3), dtype=float64)\n",
      "tf.Tensor(\n",
      "[[0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]], shape=(4, 3), dtype=float64)\n",
      "tf.Tensor(\n",
      "[[0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]], shape=(4, 3), dtype=float64)\n",
      "tf.Tensor(\n",
      "[[0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]], shape=(4, 3), dtype=float64)\n",
      "tf.Tensor(\n",
      "[[0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]], shape=(4, 3), dtype=float64)\n",
      "tf.Tensor(\n",
      "[[0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]], shape=(4, 3), dtype=float64)\n",
      "tf.Tensor(\n",
      "[[0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]], shape=(4, 3), dtype=float64)\n",
      "tf.Tensor(\n",
      "[[0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]], shape=(4, 3), dtype=float64)\n",
      "tf.Tensor(\n",
      "[[0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]], shape=(4, 3), dtype=float64)\n",
      "tf.Tensor(\n",
      "[[0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]], shape=(4, 3), dtype=float64)\n",
      "tf.Tensor(\n",
      "[[0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]], shape=(4, 3), dtype=float64)\n",
      "tf.Tensor(\n",
      "[[0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]], shape=(4, 3), dtype=float64)\n",
      "tf.Tensor(\n",
      "[[0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]], shape=(4, 3), dtype=float64)\n",
      "tf.Tensor(\n",
      "[[0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]], shape=(4, 3), dtype=float64)\n",
      "tf.Tensor(\n",
      "[[0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]], shape=(4, 3), dtype=float64)\n",
      "tf.Tensor(\n",
      "[[0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]], shape=(4, 3), dtype=float64)\n",
      "tf.Tensor(\n",
      "[[0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]], shape=(4, 3), dtype=float64)\n",
      "tf.Tensor(\n",
      "[[0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]], shape=(4, 3), dtype=float64)\n",
      "tf.Tensor(\n",
      "[[0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]], shape=(4, 3), dtype=float64)\n",
      "tf.Tensor(\n",
      "[[0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]], shape=(4, 3), dtype=float64)\n",
      "tf.Tensor(\n",
      "[[0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]], shape=(4, 3), dtype=float64)\n",
      "tf.Tensor(\n",
      "[[0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]], shape=(4, 3), dtype=float64)\n",
      "tf.Tensor(\n",
      "[[0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]], shape=(4, 3), dtype=float64)\n",
      "tf.Tensor(\n",
      "[[0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]], shape=(4, 3), dtype=float64)\n",
      "tf.Tensor(\n",
      "[[0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]], shape=(4, 3), dtype=float64)\n",
      "tf.Tensor(\n",
      "[[0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]], shape=(4, 3), dtype=float64)\n",
      "tf.Tensor(\n",
      "[[0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]], shape=(4, 3), dtype=float64)\n",
      "tf.Tensor(\n",
      "[[0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]], shape=(4, 3), dtype=float64)\n",
      "tf.Tensor(\n",
      "[[0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]], shape=(4, 3), dtype=float64)\n",
      "tf.Tensor(\n",
      "[[0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]], shape=(4, 3), dtype=float64)\n",
      "tf.Tensor(\n",
      "[[0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]], shape=(4, 3), dtype=float64)\n",
      "tf.Tensor(\n",
      "[[0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]], shape=(4, 3), dtype=float64)\n",
      "tf.Tensor(\n",
      "[[0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]], shape=(4, 3), dtype=float64)\n",
      "tf.Tensor(\n",
      "[[0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]], shape=(4, 3), dtype=float64)\n",
      "tf.Tensor(\n",
      "[[0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]], shape=(4, 3), dtype=float64)\n",
      "tf.Tensor(\n",
      "[[0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]], shape=(4, 3), dtype=float64)\n",
      "tf.Tensor(\n",
      "[[0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]], shape=(4, 3), dtype=float64)\n",
      "tf.Tensor(\n",
      "[[0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]], shape=(4, 3), dtype=float64)\n",
      "tf.Tensor(\n",
      "[[0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]], shape=(4, 3), dtype=float64)\n",
      "tf.Tensor(\n",
      "[[0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]], shape=(4, 3), dtype=float64)\n",
      "tf.Tensor(\n",
      "[[0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]], shape=(4, 3), dtype=float64)\n",
      "tf.Tensor(\n",
      "[[0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]], shape=(4, 3), dtype=float64)\n",
      "tf.Tensor(\n",
      "[[0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]], shape=(4, 3), dtype=float64)\n",
      "tf.Tensor(\n",
      "[[0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]], shape=(4, 3), dtype=float64)\n",
      "tf.Tensor(\n",
      "[[0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]], shape=(4, 3), dtype=float64)\n",
      "tf.Tensor(\n",
      "[[0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]], shape=(4, 3), dtype=float64)\n",
      "tf.Tensor(\n",
      "[[0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]], shape=(4, 3), dtype=float64)\n",
      "tf.Tensor(\n",
      "[[0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]], shape=(4, 3), dtype=float64)\n",
      "tf.Tensor(\n",
      "[[0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]], shape=(4, 3), dtype=float64)\n",
      "tf.Tensor(\n",
      "[[0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]], shape=(4, 3), dtype=float64)\n",
      "tf.Tensor(\n",
      "[[0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]], shape=(4, 3), dtype=float64)\n",
      "tf.Tensor(\n",
      "[[0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]], shape=(4, 3), dtype=float64)\n",
      "tf.Tensor(\n",
      "[[0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]], shape=(4, 3), dtype=float64)\n",
      "tf.Tensor(\n",
      "[[0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]], shape=(4, 3), dtype=float64)\n",
      "tf.Tensor(\n",
      "[[0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]], shape=(4, 3), dtype=float64)\n",
      "tf.Tensor(\n",
      "[[0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]], shape=(4, 3), dtype=float64)\n",
      "tf.Tensor(\n",
      "[[0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]], shape=(4, 3), dtype=float64)\n",
      "tf.Tensor(\n",
      "[[0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]], shape=(4, 3), dtype=float64)\n",
      "tf.Tensor(\n",
      "[[0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]], shape=(4, 3), dtype=float64)\n",
      "tf.Tensor(\n",
      "[[0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]], shape=(4, 3), dtype=float64)\n",
      "tf.Tensor(\n",
      "[[0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]], shape=(4, 3), dtype=float64)\n",
      "tf.Tensor(\n",
      "[[0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]], shape=(4, 3), dtype=float64)\n",
      "tf.Tensor(\n",
      "[[0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]], shape=(4, 3), dtype=float64)\n",
      "tf.Tensor(\n",
      "[[0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]], shape=(4, 3), dtype=float64)\n",
      "tf.Tensor(\n",
      "[[0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]], shape=(4, 3), dtype=float64)\n",
      "tf.Tensor(\n",
      "[[0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]], shape=(4, 3), dtype=float64)\n",
      "tf.Tensor(\n",
      "[[0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]], shape=(4, 3), dtype=float64)\n",
      "tf.Tensor(\n",
      "[[0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]], shape=(4, 3), dtype=float64)\n",
      "tf.Tensor(\n",
      "[[0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]], shape=(4, 3), dtype=float64)\n",
      "tf.Tensor(\n",
      "[[0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]], shape=(4, 3), dtype=float64)\n",
      "tf.Tensor(\n",
      "[[0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]], shape=(4, 3), dtype=float64)\n",
      "tf.Tensor(\n",
      "[[0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]], shape=(4, 3), dtype=float64)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]], shape=(4, 3), dtype=float64)\n",
      "tf.Tensor(\n",
      "[[0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]], shape=(4, 3), dtype=float64)\n",
      "tf.Tensor(\n",
      "[[0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]], shape=(4, 3), dtype=float64)\n",
      "tf.Tensor(\n",
      "[[0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]], shape=(4, 3), dtype=float64)\n",
      "tf.Tensor(\n",
      "[[0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]], shape=(4, 3), dtype=float64)\n",
      "tf.Tensor(\n",
      "[[0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]], shape=(4, 3), dtype=float64)\n",
      "tf.Tensor(\n",
      "[[0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]], shape=(4, 3), dtype=float64)\n",
      "tf.Tensor(\n",
      "[[0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]], shape=(4, 3), dtype=float64)\n",
      "tf.Tensor(\n",
      "[[0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]], shape=(4, 3), dtype=float64)\n",
      "tf.Tensor(\n",
      "[[0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]], shape=(4, 3), dtype=float64)\n",
      "tf.Tensor(\n",
      "[[0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]], shape=(4, 3), dtype=float64)\n",
      "tf.Tensor(\n",
      "[[0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]], shape=(4, 3), dtype=float64)\n",
      "tf.Tensor(\n",
      "[[0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]], shape=(4, 3), dtype=float64)\n",
      "tf.Tensor(\n",
      "[[0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]], shape=(4, 3), dtype=float64)\n",
      "tf.Tensor(\n",
      "[[0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]], shape=(4, 3), dtype=float64)\n",
      "tf.Tensor(\n",
      "[[0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]], shape=(4, 3), dtype=float64)\n",
      "tf.Tensor(\n",
      "[[0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]], shape=(4, 3), dtype=float64)\n",
      "tf.Tensor(\n",
      "[[0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]], shape=(4, 3), dtype=float64)\n",
      "tf.Tensor(\n",
      "[[0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]], shape=(4, 3), dtype=float64)\n",
      "tf.Tensor(\n",
      "[[0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]], shape=(4, 3), dtype=float64)\n",
      "tf.Tensor(\n",
      "[[0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]], shape=(4, 3), dtype=float64)\n",
      "tf.Tensor(\n",
      "[[0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]], shape=(4, 3), dtype=float64)\n",
      "tf.Tensor(\n",
      "[[0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]], shape=(4, 3), dtype=float64)\n",
      "tf.Tensor(\n",
      "[[0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]], shape=(4, 3), dtype=float64)\n",
      "tf.Tensor(\n",
      "[[0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]], shape=(4, 3), dtype=float64)\n",
      "tf.Tensor(\n",
      "[[0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]], shape=(4, 3), dtype=float64)\n",
      "tf.Tensor(\n",
      "[[0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]], shape=(4, 3), dtype=float64)\n",
      "tf.Tensor(\n",
      "[[0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]], shape=(4, 3), dtype=float64)\n",
      "tf.Tensor(\n",
      "[[0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]], shape=(4, 3), dtype=float64)\n",
      "tf.Tensor(\n",
      "[[0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]], shape=(4, 3), dtype=float64)\n",
      "tf.Tensor(\n",
      "[[0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]], shape=(4, 3), dtype=float64)\n",
      "tf.Tensor(\n",
      "[[0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]], shape=(4, 3), dtype=float64)\n",
      "tf.Tensor(\n",
      "[[0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]], shape=(4, 3), dtype=float64)\n",
      "tf.Tensor(\n",
      "[[0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]], shape=(4, 3), dtype=float64)\n",
      "tf.Tensor(\n",
      "[[0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]], shape=(4, 3), dtype=float64)\n",
      "tf.Tensor(\n",
      "[[0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]], shape=(4, 3), dtype=float64)\n",
      "tf.Tensor(\n",
      "[[0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]], shape=(4, 3), dtype=float64)\n",
      "tf.Tensor(\n",
      "[[0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]], shape=(4, 3), dtype=float64)\n",
      "tf.Tensor(\n",
      "[[0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]], shape=(4, 3), dtype=float64)\n",
      "tf.Tensor(\n",
      "[[0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]], shape=(4, 3), dtype=float64)\n",
      "tf.Tensor(\n",
      "[[0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]], shape=(4, 3), dtype=float64)\n",
      "tf.Tensor(\n",
      "[[0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]], shape=(4, 3), dtype=float64)\n",
      "tf.Tensor(\n",
      "[[0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]], shape=(4, 3), dtype=float64)\n",
      "tf.Tensor(\n",
      "[[0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]], shape=(4, 3), dtype=float64)\n",
      "tf.Tensor(\n",
      "[[0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]], shape=(4, 3), dtype=float64)\n",
      "tf.Tensor(\n",
      "[[0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]], shape=(4, 3), dtype=float64)\n",
      "tf.Tensor(\n",
      "[[0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]], shape=(4, 3), dtype=float64)\n",
      "tf.Tensor(\n",
      "[[0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]], shape=(4, 3), dtype=float64)\n",
      "tf.Tensor(\n",
      "[[0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]], shape=(4, 3), dtype=float64)\n",
      "tf.Tensor(\n",
      "[[0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]], shape=(4, 3), dtype=float64)\n",
      "tf.Tensor(\n",
      "[[0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]], shape=(4, 3), dtype=float64)\n",
      "tf.Tensor(\n",
      "[[0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]], shape=(4, 3), dtype=float64)\n",
      "tf.Tensor(\n",
      "[[0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]], shape=(4, 3), dtype=float64)\n",
      "tf.Tensor(\n",
      "[[0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]], shape=(4, 3), dtype=float64)\n",
      "tf.Tensor(\n",
      "[[0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]], shape=(4, 3), dtype=float64)\n",
      "tf.Tensor(\n",
      "[[0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]], shape=(4, 3), dtype=float64)\n",
      "tf.Tensor(\n",
      "[[0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]], shape=(4, 3), dtype=float64)\n",
      "tf.Tensor(\n",
      "[[0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]], shape=(4, 3), dtype=float64)\n",
      "tf.Tensor(\n",
      "[[0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]], shape=(4, 3), dtype=float64)\n",
      "tf.Tensor(\n",
      "[[0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]], shape=(4, 3), dtype=float64)\n",
      "tf.Tensor(\n",
      "[[0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]], shape=(4, 3), dtype=float64)\n",
      "tf.Tensor(\n",
      "[[0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]], shape=(4, 3), dtype=float64)\n",
      "tf.Tensor(\n",
      "[[0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]], shape=(4, 3), dtype=float64)\n",
      "tf.Tensor(\n",
      "[[0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]], shape=(4, 3), dtype=float64)\n",
      "tf.Tensor(\n",
      "[[0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]], shape=(4, 3), dtype=float64)\n",
      "tf.Tensor(\n",
      "[[0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]], shape=(4, 3), dtype=float64)\n",
      "tf.Tensor(\n",
      "[[0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]], shape=(4, 3), dtype=float64)\n",
      "tf.Tensor(\n",
      "[[0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]], shape=(4, 3), dtype=float64)\n",
      "tf.Tensor(\n",
      "[[0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]], shape=(4, 3), dtype=float64)\n",
      "tf.Tensor(\n",
      "[[0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]], shape=(4, 3), dtype=float64)\n",
      "tf.Tensor(\n",
      "[[0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]], shape=(4, 3), dtype=float64)\n",
      "tf.Tensor(\n",
      "[[0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]], shape=(4, 3), dtype=float64)\n",
      "tf.Tensor(\n",
      "[[0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]], shape=(4, 3), dtype=float64)\n",
      "tf.Tensor(\n",
      "[[0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]], shape=(4, 3), dtype=float64)\n",
      "tf.Tensor(\n",
      "[[0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]], shape=(4, 3), dtype=float64)\n",
      "tf.Tensor(\n",
      "[[0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]], shape=(4, 3), dtype=float64)\n",
      "tf.Tensor(\n",
      "[[0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]], shape=(4, 3), dtype=float64)\n",
      "tf.Tensor(\n",
      "[[0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]], shape=(4, 3), dtype=float64)\n",
      "tf.Tensor(\n",
      "[[0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]], shape=(4, 3), dtype=float64)\n",
      "tf.Tensor(\n",
      "[[0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]], shape=(4, 3), dtype=float64)\n",
      "tf.Tensor(\n",
      "[[0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]], shape=(4, 3), dtype=float64)\n",
      "tf.Tensor(\n",
      "[[0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]], shape=(4, 3), dtype=float64)\n",
      "tf.Tensor(\n",
      "[[0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]], shape=(4, 3), dtype=float64)\n",
      "tf.Tensor(\n",
      "[[0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]], shape=(4, 3), dtype=float64)\n",
      "tf.Tensor(\n",
      "[[0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]], shape=(4, 3), dtype=float64)\n",
      "tf.Tensor(\n",
      "[[0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]], shape=(4, 3), dtype=float64)\n",
      "tf.Tensor(\n",
      "[[0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]], shape=(4, 3), dtype=float64)\n",
      "tf.Tensor(\n",
      "[[0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]], shape=(4, 3), dtype=float64)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "tf.keras.backend.set_floatx('float64')\n",
    "\n",
    "### Data preparation\n",
    "data = np.genfromtxt('iris_full_sm.csv', delimiter=',', skip_header=1)\n",
    "X = data[:,0:4]\n",
    "y = data[:,4:]\n",
    "\n",
    "\n",
    "# predict    \n",
    "def predict(x, W, b):\n",
    "    return tf.math.softmax(tf.matmul(x, W) + b)\n",
    "\n",
    "    \n",
    "### weights\n",
    "W = tf.Variable(tf.random.normal((4, 3), dtype=tf.float64)*100, dtype=tf.float64)\n",
    "b = tf.Variable(tf.random.normal((3,), dtype=tf.float64)*100)\n",
    "\n",
    "### training\n",
    "learning_rate = 0.1\n",
    "num_epochs = 200\n",
    "\n",
    "### loss function\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy()\n",
    "\n",
    "largeW_losses = [] # for debug\n",
    "for epoch in range(num_epochs):\n",
    "    with tf.GradientTape() as t:\n",
    "        # output\n",
    "        output = predict(X, W, b)\n",
    "        \n",
    "        # loss\n",
    "        loss_v = loss(y, output)\n",
    "        largeW_losses.append(loss_v.numpy())\n",
    "\n",
    "        # gradient\n",
    "        dW, db = t.gradient(loss_v, [W, b])\n",
    "        print(dW)\n",
    "        # update\n",
    "        W.assign_sub(learning_rate * dW) \n",
    "        b.assign_sub(learning_rate * db)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plt.yscale('linear')\n",
    "# plt.title('symlog')\n",
    "# plt.ylim(10.74, 10.76)\n",
    "plt.grid(True)\n",
    "plt.plot(largeW_losses)\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Theo như quan sát ở trên thì khi khởi tạo weight quá cao sẽ dẫn đến việc loss dường như không thay đổi (có thể không học được gì sau các lần train). Đây là vấ đề liên quan đến khởi tạo weights\n",
    "\n",
    "Các vấn đề  thường gặp:\n",
    "- Khởi tạo các weights là 1 constant, bias = 0, dẫn đến việc các units trong hidden layers học cùng những thứ giống nhau dẫn đến việc tăng số units cũng trở nên vô ít. Ví dụ 1 hidden layer có 2 units, nếu khới tại weigths = alpha và bias = 0 với input là 2 features (x1, x2) thì cả 2 unit có kết quả output như nhau alpha*x1 + alpha*x2. Và khi trong quá trình backward cả 2 unit có cùng loss, dẫn đến việc các units khác nhau không học các khía cạnh khác nhau của các features.\n",
    "\n",
    "**=> Dẫn đến việc phải khởi tạo weight random.**\n",
    "\n",
    "- Nhưng khi khởi tạo weight random phát sinh ra 2 vấn đề, weights quá lớn hoặc weights quá nhỏ sẽ dẫn đến việc model không thể hội tụ hoặc học rất chậm. 2 vấn đề này được gọi là exploding và vanishing gradients.\n",
    "\n",
    "  + Exploding: Giả xử khi khởi tạo weight rất lớn, và model có rất nhiều layers, thì khi thực hiện forward dẫn đến output của từng layer có giá trị tăng theo cấp số mũ nếu như biến khai báo không đủ sẽ dẫn đến overflow, thứ 2 trong quá trình backward loss quá lớn dẫn đến việc gradient sẽ đi 1 khoảng lớn và giao động liên tục điểm cực tiểu và không thể hội tụ được.\n",
    "  \n",
    "  + Vanishing: Ngược lại khi weight quá nhỏ dẫn đến việc giá trị output giảm theo cấp số  mũ và khi backward giá trị update gần như bằng không dẫn đến việc model gần như không học thêm nữa hoặc học rất chậm.\n",
    "  \n",
    "**=> Để giải quyết được vấn đề thì có 2 yếu caauf: mean của các activations phải là 0, và variance của các activations phải gần như không đổi hoặc thay đổi rất ít khi qua các layers trong model**\n",
    "\n",
    "[Xavier initialization](http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf?hc_location=ufi]) được khuyên dùng và đây cũng là default initialize weight trong Dense của tensorflow.\n",
    "\n",
    "\n",
    "![image.png](attachment:image.png)\n",
    "\n",
    "Theo công thức trên, layer thứ l ký hiệu là l, bias của layer l khởi tạo = 0, weights được chọn random từ normal distribution, với mean = 0, variance = nghịch đảo số lượng neurons (units) của layer trước \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Vấn đề ở trên weights lớn nhưng loss không thay đổi do weight khởi tạo lớn dẫn đến output qua sigmoid sẽ gần như bằng 1, và gradient dường như không đổi vì khi vẽ ra slope của hàm sigmoid tiệm cận 1 gần như là 1 đường thẳng nên loss không đổi**\n",
    "\n",
    "**Vấn đề em không hiểu là tại sao gradient của weight lại bằng 0**\n",
    "\n",
    "**Khi dùng Xavier initialization vấn đề được giải quyết**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "tf.keras.backend.set_floatx('float64')\n",
    "\n",
    "### Data preparation\n",
    "data = np.genfromtxt('iris_full_sm.csv', delimiter=',', skip_header=1)\n",
    "X = data[:,0:4]\n",
    "y = data[:,4:]\n",
    "\n",
    "\n",
    "# predict    \n",
    "def predict(x, W, b):\n",
    "    return tf.math.softmax(tf.matmul(x, W) + b)\n",
    "\n",
    "    \n",
    "### Xavier initial weights\n",
    "W = tf.Variable(tf.random.normal((4, 3), mean=0, stddev= 1/X.shape[1] , dtype=tf.float64), dtype=tf.float64)\n",
    "b = tf.Variable(0, dtype=tf.float64)\n",
    "\n",
    "### training\n",
    "learning_rate = 0.1\n",
    "num_epochs = 200\n",
    "\n",
    "### loss function\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy()\n",
    "\n",
    "xavier_init_losses = [] # for debug\n",
    "for epoch in range(num_epochs):\n",
    "    with tf.GradientTape() as t:\n",
    "        # output\n",
    "        output = predict(X, W, b)\n",
    "        \n",
    "        # loss\n",
    "        loss_v = loss(y, output)\n",
    "        xavier_init_losses.append(loss_v.numpy())\n",
    "        \n",
    "        # gradient\n",
    "        dW, db = t.gradient(loss_v, [W, b])\n",
    "        \n",
    "        # update\n",
    "        W.assign_sub(learning_rate * dW) \n",
    "        b.assign_sub(learning_rate * db)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(xavier_init_losses)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "https://towardsdatascience.com/weight-initialization-techniques-in-neural-networks-26c649eb3b78\n",
    "\n",
    "https://towardsdatascience.com/weight-initialization-in-neural-networks-a-journey-from-the-basics-to-kaiming-954fb9b47c79"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.deeplearning.ai/ai-notes/initialization/index.html\n",
    "\n",
    "http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf?hc_location=ufi]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
