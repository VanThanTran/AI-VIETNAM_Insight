{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_DEVICE_ORDER=PCI_BUS_ID\n",
      "env: CUDA_VISIBLE_DEVICES=0\n",
      "Epoch 1/150\n",
      "196/196 [==============================] - 13s 68ms/step - loss: 2.3468 - accuracy: 0.3242 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00\n",
      "Epoch 2/150\n",
      "196/196 [==============================] - 7s 37ms/step - loss: 1.6651 - accuracy: 0.4580 - val_loss: 1.7566 - val_accuracy: 0.4690\n",
      "Epoch 3/150\n",
      "196/196 [==============================] - 7s 37ms/step - loss: 1.4723 - accuracy: 0.5348 - val_loss: 1.3421 - val_accuracy: 0.5817\n",
      "Epoch 4/150\n",
      "196/196 [==============================] - 7s 37ms/step - loss: 1.3401 - accuracy: 0.5814 - val_loss: 1.1876 - val_accuracy: 0.6441\n",
      "Epoch 5/150\n",
      "196/196 [==============================] - 7s 37ms/step - loss: 1.2238 - accuracy: 0.6265 - val_loss: 1.0678 - val_accuracy: 0.6835\n",
      "Epoch 6/150\n",
      "196/196 [==============================] - 7s 37ms/step - loss: 1.1352 - accuracy: 0.6599 - val_loss: 1.0398 - val_accuracy: 0.7064\n",
      "Epoch 7/150\n",
      "196/196 [==============================] - 7s 37ms/step - loss: 1.0697 - accuracy: 0.6834 - val_loss: 0.9195 - val_accuracy: 0.7391\n",
      "Epoch 8/150\n",
      "196/196 [==============================] - 7s 37ms/step - loss: 1.0128 - accuracy: 0.7037 - val_loss: 0.9731 - val_accuracy: 0.7226\n",
      "Epoch 9/150\n",
      "196/196 [==============================] - 7s 37ms/step - loss: 0.9699 - accuracy: 0.7199 - val_loss: 0.8538 - val_accuracy: 0.7581\n",
      "Epoch 10/150\n",
      "196/196 [==============================] - 7s 37ms/step - loss: 0.9427 - accuracy: 0.7319 - val_loss: 0.8661 - val_accuracy: 0.7620\n",
      "Epoch 11/150\n",
      "196/196 [==============================] - 7s 37ms/step - loss: 0.8958 - accuracy: 0.7476 - val_loss: 0.8431 - val_accuracy: 0.7702\n",
      "Epoch 12/150\n",
      "196/196 [==============================] - 7s 37ms/step - loss: 0.8750 - accuracy: 0.7574 - val_loss: 0.7899 - val_accuracy: 0.7879\n",
      "Epoch 13/150\n",
      "196/196 [==============================] - 7s 37ms/step - loss: 0.8500 - accuracy: 0.7654 - val_loss: 0.7836 - val_accuracy: 0.7929\n",
      "Epoch 14/150\n",
      "196/196 [==============================] - 7s 38ms/step - loss: 0.8337 - accuracy: 0.7724 - val_loss: 0.7328 - val_accuracy: 0.8076\n",
      "Epoch 15/150\n",
      "196/196 [==============================] - 7s 37ms/step - loss: 0.8132 - accuracy: 0.7795 - val_loss: 0.7380 - val_accuracy: 0.8065\n",
      "Epoch 16/150\n",
      "196/196 [==============================] - 7s 37ms/step - loss: 0.7974 - accuracy: 0.7872 - val_loss: 0.7255 - val_accuracy: 0.8091\n",
      "Epoch 17/150\n",
      "196/196 [==============================] - 7s 37ms/step - loss: 0.7851 - accuracy: 0.7914 - val_loss: 0.7576 - val_accuracy: 0.8061\n",
      "Epoch 18/150\n",
      "196/196 [==============================] - 7s 37ms/step - loss: 0.7716 - accuracy: 0.7985 - val_loss: 0.7312 - val_accuracy: 0.8153\n",
      "Epoch 19/150\n",
      "196/196 [==============================] - 7s 37ms/step - loss: 0.7657 - accuracy: 0.8002 - val_loss: 0.6909 - val_accuracy: 0.8250\n",
      "Epoch 20/150\n",
      "196/196 [==============================] - 7s 37ms/step - loss: 0.7522 - accuracy: 0.8059 - val_loss: 0.6869 - val_accuracy: 0.8291\n",
      "Epoch 21/150\n",
      "196/196 [==============================] - 7s 38ms/step - loss: 0.7412 - accuracy: 0.8105 - val_loss: 0.6787 - val_accuracy: 0.8341\n",
      "Epoch 22/150\n",
      "196/196 [==============================] - 7s 37ms/step - loss: 0.7369 - accuracy: 0.8138 - val_loss: 0.6791 - val_accuracy: 0.8347\n",
      "Epoch 23/150\n",
      "196/196 [==============================] - 7s 37ms/step - loss: 0.7274 - accuracy: 0.8163 - val_loss: 0.6937 - val_accuracy: 0.8318\n",
      "Epoch 24/150\n",
      "196/196 [==============================] - 7s 37ms/step - loss: 0.7205 - accuracy: 0.8204 - val_loss: 0.6663 - val_accuracy: 0.8430\n",
      "Epoch 25/150\n",
      "196/196 [==============================] - 7s 38ms/step - loss: 0.7150 - accuracy: 0.8229 - val_loss: 0.6432 - val_accuracy: 0.8502\n",
      "Epoch 26/150\n",
      "196/196 [==============================] - 7s 38ms/step - loss: 0.7070 - accuracy: 0.8279 - val_loss: 0.6703 - val_accuracy: 0.8411\n",
      "Epoch 27/150\n",
      "196/196 [==============================] - 7s 37ms/step - loss: 0.6954 - accuracy: 0.8329 - val_loss: 0.6530 - val_accuracy: 0.8488\n",
      "Epoch 28/150\n",
      "196/196 [==============================] - 7s 38ms/step - loss: 0.7014 - accuracy: 0.8311 - val_loss: 0.6798 - val_accuracy: 0.8458\n",
      "Epoch 29/150\n",
      "196/196 [==============================] - 7s 38ms/step - loss: 0.6898 - accuracy: 0.8352 - val_loss: 0.6425 - val_accuracy: 0.8559\n",
      "Epoch 30/150\n",
      "196/196 [==============================] - 7s 38ms/step - loss: 0.6884 - accuracy: 0.8368 - val_loss: 0.6612 - val_accuracy: 0.8485\n",
      "Epoch 31/150\n",
      "196/196 [==============================] - 7s 37ms/step - loss: 0.6882 - accuracy: 0.8395 - val_loss: 0.6323 - val_accuracy: 0.8614\n",
      "Epoch 32/150\n",
      "196/196 [==============================] - 7s 37ms/step - loss: 0.6852 - accuracy: 0.8404 - val_loss: 0.6395 - val_accuracy: 0.8547\n",
      "Epoch 33/150\n",
      "196/196 [==============================] - 7s 38ms/step - loss: 0.6757 - accuracy: 0.8435 - val_loss: 0.6538 - val_accuracy: 0.8558\n",
      "Epoch 34/150\n",
      "196/196 [==============================] - 7s 37ms/step - loss: 0.6765 - accuracy: 0.8445 - val_loss: 0.6431 - val_accuracy: 0.8602\n",
      "Epoch 35/150\n",
      "196/196 [==============================] - 7s 37ms/step - loss: 0.6704 - accuracy: 0.8472 - val_loss: 0.6266 - val_accuracy: 0.8665\n",
      "Epoch 36/150\n",
      "196/196 [==============================] - 7s 38ms/step - loss: 0.6731 - accuracy: 0.8460 - val_loss: 0.6333 - val_accuracy: 0.8623\n",
      "Epoch 37/150\n",
      "196/196 [==============================] - 7s 37ms/step - loss: 0.6719 - accuracy: 0.8477 - val_loss: 0.6336 - val_accuracy: 0.8643\n",
      "Epoch 38/150\n",
      "196/196 [==============================] - 7s 37ms/step - loss: 0.6588 - accuracy: 0.8530 - val_loss: 0.6256 - val_accuracy: 0.8701\n",
      "Epoch 39/150\n",
      "196/196 [==============================] - 7s 37ms/step - loss: 0.6641 - accuracy: 0.8514 - val_loss: 0.6184 - val_accuracy: 0.8691\n",
      "Epoch 40/150\n",
      "196/196 [==============================] - 7s 37ms/step - loss: 0.6605 - accuracy: 0.8555 - val_loss: 0.6215 - val_accuracy: 0.8694\n",
      "Epoch 41/150\n",
      "196/196 [==============================] - 7s 38ms/step - loss: 0.6598 - accuracy: 0.8533 - val_loss: 0.6304 - val_accuracy: 0.8682\n",
      "Epoch 42/150\n",
      "196/196 [==============================] - 7s 38ms/step - loss: 0.6532 - accuracy: 0.8584 - val_loss: 0.6344 - val_accuracy: 0.8648\n",
      "Epoch 43/150\n",
      "196/196 [==============================] - 7s 37ms/step - loss: 0.6592 - accuracy: 0.8570 - val_loss: 0.6641 - val_accuracy: 0.8583\n",
      "Epoch 44/150\n",
      "196/196 [==============================] - 7s 38ms/step - loss: 0.6500 - accuracy: 0.8602 - val_loss: 0.6274 - val_accuracy: 0.8709\n",
      "Epoch 45/150\n",
      "196/196 [==============================] - 7s 38ms/step - loss: 0.6493 - accuracy: 0.8621 - val_loss: 0.6365 - val_accuracy: 0.8681\n",
      "Epoch 46/150\n",
      "196/196 [==============================] - 7s 38ms/step - loss: 0.6472 - accuracy: 0.8623 - val_loss: 0.6321 - val_accuracy: 0.8693\n",
      "Epoch 47/150\n",
      "196/196 [==============================] - 7s 37ms/step - loss: 0.6485 - accuracy: 0.8624 - val_loss: 0.6217 - val_accuracy: 0.8732\n",
      "Epoch 48/150\n",
      "196/196 [==============================] - 7s 37ms/step - loss: 0.6464 - accuracy: 0.8619 - val_loss: 0.6222 - val_accuracy: 0.8750\n",
      "Epoch 49/150\n",
      "196/196 [==============================] - 7s 38ms/step - loss: 0.6478 - accuracy: 0.8631 - val_loss: 0.6391 - val_accuracy: 0.8683\n",
      "Epoch 50/150\n",
      "196/196 [==============================] - 7s 38ms/step - loss: 0.6409 - accuracy: 0.8659 - val_loss: 0.6318 - val_accuracy: 0.8752\n",
      "Epoch 51/150\n",
      "196/196 [==============================] - 7s 38ms/step - loss: 0.6436 - accuracy: 0.8660 - val_loss: 0.6338 - val_accuracy: 0.8756\n",
      "Epoch 52/150\n",
      "196/196 [==============================] - 7s 38ms/step - loss: 0.6348 - accuracy: 0.8687 - val_loss: 0.6295 - val_accuracy: 0.8749\n",
      "Epoch 53/150\n",
      "196/196 [==============================] - 7s 38ms/step - loss: 0.6366 - accuracy: 0.8699 - val_loss: 0.6306 - val_accuracy: 0.8757\n",
      "Epoch 54/150\n",
      "196/196 [==============================] - 7s 38ms/step - loss: 0.6370 - accuracy: 0.8682 - val_loss: 0.6221 - val_accuracy: 0.8782\n",
      "Epoch 55/150\n",
      "196/196 [==============================] - 7s 38ms/step - loss: 0.6396 - accuracy: 0.8664 - val_loss: 0.6366 - val_accuracy: 0.8750\n",
      "Epoch 56/150\n",
      "196/196 [==============================] - 7s 37ms/step - loss: 0.6327 - accuracy: 0.8708 - val_loss: 0.6292 - val_accuracy: 0.8775\n",
      "Epoch 57/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "196/196 [==============================] - 7s 38ms/step - loss: 0.6330 - accuracy: 0.8719 - val_loss: 0.6387 - val_accuracy: 0.8797\n",
      "Epoch 58/150\n",
      "196/196 [==============================] - 7s 38ms/step - loss: 0.6288 - accuracy: 0.8727 - val_loss: 0.6174 - val_accuracy: 0.8824\n",
      "Epoch 59/150\n",
      "196/196 [==============================] - 7s 38ms/step - loss: 0.6305 - accuracy: 0.8736 - val_loss: 0.6351 - val_accuracy: 0.8777\n",
      "Epoch 60/150\n",
      "196/196 [==============================] - 7s 38ms/step - loss: 0.6226 - accuracy: 0.8748 - val_loss: 0.6388 - val_accuracy: 0.8773\n",
      "Epoch 61/150\n",
      "196/196 [==============================] - 7s 38ms/step - loss: 0.6270 - accuracy: 0.8748 - val_loss: 0.6225 - val_accuracy: 0.8822\n",
      "Epoch 62/150\n",
      "196/196 [==============================] - 7s 38ms/step - loss: 0.6241 - accuracy: 0.8756 - val_loss: 0.6339 - val_accuracy: 0.8771\n",
      "Epoch 63/150\n",
      "196/196 [==============================] - 7s 38ms/step - loss: 0.6246 - accuracy: 0.8768 - val_loss: 0.6229 - val_accuracy: 0.8826\n",
      "Epoch 64/150\n",
      "196/196 [==============================] - 7s 38ms/step - loss: 0.6188 - accuracy: 0.8774 - val_loss: 0.6076 - val_accuracy: 0.8890\n",
      "Epoch 65/150\n",
      "196/196 [==============================] - 7s 38ms/step - loss: 0.6185 - accuracy: 0.8788 - val_loss: 0.6667 - val_accuracy: 0.8695\n",
      "Epoch 66/150\n",
      "196/196 [==============================] - 7s 38ms/step - loss: 0.6187 - accuracy: 0.8791 - val_loss: 0.6328 - val_accuracy: 0.8798\n",
      "Epoch 67/150\n",
      "196/196 [==============================] - 7s 37ms/step - loss: 0.6220 - accuracy: 0.8770 - val_loss: 0.6154 - val_accuracy: 0.8850\n",
      "Epoch 68/150\n",
      "196/196 [==============================] - 7s 38ms/step - loss: 0.6149 - accuracy: 0.8806 - val_loss: 0.6395 - val_accuracy: 0.8815\n",
      "Epoch 69/150\n",
      "196/196 [==============================] - 7s 38ms/step - loss: 0.6130 - accuracy: 0.8815 - val_loss: 0.6300 - val_accuracy: 0.8834\n",
      "Epoch 70/150\n",
      "196/196 [==============================] - 7s 38ms/step - loss: 0.6089 - accuracy: 0.8830 - val_loss: 0.5832 - val_accuracy: 0.8984\n",
      "Epoch 71/150\n",
      "196/196 [==============================] - 7s 38ms/step - loss: 0.5706 - accuracy: 0.8953 - val_loss: 0.5975 - val_accuracy: 0.8925\n",
      "Epoch 72/150\n",
      "196/196 [==============================] - 7s 38ms/step - loss: 0.5621 - accuracy: 0.8980 - val_loss: 0.5949 - val_accuracy: 0.8956\n",
      "Epoch 73/150\n",
      "196/196 [==============================] - 7s 38ms/step - loss: 0.5594 - accuracy: 0.8975 - val_loss: 0.5857 - val_accuracy: 0.8983\n",
      "Epoch 74/150\n",
      "196/196 [==============================] - 7s 37ms/step - loss: 0.5510 - accuracy: 0.8981 - val_loss: 0.5713 - val_accuracy: 0.8976\n",
      "Epoch 75/150\n",
      "196/196 [==============================] - 7s 38ms/step - loss: 0.5458 - accuracy: 0.8991 - val_loss: 0.5725 - val_accuracy: 0.9005\n",
      "Epoch 76/150\n",
      "196/196 [==============================] - 7s 38ms/step - loss: 0.5423 - accuracy: 0.9011 - val_loss: 0.5766 - val_accuracy: 0.8962\n",
      "Epoch 77/150\n",
      "196/196 [==============================] - 7s 38ms/step - loss: 0.5366 - accuracy: 0.9020 - val_loss: 0.5620 - val_accuracy: 0.9006\n",
      "Epoch 78/150\n",
      "196/196 [==============================] - 7s 38ms/step - loss: 0.5335 - accuracy: 0.9031 - val_loss: 0.5602 - val_accuracy: 0.8987\n",
      "Epoch 79/150\n",
      "196/196 [==============================] - 7s 38ms/step - loss: 0.5338 - accuracy: 0.9015 - val_loss: 0.5630 - val_accuracy: 0.8983\n",
      "Epoch 80/150\n",
      "196/196 [==============================] - 7s 38ms/step - loss: 0.5300 - accuracy: 0.9024 - val_loss: 0.5672 - val_accuracy: 0.8993\n",
      "Epoch 81/150\n",
      "196/196 [==============================] - 7s 38ms/step - loss: 0.5336 - accuracy: 0.9006 - val_loss: 0.5501 - val_accuracy: 0.9035\n",
      "Epoch 82/150\n",
      "196/196 [==============================] - 7s 38ms/step - loss: 0.5223 - accuracy: 0.9038 - val_loss: 0.5594 - val_accuracy: 0.8987\n",
      "Epoch 83/150\n",
      "196/196 [==============================] - 7s 38ms/step - loss: 0.5208 - accuracy: 0.9046 - val_loss: 0.5544 - val_accuracy: 0.8993\n",
      "Epoch 84/150\n",
      "196/196 [==============================] - 7s 38ms/step - loss: 0.5202 - accuracy: 0.9047 - val_loss: 0.5504 - val_accuracy: 0.9029\n",
      "Epoch 85/150\n",
      "196/196 [==============================] - 7s 38ms/step - loss: 0.5218 - accuracy: 0.9040 - val_loss: 0.5457 - val_accuracy: 0.9026\n",
      "Epoch 86/150\n",
      "196/196 [==============================] - 7s 37ms/step - loss: 0.5140 - accuracy: 0.9058 - val_loss: 0.5444 - val_accuracy: 0.9032\n",
      "Epoch 87/150\n",
      "196/196 [==============================] - 7s 38ms/step - loss: 0.5096 - accuracy: 0.9087 - val_loss: 0.5515 - val_accuracy: 0.9009\n",
      "Epoch 88/150\n",
      "196/196 [==============================] - 7s 38ms/step - loss: 0.5146 - accuracy: 0.9050 - val_loss: 0.5513 - val_accuracy: 0.8969\n",
      "Epoch 89/150\n",
      "196/196 [==============================] - 7s 38ms/step - loss: 0.5117 - accuracy: 0.9068 - val_loss: 0.5464 - val_accuracy: 0.9021\n",
      "Epoch 90/150\n",
      "196/196 [==============================] - 7s 38ms/step - loss: 0.5075 - accuracy: 0.9069 - val_loss: 0.5396 - val_accuracy: 0.9036\n",
      "Epoch 91/150\n",
      "196/196 [==============================] - 7s 38ms/step - loss: 0.5089 - accuracy: 0.9068 - val_loss: 0.5490 - val_accuracy: 0.9004\n",
      "Epoch 92/150\n",
      "196/196 [==============================] - 7s 38ms/step - loss: 0.5046 - accuracy: 0.9093 - val_loss: 0.5382 - val_accuracy: 0.9048\n",
      "Epoch 93/150\n",
      "196/196 [==============================] - 7s 38ms/step - loss: 0.4998 - accuracy: 0.9086 - val_loss: 0.5424 - val_accuracy: 0.9012\n",
      "Epoch 94/150\n",
      "196/196 [==============================] - 7s 38ms/step - loss: 0.4989 - accuracy: 0.9085 - val_loss: 0.5379 - val_accuracy: 0.9040\n",
      "Epoch 95/150\n",
      "196/196 [==============================] - 7s 38ms/step - loss: 0.5055 - accuracy: 0.9070 - val_loss: 0.5506 - val_accuracy: 0.8976\n",
      "Epoch 96/150\n",
      "196/196 [==============================] - 7s 38ms/step - loss: 0.5027 - accuracy: 0.9079 - val_loss: 0.5538 - val_accuracy: 0.9000\n",
      "Epoch 97/150\n",
      "196/196 [==============================] - 7s 38ms/step - loss: 0.5025 - accuracy: 0.9076 - val_loss: 0.5439 - val_accuracy: 0.9002\n",
      "Epoch 98/150\n",
      "196/196 [==============================] - 7s 38ms/step - loss: 0.4944 - accuracy: 0.9101 - val_loss: 0.5336 - val_accuracy: 0.9048\n",
      "Epoch 99/150\n",
      "196/196 [==============================] - 7s 38ms/step - loss: 0.4959 - accuracy: 0.9080 - val_loss: 0.5478 - val_accuracy: 0.9008\n",
      "Epoch 100/150\n",
      "196/196 [==============================] - 7s 38ms/step - loss: 0.4952 - accuracy: 0.9102 - val_loss: 0.5395 - val_accuracy: 0.9030\n",
      "Epoch 101/150\n",
      "196/196 [==============================] - 7s 38ms/step - loss: 0.4897 - accuracy: 0.9102 - val_loss: 0.5368 - val_accuracy: 0.9011\n",
      "Epoch 102/150\n",
      "196/196 [==============================] - 7s 38ms/step - loss: 0.4912 - accuracy: 0.9096 - val_loss: 0.5381 - val_accuracy: 0.9008\n",
      "Epoch 103/150\n",
      "196/196 [==============================] - 7s 38ms/step - loss: 0.4894 - accuracy: 0.9107 - val_loss: 0.5408 - val_accuracy: 0.9002\n",
      "Epoch 104/150\n",
      "196/196 [==============================] - 7s 38ms/step - loss: 0.4913 - accuracy: 0.9092 - val_loss: 0.5413 - val_accuracy: 0.9039\n",
      "Epoch 105/150\n",
      "196/196 [==============================] - 7s 38ms/step - loss: 0.4882 - accuracy: 0.9121 - val_loss: 0.5353 - val_accuracy: 0.9043\n",
      "Epoch 106/150\n",
      "196/196 [==============================] - 7s 38ms/step - loss: 0.4890 - accuracy: 0.9098 - val_loss: 0.5263 - val_accuracy: 0.9057\n",
      "Epoch 107/150\n",
      "196/196 [==============================] - 7s 38ms/step - loss: 0.4887 - accuracy: 0.9104 - val_loss: 0.5329 - val_accuracy: 0.9033\n",
      "Epoch 108/150\n",
      "196/196 [==============================] - 7s 38ms/step - loss: 0.4821 - accuracy: 0.9112 - val_loss: 0.5381 - val_accuracy: 0.9008\n",
      "Epoch 109/150\n",
      "196/196 [==============================] - 7s 37ms/step - loss: 0.4832 - accuracy: 0.9120 - val_loss: 0.5429 - val_accuracy: 0.9003\n",
      "Epoch 110/150\n",
      "196/196 [==============================] - 7s 38ms/step - loss: 0.4766 - accuracy: 0.9150 - val_loss: 0.5215 - val_accuracy: 0.9081\n",
      "Epoch 111/150\n",
      "196/196 [==============================] - 7s 38ms/step - loss: 0.4536 - accuracy: 0.9225 - val_loss: 0.5205 - val_accuracy: 0.9097\n",
      "Epoch 112/150\n",
      "196/196 [==============================] - 7s 38ms/step - loss: 0.4475 - accuracy: 0.9245 - val_loss: 0.5185 - val_accuracy: 0.9103\n",
      "Epoch 113/150\n",
      "196/196 [==============================] - 7s 38ms/step - loss: 0.4454 - accuracy: 0.9237 - val_loss: 0.5165 - val_accuracy: 0.9089\n",
      "Epoch 114/150\n",
      "196/196 [==============================] - 7s 38ms/step - loss: 0.4372 - accuracy: 0.9278 - val_loss: 0.5207 - val_accuracy: 0.9106\n",
      "Epoch 115/150\n",
      "196/196 [==============================] - 7s 38ms/step - loss: 0.4372 - accuracy: 0.9263 - val_loss: 0.5209 - val_accuracy: 0.9106\n",
      "Epoch 116/150\n",
      "196/196 [==============================] - 7s 38ms/step - loss: 0.4356 - accuracy: 0.9272 - val_loss: 0.5185 - val_accuracy: 0.9102\n",
      "Epoch 117/150\n",
      "196/196 [==============================] - 7s 38ms/step - loss: 0.4301 - accuracy: 0.9282 - val_loss: 0.5169 - val_accuracy: 0.9099\n",
      "Epoch 118/150\n",
      "196/196 [==============================] - 7s 38ms/step - loss: 0.4297 - accuracy: 0.9283 - val_loss: 0.5141 - val_accuracy: 0.9119\n",
      "Epoch 119/150\n",
      "196/196 [==============================] - 7s 38ms/step - loss: 0.4275 - accuracy: 0.9291 - val_loss: 0.5148 - val_accuracy: 0.9114\n",
      "Epoch 120/150\n",
      "196/196 [==============================] - 7s 38ms/step - loss: 0.4263 - accuracy: 0.9296 - val_loss: 0.5127 - val_accuracy: 0.9124\n",
      "Epoch 121/150\n",
      "196/196 [==============================] - 7s 37ms/step - loss: 0.4238 - accuracy: 0.9304 - val_loss: 0.5149 - val_accuracy: 0.9107\n",
      "Epoch 122/150\n",
      "196/196 [==============================] - 7s 38ms/step - loss: 0.4289 - accuracy: 0.9274 - val_loss: 0.5113 - val_accuracy: 0.9128\n",
      "Epoch 123/150\n",
      "196/196 [==============================] - 7s 38ms/step - loss: 0.4221 - accuracy: 0.9301 - val_loss: 0.5102 - val_accuracy: 0.9125\n",
      "Epoch 124/150\n",
      "196/196 [==============================] - 7s 38ms/step - loss: 0.4191 - accuracy: 0.9303 - val_loss: 0.5128 - val_accuracy: 0.9144\n",
      "Epoch 125/150\n",
      "196/196 [==============================] - 7s 38ms/step - loss: 0.4173 - accuracy: 0.9310 - val_loss: 0.5101 - val_accuracy: 0.9144\n",
      "Epoch 126/150\n",
      "196/196 [==============================] - 7s 38ms/step - loss: 0.4138 - accuracy: 0.9325 - val_loss: 0.5059 - val_accuracy: 0.9128\n",
      "Epoch 127/150\n",
      "196/196 [==============================] - 7s 38ms/step - loss: 0.4179 - accuracy: 0.9309 - val_loss: 0.5111 - val_accuracy: 0.9126\n",
      "Epoch 128/150\n",
      "196/196 [==============================] - 7s 38ms/step - loss: 0.4121 - accuracy: 0.9322 - val_loss: 0.5080 - val_accuracy: 0.9139\n",
      "Epoch 129/150\n",
      "196/196 [==============================] - 7s 38ms/step - loss: 0.4144 - accuracy: 0.9302 - val_loss: 0.5035 - val_accuracy: 0.9133\n",
      "Epoch 130/150\n",
      "196/196 [==============================] - 7s 38ms/step - loss: 0.4111 - accuracy: 0.9321 - val_loss: 0.5062 - val_accuracy: 0.9138\n",
      "Epoch 131/150\n",
      "196/196 [==============================] - 7s 38ms/step - loss: 0.4071 - accuracy: 0.9326 - val_loss: 0.5095 - val_accuracy: 0.9134\n",
      "Epoch 132/150\n",
      "196/196 [==============================] - 7s 37ms/step - loss: 0.4126 - accuracy: 0.9315 - val_loss: 0.5004 - val_accuracy: 0.9140\n",
      "Epoch 133/150\n",
      "196/196 [==============================] - 7s 38ms/step - loss: 0.4083 - accuracy: 0.9326 - val_loss: 0.5072 - val_accuracy: 0.9138\n",
      "Epoch 134/150\n",
      "196/196 [==============================] - 7s 37ms/step - loss: 0.4058 - accuracy: 0.9331 - val_loss: 0.5063 - val_accuracy: 0.9133\n",
      "Epoch 135/150\n",
      "196/196 [==============================] - 7s 38ms/step - loss: 0.4067 - accuracy: 0.9329 - val_loss: 0.4996 - val_accuracy: 0.9150\n",
      "Epoch 136/150\n",
      "196/196 [==============================] - 7s 38ms/step - loss: 0.4062 - accuracy: 0.9318 - val_loss: 0.5027 - val_accuracy: 0.9141\n",
      "Epoch 137/150\n",
      "196/196 [==============================] - 7s 38ms/step - loss: 0.4062 - accuracy: 0.9337 - val_loss: 0.4998 - val_accuracy: 0.9136\n",
      "Epoch 138/150\n",
      "196/196 [==============================] - 7s 38ms/step - loss: 0.4002 - accuracy: 0.9342 - val_loss: 0.5023 - val_accuracy: 0.9144\n",
      "Epoch 139/150\n",
      "196/196 [==============================] - 7s 38ms/step - loss: 0.4038 - accuracy: 0.9319 - val_loss: 0.4965 - val_accuracy: 0.9147\n",
      "Epoch 140/150\n",
      "196/196 [==============================] - 7s 38ms/step - loss: 0.4000 - accuracy: 0.9343 - val_loss: 0.5020 - val_accuracy: 0.9147\n",
      "Epoch 141/150\n",
      "196/196 [==============================] - 7s 38ms/step - loss: 0.3992 - accuracy: 0.9348 - val_loss: 0.5015 - val_accuracy: 0.9146\n",
      "Epoch 142/150\n",
      "196/196 [==============================] - 7s 38ms/step - loss: 0.3958 - accuracy: 0.9368 - val_loss: 0.4990 - val_accuracy: 0.9138\n",
      "Epoch 143/150\n",
      "196/196 [==============================] - 7s 38ms/step - loss: 0.3973 - accuracy: 0.9353 - val_loss: 0.4984 - val_accuracy: 0.9147\n",
      "Epoch 144/150\n",
      "196/196 [==============================] - 7s 37ms/step - loss: 0.3984 - accuracy: 0.9349 - val_loss: 0.4993 - val_accuracy: 0.9146\n",
      "Epoch 145/150\n",
      "196/196 [==============================] - 7s 37ms/step - loss: 0.3973 - accuracy: 0.9344 - val_loss: 0.5018 - val_accuracy: 0.9157\n",
      "Epoch 146/150\n",
      "196/196 [==============================] - 7s 38ms/step - loss: 0.3947 - accuracy: 0.9340 - val_loss: 0.5018 - val_accuracy: 0.9150\n",
      "Epoch 147/150\n",
      "196/196 [==============================] - 7s 37ms/step - loss: 0.3936 - accuracy: 0.9357 - val_loss: 0.5003 - val_accuracy: 0.9127\n",
      "Epoch 148/150\n",
      "196/196 [==============================] - 7s 38ms/step - loss: 0.3892 - accuracy: 0.9366 - val_loss: 0.5018 - val_accuracy: 0.9136\n",
      "Epoch 149/150\n",
      "196/196 [==============================] - 7s 38ms/step - loss: 0.3877 - accuracy: 0.9368 - val_loss: 0.5025 - val_accuracy: 0.9138\n",
      "Epoch 150/150\n",
      "196/196 [==============================] - 7s 37ms/step - loss: 0.3935 - accuracy: 0.9351 - val_loss: 0.4979 - val_accuracy: 0.9156\n"
     ]
    }
   ],
   "source": [
    "%env CUDA_DEVICE_ORDER=PCI_BUS_ID\n",
    "%env CUDA_VISIBLE_DEVICES=0\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.layers import Dense, Activation, Flatten, Dropout, BatchNormalization\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D\n",
    "from tensorflow.keras.datasets import cifar10\n",
    "from tensorflow.keras import regularizers, optimizers\n",
    "import numpy as np\n",
    "\n",
    "physical_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "assert len(physical_devices) > 0, \"Not enough GPU hardware devices available\"\n",
    "config = tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "\n",
    "\n",
    "### full code-add instance norm ###\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "\n",
    "#z-score\n",
    "mean = np.mean(x_train,axis=(0,1,2,3))\n",
    "std  = np.std(x_train,axis=(0,1,2,3))\n",
    "x_train = (x_train-mean)/(std+1e-7)\n",
    "x_test  = (x_test-mean)/(std+1e-7)\n",
    "\n",
    "baseMapNum = 32\n",
    "weight_decay = 1e-5\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, (3,3), padding='same', activation='relu', kernel_regularizer=regularizers.l1_l2(weight_decay, weight_decay), input_shape=x_train.shape[1:]))\n",
    "model.add(tfa.layers.InstanceNormalization())\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(32, (3,3), padding='same', activation='relu', \n",
    "                 kernel_regularizer=regularizers.l1_l2(weight_decay, weight_decay)))\n",
    "model.add(tfa.layers.InstanceNormalization())\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.3))\n",
    "\n",
    "model.add(Conv2D(64, (3,3), padding='same', activation='relu', kernel_regularizer=regularizers.l1_l2(weight_decay, weight_decay)))\n",
    "model.add(tfa.layers.InstanceNormalization())\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(64, (3,3), padding='same', activation='relu', kernel_regularizer=regularizers.l1_l2(weight_decay, weight_decay)))\n",
    "model.add(tfa.layers.InstanceNormalization())\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.3))\n",
    "\n",
    "model.add(Conv2D(128, (3,3), padding='same', activation='relu', kernel_regularizer=regularizers.l1_l2(weight_decay, weight_decay)))\n",
    "model.add(tfa.layers.InstanceNormalization())\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(128, (3,3), padding='same', activation='relu', kernel_regularizer=regularizers.l1_l2(weight_decay, weight_decay)))\n",
    "model.add(tfa.layers.InstanceNormalization())\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.4))\n",
    "\n",
    "model.add(Conv2D(128, (3,3), padding='same', activation='relu', kernel_regularizer=regularizers.l1_l2(weight_decay, weight_decay)))\n",
    "model.add(tfa.layers.InstanceNormalization())\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(128, (3,3), padding='same', activation='relu', kernel_regularizer=regularizers.l1_l2(weight_decay, weight_decay)))\n",
    "model.add(tfa.layers.InstanceNormalization())\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.4))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128))\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "#model.summary()\n",
    "\n",
    "#training\n",
    "batch_size = 256\n",
    "epochs = 150\n",
    "steps = x_train.shape[0] // batch_size\n",
    "\n",
    "boundaries = [steps*70, steps*110]\n",
    "values = [0.001, 0.0005, 0.0001]\n",
    "schedules = keras.optimizers.schedules.PiecewiseConstantDecay(boundaries, values)\n",
    "opt_adam  = keras.optimizers.Adam(learning_rate=schedules)\n",
    "\n",
    "\n",
    "# data augmentation\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "test_dataset  = tf.data.Dataset.from_tensor_slices((x_test, y_test))\n",
    "\n",
    "def process_data(image, label):\n",
    "    if tf.random.uniform(()) > 0.5:\n",
    "        image = tf.image.resize(image, (38, 38))\n",
    "        image = tf.image.random_crop(image, size=[32, 32, 3])\n",
    "        \n",
    "    if tf.random.uniform(()) > 0.5:\n",
    "        image = tf.image.flip_left_right(image)\n",
    "\n",
    "    return image, label\n",
    "\n",
    "train_batches = (train_dataset.shuffle(256*4)\n",
    "                              .map(process_data, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "                              .batch(batch_size)\n",
    "                              .prefetch(tf.data.experimental.AUTOTUNE) )\n",
    "\n",
    "test_dataset = (test_dataset.batch(batch_size))\n",
    "\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer=opt_adam, metrics=['accuracy'])\n",
    "history = model.fit(train_batches, validation_data=test_dataset, epochs=epochs, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32.418\n",
      "45.798\n",
      "53.482002\n",
      "58.143997\n",
      "62.646\n",
      "65.99\n",
      "68.342\n",
      "70.371994\n",
      "71.992\n",
      "73.186\n",
      "74.757996\n",
      "75.743996\n",
      "76.542\n",
      "77.236\n",
      "77.948\n",
      "78.715996\n",
      "79.144\n",
      "79.848\n",
      "80.022\n",
      "80.593994\n",
      "81.048\n",
      "81.383995\n",
      "81.632\n",
      "82.037994\n",
      "82.292\n",
      "82.792\n",
      "83.285995\n",
      "83.112\n",
      "83.516\n",
      "83.678\n",
      "83.954\n",
      "84.038\n",
      "84.348\n",
      "84.454\n",
      "84.715996\n",
      "84.598\n",
      "84.774\n",
      "85.299995\n",
      "85.136\n",
      "85.548004\n",
      "85.332\n",
      "85.836\n",
      "85.7\n",
      "86.02\n",
      "86.21\n",
      "86.228004\n",
      "86.236\n",
      "86.189995\n",
      "86.31\n",
      "86.592\n",
      "86.596\n",
      "86.872\n",
      "86.992004\n",
      "86.822\n",
      "86.644\n",
      "87.082\n",
      "87.187996\n",
      "87.268\n",
      "87.362\n",
      "87.476\n",
      "87.478\n",
      "87.558\n",
      "87.67799\n",
      "87.742004\n",
      "87.88\n",
      "87.91\n",
      "87.696\n",
      "88.06\n",
      "88.147995\n",
      "88.3\n",
      "89.526\n",
      "89.796\n",
      "89.75\n",
      "89.814\n",
      "89.911995\n",
      "90.112\n",
      "90.200005\n",
      "90.314\n",
      "90.15\n",
      "90.236\n",
      "90.064\n",
      "90.384\n",
      "90.46\n",
      "90.472\n",
      "90.402\n",
      "90.58\n",
      "90.866\n",
      "90.502\n",
      "90.675995\n",
      "90.688\n",
      "90.682\n",
      "90.925995\n",
      "90.862\n",
      "90.854004\n",
      "90.696\n",
      "90.792\n",
      "90.764\n",
      "91.006\n",
      "90.804\n",
      "91.024\n",
      "91.022\n",
      "90.958\n",
      "91.072\n",
      "90.922\n",
      "91.21\n",
      "90.978004\n",
      "91.04401\n",
      "91.122\n",
      "91.198\n",
      "91.496\n",
      "92.25\n",
      "92.45\n",
      "92.368004\n",
      "92.78\n",
      "92.628006\n",
      "92.72\n",
      "92.822\n",
      "92.826004\n",
      "92.906\n",
      "92.96\n",
      "93.036\n",
      "92.738\n",
      "93.014\n",
      "93.034\n",
      "93.096\n",
      "93.252\n",
      "93.09\n",
      "93.22\n",
      "93.018\n",
      "93.212006\n",
      "93.264\n",
      "93.146\n",
      "93.264\n",
      "93.31\n",
      "93.294\n",
      "93.184\n",
      "93.366\n",
      "93.422005\n",
      "93.192\n",
      "93.432\n",
      "93.482\n",
      "93.67799\n",
      "93.53\n",
      "93.488\n",
      "93.444\n",
      "93.396\n",
      "93.574\n",
      "93.658\n",
      "93.684\n",
      "93.512\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "accs = np.array(history.history['accuracy'])*100\n",
    "for i in range(150):\n",
    "    print(accs[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40.43\n",
      "46.9\n",
      "58.170002\n",
      "64.41\n",
      "68.35\n",
      "70.64\n",
      "73.909996\n",
      "72.259995\n",
      "75.81\n",
      "76.200005\n",
      "77.020004\n",
      "78.78999\n",
      "79.29\n",
      "80.76\n",
      "80.65\n",
      "80.909996\n",
      "80.61\n",
      "81.53\n",
      "82.5\n",
      "82.91\n",
      "83.41\n",
      "83.47\n",
      "83.18\n",
      "84.3\n",
      "85.02\n",
      "84.11\n",
      "84.88\n",
      "84.58\n",
      "85.59\n",
      "84.85\n",
      "86.14\n",
      "85.47\n",
      "85.579994\n",
      "86.02\n",
      "86.65\n",
      "86.229996\n",
      "86.43\n",
      "87.01\n",
      "86.909996\n",
      "86.94\n",
      "86.82\n",
      "86.479996\n",
      "85.829994\n",
      "87.09\n",
      "86.81\n",
      "86.93\n",
      "87.32\n",
      "87.5\n",
      "86.83\n",
      "87.52\n",
      "87.56\n",
      "87.49\n",
      "87.57\n",
      "87.82\n",
      "87.5\n",
      "87.75\n",
      "87.97\n",
      "88.24\n",
      "87.77\n",
      "87.73\n",
      "88.22\n",
      "87.71\n",
      "88.26\n",
      "88.9\n",
      "86.95\n",
      "87.98\n",
      "88.5\n",
      "88.15\n",
      "88.340004\n",
      "89.840004\n",
      "89.25\n",
      "89.560005\n",
      "89.83\n",
      "89.76\n",
      "90.05\n",
      "89.62\n",
      "90.060005\n",
      "89.87\n",
      "89.83\n",
      "89.93\n",
      "90.35\n",
      "89.87\n",
      "89.93\n",
      "90.29\n",
      "90.26\n",
      "90.32\n",
      "90.090004\n",
      "89.69\n",
      "90.21\n",
      "90.36\n",
      "90.04\n",
      "90.48\n",
      "90.12\n",
      "90.4\n",
      "89.76\n",
      "90.0\n",
      "90.020004\n",
      "90.48\n",
      "90.08\n",
      "90.3\n",
      "90.11\n",
      "90.08\n",
      "90.020004\n",
      "90.39\n",
      "90.43\n",
      "90.57\n",
      "90.33\n",
      "90.08\n",
      "90.03\n",
      "90.81\n",
      "90.97\n",
      "91.03\n",
      "90.89\n",
      "91.06\n",
      "91.06\n",
      "91.02\n",
      "90.99\n",
      "91.189995\n",
      "91.14\n",
      "91.24\n",
      "91.07\n",
      "91.28\n",
      "91.25\n",
      "91.439995\n",
      "91.439995\n",
      "91.28\n",
      "91.259995\n",
      "91.39\n",
      "91.329994\n",
      "91.38\n",
      "91.34\n",
      "91.399994\n",
      "91.38\n",
      "91.329994\n",
      "91.5\n",
      "91.409996\n",
      "91.36\n",
      "91.439995\n",
      "91.46999\n",
      "91.46999\n",
      "91.46\n",
      "91.38\n",
      "91.46999\n",
      "91.46\n",
      "91.57\n",
      "91.5\n",
      "91.27\n",
      "91.36\n",
      "91.38\n",
      "91.56\n"
     ]
    }
   ],
   "source": [
    "val_as = np.array(history.history['val_accuracy'])*100\n",
    "for i in range(150):\n",
    "    print(val_as[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
